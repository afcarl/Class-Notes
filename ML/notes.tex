\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}


\documentclass[a4paper]{article}
\usepackage{amsmath}
\begin{document}
\title{Machine Learning CS726 Fall
 '11}
\author{Angjoo Kanazawa}
\maketitle
\section{September 1st: Class one}

\subsection{Logistics}
\label{sec:logistics}


\begin{itemize}
\item Uname: cmsc726 Pass: generalize
\item Programming projects \textbf{27\%}: 3 total, after 48hrs 50\%
  down, teams. (Python with numpy).
\item Written hw \textbf{18\%}: 13 of them, one per week out of 3
  scales \{0, 0.5, 1\}. Individual, can't belate.
\item Midterm \textbf{25\%}:
\item Final ``practical'' exam \textbf{25\%}: Canned or your
  choice/teams, presentations during the final slot
\item Class/Piazza participation \textbf{5\%}
\end{itemize}
Time expectation
\begin{itemize}
\item 3 hrs in class
\item 2 hrs reading
\item 2 hrs on written asg
\item 2 hrs on programming projects
\item Don't ask questions that have already been answered
\end{itemize}
Do now:
\begin{itemize}
\item \textbf{Do hw00}: Due 6 Sept, next tuestday (Submit in .pdf only using \emph{handin})
\item \textbf{Do the first reading}: pass protected, read the web page.
\item \textbf{Sign up}: Subscribe to the Piazza group
\end{itemize}

\subsection{Contents}
\label{sec:contents}
\textbf{Classification}
\begin{itemize}
\item When there's an ambiguity in the data, which function are you
  going to learn? => \textbf{Inductive Bias} (i.e. bird vs non-bird,
  fly vs land, background focus vs bg blurred)
\item Which is the more reasonable distinction to make?
\item The primary way different learning algorithms vary. There's no
  right answer. Does your inductive bias match your problem?
\item 3 primary components: labels, features, model
\end{itemize}
\textbf{Ingredients for classification}
\begin{enumerate}
\item Feature representation
  \begin{itemize}
  \item not typically a focus of machine learning
  \item Considered as ``problem specific'', but very problematic if bad.
  \end{itemize}
\item Training data: labeled examples
  \begin{itemize}
  \item Often expensive
  \item Sometimes available for free
  \end{itemize}
\item Model
  \begin{itemize}
  \item No single learning algorithm is always good (``no free lunch''
    theorem)
  \item Requires some control over \textbf{generalization}
  \end{itemize}
\end{enumerate}


\textbf{Regression} is like classification, except that the \emph{labels are
real values.}
\textbf{Structured Prediction} 


\pagebreak
\section{September 6th: Class 2}
Answer hw questions in few sentences.

\subsection{Intro cont}
\begin{itemize}
\item 95\% of things in ML are binary classification. But many ML
  problems reduce to binary classification. We don't have to develop
  solutions for different problems
\item \textbf{Reinforcement Learning }- unlike classification, regression, and
  unsupervised learning, RL doesnot recieve examples, but it
  \textbf{experiences} by interacting with the world.
\item RL always has \emph{time} as a variable. i.e. chess, robot
  control. Comes with the ``exploration vs exploitation''
  trade-off. Humans are really bad at exploitation.
\item Defined by \emph{state space}, the world, \emph{actions}, and
  the \emph{reward}.
\item Is a natural extention to search etc we won't cover this too much.
\end{itemize}

Why we need some math:
\begin{itemize}
\item \textbf{Calc/Linear Algebra}: finding maxima/minima of
  functions, allows high-dimensional data analysis.
\item \textbf{Probability}: the study of the outcome of repeated
  experiments (frequentist), the study of the plausibility of some
  event (bayesian)
\item \textbf{Statistics}: the analysis and interpretation of
  data. Uses probability theory a lot. Stat is very similar to ML. But
  stat is about analysis/interpretation of data, but ML is about
  generalizing and using the analysis to make predictions.
  \begin{itemize}
  \item A lady drinking tea..
  \item model vs predictions
  \item model fit vs generalization
  \item explain the world vs predict the future
  \end{itemize}
\end{itemize}

\subsection{Decision Trees}
\begin{itemize}
\item Asks questions at nodes, edges are possible options.
\item Super efficient, the amount of time is the depth of the tree.
\item Histogram representation: in the absence of extra information
  (no info, no question), make a frequency histogram of the end result. i.e. 62\%
  play, 38\% no play.
\item Hope that you get about 62\% right. Assumption: relative frequency of
  play/no play is indicative of the real distribution of play/no play.
\item Try to ask one question at a time that's most benefitial.

\item \emph{example: is sunny?} =>\{no (play-80\%, noplay-20\%),
  yes(play-33\%, noplay-66\%)\}. If no, I'll guess play and expect to
  get 80\%, if yes, I'll guess no and expect about 2/3rds right.
\item On 5/8th of the days it's not sunny, 3/8th of the days is sunny.
\item $5/8\times 80\% + 3/8\times 66\% \approx 75\%$
\item \emph{example: is windy?} => \{ yes(p-50\%, np-50\%),
  no(p-75\%, np-25\%). Frequecy of windy days is 1/2.
\item So the training accuracy under this feature(question): $1/2(75\%) + 1/2(50\%) =5/8 = 62\% $
\item So, do I want to ask ``is windy'' or ``is sunny''? Is sunny!
\item For each of the featuers, we're going to compute the training
  accuracy, get the feature that maximizes the accuracy. Then make
  that the first question on the decision tree.
\item After the root of the tree is found, remove that feature,
  partition the data associated with the correct edge (not sunny days
  or sunny days) and repeat the step == The algorithm for decision
  tree:
  \begin{enumerate}
  \item \texttt{guess - get the most frequent answer}
  \item \texttt{base case: labels are umbiguous, no more features.}
  \item \texttt{else for all features, compute the score (training
    accuracy). Find the guy with max score, split the data, remove
    that guy, recurse on left and right}
  \end{enumerate}


\item A recursive function, so the depth of the recursion is the number
  of features, if the features are binary (other wise it's the number
  of possible feature values etc). So this will terminate.
\item Eventually you'll ask every question, so we'll hit the second
  base case only if there is an inconsistency (two examples with exact
  same features but with different labels)
\item \textbf{goal}: good future predictions given (label, example)
  pairs. A probability distribution, $D$, is the guy generating the
  examples of what the problem looks like. Is the God. Good future
  predictions means minimum test error.

\item $P_D(label, example)$. If I have learned well, I
  should be correctly assign labels to examples that $D$ thinks is likely.
\item test error=$\epsilon = E_{(x,y)~D}[f(x) \neq y]$, where $f(x)$
  is the prediction, $y$ is the true label.

\item $x$ may be reasonable, or not, because it may not fit the type
  of data for the problem.
\item We don't have $D$, but we have the training data $(x,y)$,
  examples drawn from $D$. Without $D$, minimizing $\epsilon$ is too hard.
\item \textbf{goal'}: good training predictions, i.e. minimize
  training error.
\item  $\hat{\epsilon} = 1/N \sum_{(x,y)\in train}[f(x)\neq y]$
\item Can the decision tree algorithm minimize $\hat{\epsilon}$? if
  there is no ambiguity in the training data, we can ask all the
  question (worst case), then we pinned down all examples (and there
  are no diff labels), and we can do this. If there is an
  ambiguity/inconsistency,  it's not possible to always get the 0 training error.
\item The best we can hope for is that whatever this minimum error is,
  it will achieve that. For this goal', the algorithm is optimum.
\item Does solving goal' help us help solve goal? if the assumption
  that test comes from the same $D$, we can.
\item Doesn't work when test data is small, and is not representative
  of the $D$.
\item Say there are spurious correlations between features and the labels in the
  training data. (looks useful in training, but not in real
  test). The decision tree will pick these features, it will make
  $\hat{\epsilon}$ low, but $\epsilon$ will be high. This is \textbf{overfitting}.
\item Solutions: constrain the height of the tree (you can only ask x
  questions and hope that it won't ask questions about this spurious feature), get more data.
\end{itemize}

\pagebreak
\section{September 8th Class 3}
\label{sec:class3}

Project 1 is online now. Implement basic classifiers. Start early! Due
27th September.

\subsection{HW1}
\label{sec:hw1}

\begin{enumerate}
\item Memorizing doesn't generalize, and overfits.
\item Look at \ref{sec:decisiontree}
\item Goal: minimize $\epsilon = \mathbf{E}_{(x,y)~D}[f(x)\neq y]$. We
  can calculate $\hat\eps =\frac{1}{N}\sum_n[f(x_n)\neq y_n]$. If
  overfit, we do too well on the training data so $\hat \eps$ will
  be tiny but not the $\eps$.
\item The algorithm should never look at the data because it'll overfit and we won't
  be able to say anything about the model's performance. We shouldn't
  look at it because we'll get a sense of what features will be
  important and fix the model accordingly.
\end{enumerate}

\subsection{Decision Tree}
\label{sec:decisiontree}
\textbf{Claim}: shallow trees are less prone to overfitting.\\
\noi
Example: two coins, A, B. Result of coin A will be the feature,$x$, that
of coin B will be the label,$y$ (they aren't correlated). Got $(1,1),
(0,0), (0,0), (0,1)$. Looks like the feature is very useful, but
not. The hope is that these accidental correlation will go down the
tree, and the actual features that work go up the tree.

The depth is a \emph{hyper parameter}. The goal of using
hyperparameters is to reduce overfitting. If we treat it as a
parameter, it'll just use max depth and lower $\hat
\eps$.  \emph{Parameters} are what you estimate on the training data.
  
\subsection{Geometry}
\label{sec:geometry}
We represent our inputs as vectors in high dimensional space. $y =
\pm1$, $x = (x_1, x_2, \cdots, x_n) \in \mathbf{R}^D$

\noi
\textbf{K-NN}: Look at $K$ nearest neighbors, label the new point as
the majority/mean of the neighbors. If $\inf$-NN, that's just taking the majority. This is
underfitting. $1$-NN would be overfitting. Choose odd $K$ so that
there won't be any ties.

\noi
\textbf{K-mediods Clustering}. 
\begin{enumerate}
\item Pick $K$ datapoints to be representatives
\item $\forall n$, put $n$ in the closest cluster (closest $K$ datapoints)
\item Choose a new prepresentative that minimizes the average dist to all
  cluster members
\item Go back to 2
\end{enumerate}
\noi
\textbf{K-means Clustering}. 
\begin{enumerate}
\item Pick $K$ datapoints randomly $\mu_1, \dots, \mu_k$
\item $\forall n$, $z_n = \arg\min_k |x_n - \mu_n|$. 
\item $\forall k$, $\mu_k = \frac{1}{n_k} \sum_{n; z_n = k}[x_n]$ (Make a new mean prepresentative who is an average of all cluster members.)
\item Go back to 2
\end{enumerate}
\noi
Does $K$-means always converge? 
Measure the quality of the solution by
the average distance from each point to its mean (score).
(Convergence as in this measure of cluster quality won't change.)\\
Observation 1:  Everytime step 2 or 3
is executed, the score goes down or stays the same. \\ 
Oberservation 2: If there are $n$ members and $k$ clusters, the
possible clustering assignment is finite, $k^n$.


\noi
Does $K$-means always find the optimal clustering?\\
No! Not necessarily. The initial point assignment is random and it
changes the result. The practical method is to do multiple
initialization and pick the smallest score.

\noi
How many iteration does it take to converge?\\
In theory it takes a time that's exponential to $k$. In practice, it
converges in like 4 iterations. Smooth analysis of why this converges
quickly. Only certain initial points take a long time. If it's taking a loong time, if you
perturbe the initial points and move it away (like how simplex is
really fast in real life).

$K=n$ ($\hat\eps=0$) or $K=1$ gives us no information about the data. As an implementation
note, some cluster centers will disappear and never come back. So be able to handle that situation.

\subsection{Curse of High Dimentionality}
\label{sec:curseofhighdimentionality}

as we make the dimention higher, the distance between points get
smaller. If every point is about the same distance from everything
else, the $k$-nn are just random points. Picking the majority of those
random points, is just another random guessing. As $D$ increases, the
distance between points concentrates to a point.
\begin{verbatim}
D = 100; hist(flatten(XtoD(rand(1000, D )) / sqrt(D)));
%XtoD takes the distance between points
\end{verbatim}

Seems like $K$-nn shouldn't work, but because the $D$ is uniform. It does work in real life,
because in real life the actual data is correlated.
\pagebreak
\section{September 15th Class 4}
\label{sec:class4}

\subsection{HW2}
\label{sec:hw2}

\begin{enumerate}
\item[2.] Decision boundary for a one nearest neighbor classifiers on
  two datapoints is a linear plane perpendicular to both points.
\item[3.] Clustering as in given labled data $(x_n, y_n)$, $\forall
  n\in N$ for each label run $K$-means on subset of data with this
  label, then run $KNN$ using $\mu_i$s. A lot faster on $KNN$ test
  time but more time on training,  in a sense clustering throws
  away bunch of data. This could avoid overfitting. (Assuming that
  we're working on relevant feature set).
\item[4.] in 2D space, the dot product, $<\bar w,\bar x>$, projects
  all the points, $X$, onto $\bar w$ and makes it 1D.\\
If $y=sign(w\cdot x)$, is the decision rule, then the boundary $B$ is $\{x: w\cdot x =
0\}$. The disadvantage is that it assumes the decision boundary lies
on the origin. This won't work if all the points are positive. So now,
modify $y$ to $y=sign(w\cdot x + b)$, s.t. $B=\{x:w\cdot x =
-b\}$. Bias is always $-b$, as $b\rarr -\infty$ , nothing can be
classified as positive.\\
Claim: using bias is the same as not using the bias but adding a new
feature. i.e. $x$ or $<1,x>=\tilde x$, where $x$ uses $w$ (to get
$w\cot x + b$) and $\tilde
x$ uses $\tilde w = <b w>$ so it's the same thing i.e. $w\cot x + b =
\tilde w \cdot \tilde x = <b,w>\cdot<1,x>$. So we can always write as
if there's not bias.
\end{enumerate}

\subsection{Perceptron}
\label{sec:perceptron}
General algorithm
\begin{verbatim}
for each (x,y)
   //if error, do an update
   if y(w\cdot x + b) \le 0: or if sign(y) == sign(w\cdot x + b):
      w = w + yx
      b = b + y
\end{verbatim}
Usually people scale the update with some constant $\gam < 1$, the
learning rate, so that it won't be so influenced by $x$. (each single
error);

\emph{Question:} If you add $\gam$ does it change?? Doesn't really change anything,
because after one update, the scale of $w$ changes, but the direction
is the same. With a bias, it'll shift it a lot of a little. 
After any number of updates, without using the learning rate, you'll
get $(w,b)$ in the end but with $\gamma$, all you'll get is $(\gam w,
\gam b)$., the decision boundary will have the same direction.\\
In the case of perceptrons, adding a $\gam$ makes no difference
(assuming we have infinite floating point). In the case that $w$ is a
unit vector, $b$ shifts the decision boundary $b$ units, if $w$ is not
unit, it shifts $b/||w||$ units. So if $||w||>>$, $b$ won't really
make that much difference.\\
More important point: the more you run, the less sentisitive the
algorithm will be for any update. Early on, the decision boundary
moves all over the place, but as the number of epoches increases, the
relative magnitude of the update is smaller than that of the beginning
updates. I.e. even if it won't converge, it'll slow down in the end.

\pagebreak
\section{September 22nd Class 6}
\label{sec:class6}

\subsection{Linear Seperability}
\label{sec:perceptron2}

Perceptrons can't do XOR, because 2D XOR is not linearly
seperable. But if you add a 3rd dimension, where $-$'s have 0 and
$+$'s have 1. Then you can have a hyperplane that seperates them.

You can add all possible combinations linearly, this is what
perceptrons do. But that's computationally expensive and makes the
feature very high dimension. This is prone to overfitting, and you
need more examples.

DT can combine few features but in a complicated way.\\
DT might fail on XOR because of all ties, features very slightly
correlated with the label..

\subsection{Evaluation/ROC}
\label{sec:roc}

Given a ratio of dataset where $1\%$ of them are positive and $99\%$
is negative, the classifier will say all of them are negative and
achieve $99\%$ accuracy.
Example for cancer detection, false negative is worse than false positive.
So there are different costs associated with different classification.
Perhaps my goal is to order the data to see what's interesting to look
at.

In biology, people look at \emph{Sensitivity vs Specificity}, we call
it \emph{Precision vs Recall}.
Precision says of all the $X$’s that you found, how many of them were
actually $X$’s? Recall asks: of all the $X$’s that were out there, how many
of them did you find? (should've found)

Choose the best threshold and get the harmonic mean, or their
\textbf{f-measure}:
$$F = \frac{2PR}{P+R}$$

\subsection{Cross-validation}
\label{sec:crossvalidation}

Get bunch of data, divide it in $n$ equal size. Say $n=10$, train on
$n_2\dots n_{10}$, and test on $n_1$, but now, you can train on $n_1
\dots n_9$, then test on $n_{10}$. You can do this $n$ times. Then
you'll get performance over all of the data I have, not just a single
slice.

Also you do this to tune your hyperparameters. For each hyperparameter
setting, you get the score of each, you want the hyperparameter that
does the best on average over all slices.

Say you run this on DT and get that depth = 17 is the best, so you
have $n$ trained classifier, get the one that does the best or,
retrain a new classifier on all $n-1$ training slices.
Usually learning a new classifier is a better idea, but it might be
slightly underfit, because you're regularizing (with depth=17), but
using more data.

4 choices: Number of slices $n=10$ if you're normal, you do $n=5$ if you're
running out of time, you do $n=2$, you'll make sure your two
classifiers aren't trained on the same data points, one more is train on everything except 1,
\emph{leave-one-out}, great for $knn$ but too expensive for others.
Leave-one-out is the least succeptable to underfitting.

\subsection{Statistical Significance}
\label{sec:statisticalsignificance}

Try to answer the question, ``given this new method, does it actually
work?''. You should use the most commen thing in your field. Two
examples we cover is \emph{Paired T-test} and \emph{Jacknifing/Bootstrapping}.

Given DT and Perceptron's performance, say perceptron always does better.
``If I were to do a new experiment, with what probability would
Perceptron beats DT''

This is what T-test does, fit gaussian to DT, fit gaussian to
Perceptron, and if the overlap is tiny, then there's small probability
that DT will beat perceptron, this gives us a $p$-value. 

Weakness is that there is a normal distribution assumption. LLN
argument, it should be kind of, but... not really because accuracy
lives between [0, 1], normal lives between $[-\infty, \infty]$.

So some people say that you should do binomial tests, but ppl still do
$p$-value.\\

\textbf{Jackknifing/Bootstrapping} (The word ``Bootstrapping'' is used
a lot everywhere). This is suited where your evaluation method is
holistic over the tneir data. Like $F$-measure.. Way around when T-test can't really be
applied. Doesn't make normality assumption, it's a
\emph{non-parametric} test.

If you have a big dataset, it's easier to seperate the two gaussian
curves (because they'll have more peaks), so you can get more
examples to get more statistical significance.

\pagebreak

\section{September 27th Class 7}
\label{sec:class7}

\subsection{HW3}
\label{sec:hw3}

\begin{enumerate}
\item About non-linearly seperable data and perceptron: Do the add one
  feature augmentation trick, why is this linearly seperable now? For
  example, $\vec w$ $D$ zeros and all indicators as corresponding
  $y_n$ the label.\\
Does this affect generalization? The above construction will not work
for test data at all. Generally it overfits, because it's like
cheating after the first couple of passes when you start using the
labels $y_n$ as the components of your new augmented $\vec w$\\
How long does it take to converge on this augmented data? Previously,
$||x||\le R$, but now, we have $||<x, 0,0,\dots,1,\dots>||?$. Square
it:
\begin{align*}
  ||<x, 0,0,\dots,1,\dots>||^2 &= ||<x,
  \dots,1,\dots>\dot<x,\dots,1,\dots>||\\
&= x\dot x + 1\\
&=||x||^2 + 1
\end{align*}
So $\hat R = \sqrt{R^2+1}$, but now $\gamma$ changes also. Using the
$\vec w$ above, the margin for this $\vec w$ is $\min_{(x,y)\in
  D}\frac{y}{||w||}w\dot x$
$||w|| = \sqrt{N}$, and max $w\dot x$ is $1$ so the new margin is at
least $\frac{1}{\sqrt{N}}$
\item Centering, Variance scaling vs DT, KNN, perceptron:\\
  \begin{itemize}
  \item \textbf{KNN} is not sensitive to centering but it is sensitive to
    variance scaling.
  \item \textbf{DT} is not sensitive to centering or scaling because the cut
    off is still the cut off.
  \item \textbf{Perceptron} is sensitive to centring and variance scaling, because if the
    feature value is huge, it'll move $\vec w$ a lot to one direction,
    it'll take more time because $R$ is huge. 
  \end{itemize}
\end{enumerate}

\subsection{Blue Box}
\label{sec:bluebox}

\begin{itemize}
\item In the realm of \emph{scale} of features, variance pruning might
  not work if done prior to scaling because it might be a good feature
  in the correct scale.
\item \emph{How do you get a confidence out of a DT or KNN?} For DT
  you can calclulate the probability. Each node is a conditional
  probability. For KNN, we can calculate the average within $k$
  neighbors and get a percentage (with $K=6$, 3 might be $VE$, 3 might
  be $-VE$, so 50\% chance
\end{itemize}

\subsection{Multiclass Classification}
\label{sec:multiclass}

\textbf{Reduction-based ML}: Is multiclass harder than binary? If I
could do binary, then I'll use that to solve multiclass.

\emph{Error-bounds}: If I can achieve $\eps$ error in binary
classification, we can have $\le f(\eps)$ error on multiclass
classification.

The big assumption is that we can achieve $\eps$ error.

\textbf{Definition:} Binary Classification.
\begin{itemize}
\item Assume some distribution $\mathbf{D}$ of $(x,y)$ pairs
\item get sample $D$ $(x_1,y_1),\dots,(x_n,y_n)$ drawn from $\mathbf{D}$
\item The goal is to compute $f:X\rarr \{-1, +1\}$
  s.t. $\mathbf{E}_{(x,y)~D}[f(x)\neq y]$ is small.
\end{itemize}

\textbf{Weighted binary classification}: Weight the negative sample less and
positive samples more, so that the classifier has to put more effort
to classify positive examples correctly. Now the goal is different it
is to minimize (arbitrarly assuming that possitive error costs more here):
$$\mathbf{E}_{(x,y)~D}
\begin{cases}
  \ah[f(x)\neq  1]& \text{ if y=1}\\
[f(x)\neq -1] &  \text{ if y=-1}\\
\end{cases}
$$
For some $\ah > 0 \in \mathbf{R}$. Above can be re-written as
$\mathbf{E}_{(x,y)~D}[\ah^{y=1}[f(x)\neq y]$
Algorithm to use: \textbf{Subsampling}. We end up with a ``balanced''
data. If the cost is $\ah=10$, I want 10th as many positive
examples. This is optimal for us.
What we want
\begin{enumerate}
\item given $\mathbf{D}^{WBC}$, we want to reduce it to $\mathbf{D}^{BC}$
\item learn a function $f$ on the binary classification problem.
\item given a test point $\hat x$, use $f$ to solve the weighted
  binary classification problem.
\end{enumerate}
The algorithm
\begin{enumerate}
\item[1a] Retain all positive examples
\item[1b] For each negative examples, sample $1/\ah$ of them.
\item[2]  $\hat y= f(\hat x)$ (i.e. 100 exmples, 98 -ve, 2ve, $\ah=2$,
  randomly select 49 negative examples.
\end{enumerate}
We want $\mathbf{E}_{(x,y)~D^{WBC}}[\ah^{y=1}[f(x)\neq y]]$, from
weighted $D$.
\begin{align*}
  \mathbf{E}_{(x,y)~D^{WBC}}[\ah^{y=1}[f(x)\neq y]]&=
 \sum_{(x,y)}\mathbf{D^{WBC}}(x,y)[\ah^{y=1}[f(x)\neq y]]\\
 &= \sum_x \begin{cases} \mathbf{D^{WBC}}(x, +1)[\ah [f(x)\neq 1]]& \\
     \mathbf{D^{WBC}}(x, -1)[f(x)\neq -1]\end{cases}\\
 &= \sum_x \begin{cases} \mathbf{D^{BC}}(x, +1)[\ah [f(x)\neq 1]]\\
     \ah\mathbf{D^{BC}}(x, -1)[f(x)\neq -1]\end{cases}\\
 &= \ah \sum_x\begin{cases} \mathbf{D^{BC}}(x, +1)[[f(x)\neq 1]]\\
     \mathbf{D^{BC}}(x, -1)[f(x)\neq -1]\end{cases}\\
&=\ah \eps
\end{align*}
Because we downselected negative examples, so to go from $D^{WBC}$ to
$D^{BC}$, we need to multiply it with $\ah$. 

Can we do better than $\ah \eps$ (on the weighted problem)? No, the
error grows linearly with the size of $\ah$. You could imagine to do
slightly better because of the sampling more of +ve but something
around there.

Sampling seems wasteful. SO instead, we can keep all negative
examples and put $\ah$ times more positive examples (replacement
of). This is \textbf{over-sampling}. Both gives us the same bounds,
but $\eps$ can be different. $\eps$ is the error rate of the BC
classifier. The big assumption is that we can achieve $\eps$, it's
easier to achive low $\eps$ in oversampling than undersampling. \emph{Even
though the bound for both sampling is $\ah\eps$, $\eps$ is smaller
from oversampling, generally.}

\subsection{Turning MC to BC}
\label{sec:MC2BC}

\begin{enumerate}
\item \textbf{OVA} (one-vs-all): with $k$ classifiers $f_1\dots f_k$, where $f_1$ seperates
  $y=1$ from $y\neq 1$ and $f_k$ seperates $y=k$ from $y\neq k$. Two
  bad things can happen:no one says yes, more than one says yes. Just
  pick at random for each, but these are the cases when the error
  occurs. Not great because of this. In practice, you pick the one
  with the highest confidence. Called brittle, because it takes only
  one error will shoot up the probability of error on MC. Any errors
 can cause grave harm, if each error is $\eps$, and you have $k$
 chances, it's pretty bad.
\item \textbf{AVA} (all-vs-all): $ k \choose 2 $ classifices where
  $f_{ij}$ seperates class $i$ from $j$.
\end{enumerate}

\pagebreak

\section{September 29th Class 8}
\label{sec:class8}

\subsection{HW 4}
\label{sec:hw4}

\begin{enumerate}
\item Task for squared error regression: Given: input space $\mathcal{X}$
  and an unknown distribution $\mathcal{D}$ over $\mathcal{X}\times\mathbf{R}$, Compute $f:X\rarr
  \mathbf{R}$, minimize $\mathbf{E}_{(x,y)~\mathcal{D}}[f(x)\neq y]$
\item \emph{To what types of error do these theorems apply?} All error
  in respect to data taken from the same distribution $\mathcal{D}$.
\item OVA vs AVA. If training for each classifier is $O(N)$, OVA:
  $O(NK)$, AVA:$O(K^2N/K) = O(K)$, if training for each classifier is
  $O(N^2)$, OVA: $O(N^2K)$, AVA: $O(K^2(N/K)^2)=O(N^2)$, so AVA is
  better! so the worst it is to train, the better and better AVA gets.
\item A ranking preference function $\omega$ that penalizes
  mispredictions linearly up to $K$: 
$$\omega(x,y) =
\begin{cases}
  min\{k, |i-j|\} & \text{ if $i\neq j$}\\
  0 & \text{ else}
\end{cases}$$
\end{enumerate}

\subsection{Continue on Multiclass classification}
\label{sec:multiclass}

There's OVA, AVA(the default), and \textbf{Tree-based}
classification. Idea is to divide and conquer, split all the classes
in two, then keep on splitting. At the leaf we make decisions. So if
we have $K$ classes, we have $K-1$ nodes and so we train $K-1$
classifiers. This divides the data, so we beat OVA's $O(K\eps^8)$, $K$
factor.

As soon as we make an error, we're done. The chance of making an error
is $\eps^b$, the number of times you can make an error is how many
nodes you go down (pf union bound: $P(A_1\cup A_2\cup \dots A_n) \le
\sum P(A_i)$, so at most the error caused by tree based is
$$\eps^{MC}\le O(\eps^blog(K))$$
If the number of features is not a power of 2, it might not be a full
binary tree with height $log(K)$, so more of the emphasis on $\le$.

But here we can't specify which order/the layout of the tree. Certain
layouts could make making a binary classifier easier. I.e. if data
were clustered in $1:(0,0), 2:(0,1), 3:(1,0), 4:(1,1)$, shouldn't
split classes $\{1,4\} vs \{2,3\}$.

\subsection{Ranking}
\label{sec:ranking}
\textbf{Bipartite Ranking}: Some are labeled good and some are
bad. The job is to rank the good, relevant ones before the bad
ones. Given nodes $a,b$, the labels is $1$ is $a$ is good and comes before $b$ and
should, $-1$ if $b$ comes before $a$.\\
We use $\omega(x,y)$ as the preference function just like binary classification's loss
function. Where $x$ is the true position, $y$ is my ranking.

\textbf{Objective}
\emph{Given} an input space $\mathcal{X}$, and an unknown distribution
$\mathbf{D}$ over $\mathcal{X}$, \emph{compute:} a function
$f:\mathcal{X}\rarr \Sigma_M$ minimizing $$\mathbf{E[\sum_{u\neq
    v}[sig_u<sig_v][\hat \sig_u < \hat \sig_v]\omega(\sig_u,\sig_v)}$$
Where $\hat sig = f(x)$. Basically did we swap $u$ adn $v$, if so, pay
$\omega(u,v)$.
$$f(x_{uv}) \begin{cases}
  +1& \text{ if } u<v\\
-1&\text{ if }v < u
\end{cases}$$
$f(x_{uv})$ answers ``is $u<_fv$?'' Now use this as a comparison
function and use any sorting algorithm, like quicksort. So it's just a
quicksort with a minor tweak:\\

Consider $f(x_{uv})$ as $P(u<v)$, so in quicksort, pick a pivot, is
this other number greater than the pivot or not? We answer this by
running $f$, and $f$ will tell us $x\%$ it's less. So it's like a
randomized quicksort.

So the weighted problem looks like:, given $(y=\{+1,-1\},x,w)$, (where
$w=\omega$, $x= x_{uv}$) we want to minimize
$$\eps^w = \mathbf{E}_{(x,y,w)~\mathcal{D}}[w(y\neq f(x))]$$
How would you solve this? Over sampling, add 2 more $(x,y)$ if
$w=2$. Undersampling, we want to keep if $w$ is high, so throw out
data with probability $1/w$. So if $w$ large, we most likely never
throw out the data. And we have $\eps^w \le \eps^b[\mathbf{E}_w[w]]$
is the best we can hope for.

\subsection{Collective Classification}
\label{sec:collectiveclassification}

Given a graph, where node is a feature, like a website, and if we were
to label each node as $+1$ not offensive, or $-1$ offensive, the idea
is that some information in the graph structure will tell us about the
label. i.e. offensive websites tend to link to offensive websites.

So train a classifier with a bigger feature, wher you include
information about the node itself plus the number of positive
neighbors and negative neighbors. Problem with this is that.......TODO!

The \textbf{Stacking algorithm}: Forget that we have a graph structure. First try
to classify each page based on its individual features alone. So
think: $f_0:\mathcal{X}\rarr \{+1, -1\}$, and now I have guesses for
all.
Now we can train a second round classifier, that takes an input from
the pages themselves and the neighbor information from the previous
classifier ( the new added info on the feautures are given by
$f_0$). So train $f_1:\mathcal{X}+f_0\text{'s output}\rarr
\{+1,-1\}$. Now, the hope is that $f_1$ wil get everything correct.
Now train $f_2$ that takes in $\mathcal{X}$ and $f_0$ and $f_1$'s
output. etc.

Problem: it never get's the true label of the neighbors, but only the
predicted data. This might be beneficial because at test time we never get to see
the true label of the neighbors. But the issue is overfitting: $f_0$
is trained and tested on the original data, so chances are it'll do
really good on the input data. So input to $f_1$ will be
unrealistically close to perfect. In the real world $f_0$ will do much
worse.

Concrete example: task for each 3D point, guess if it corresponds to a
tree or not, don't get info but angle and distance. It's nearly
impossible to classify the point just by using it's features, you
really need the entire graph structure. So at train time we do well,
but ... not in test..
\end{document}
