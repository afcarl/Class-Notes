\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\del}{\delta}
\newcommand{\thet}{\theta}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\wand}{\wedge}
\newcommand{\contra}{\Rightarrow\Leftarrow}

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\documentclass[a4paper]{article}
\usepackage{amsmath}
\begin{document}
\title{Machine Learning CS726 Fall
 '11}
\author{Angjoo Kanazawa}
\maketitle
\section{September 1st: Class one}

\subsection{Logistics}
\label{sec:logistics}


\begin{itemize}
\item Uname: cmsc726 Pass: generalize
\item Programming projects \textbf{27\%}: 3 total, after 48hrs 50\%
  down, teams. (Python with numpy).
\item Written hw \textbf{18\%}: 13 of them, one per week out of 3
  scales \{0, 0.5, 1\}. Individual, can't belate.
\item Midterm \textbf{25\%}:
\item Final ``practical'' exam \textbf{25\%}: Canned or your
  choice/teams, presentations during the final slot
\item Class/Piazza participation \textbf{5\%}
\end{itemize}
Time expectation
\begin{itemize}
\item 3 hrs in class
\item 2 hrs reading
\item 2 hrs on written asg
\item 2 hrs on programming projects
\item Don't ask questions that have already been answered
\end{itemize}
Do now:
\begin{itemize}
\item \textbf{Do hw00}: Due 6 Sept, next tuestday (Submit in .pdf only using \emph{handin})
\item \textbf{Do the first reading}: pass protected, read the web page.
\item \textbf{Sign up}: Subscribe to the Piazza group
\end{itemize}

\subsection{Contents}
\label{sec:contents}
\textbf{Classification}
\begin{itemize}
\item When there's an ambiguity in the data, which function are you
  going to learn? => \textbf{Inductive Bias} (i.e. bird vs non-bird,
  fly vs land, background focus vs bg blurred)
\item Which is the more reasonable distinction to make?
\item The primary way different learning algorithms vary. There's no
  right answer. Does your inductive bias match your problem?
\item 3 primary components: labels, features, model
\end{itemize}
\textbf{Ingredients for classification}
\begin{enumerate}
\item Feature representation
  \begin{itemize}
  \item not typically a focus of machine learning
  \item Considered as ``problem specific'', but very problematic if bad.
  \end{itemize}
\item Training data: labeled examples
  \begin{itemize}
  \item Often expensive
  \item Sometimes available for free
  \end{itemize}
\item Model
  \begin{itemize}
  \item No single learning algorithm is always good (``no free lunch''
    theorem)
  \item Requires some control over \textbf{generalization}
  \end{itemize}
\end{enumerate}


\textbf{Regression} is like classification, except that the \emph{labels are
real values.}
\textbf{Structured Prediction} 


\pagebreak
\section{September 6th: Class 2}
Answer hw questions in few sentences.

\subsection{Intro cont}
\begin{itemize}
\item 95\% of things in ML are binary classification. But many ML
  problems reduce to binary classification. We don't have to develop
  solutions for different problems
\item \textbf{Reinforcement Learning }- unlike classification, regression, and
  unsupervised learning, RL doesnot recieve examples, but it
  \textbf{experiences} by interacting with the world.
\item RL always has \emph{time} as a variable. i.e. chess, robot
  control. Comes with the ``exploration vs exploitation''
  trade-off. Humans are really bad at exploitation.
\item Defined by \emph{state space}, the world, \emph{actions}, and
  the \emph{reward}.
\item Is a natural extention to search etc we won't cover this too much.
\end{itemize}

Why we need some math:
\begin{itemize}
\item \textbf{Calc/Linear Algebra}: finding maxima/minima of
  functions, allows high-dimensional data analysis.
\item \textbf{Probability}: the study of the outcome of repeated
  experiments (frequentist), the study of the plausibility of some
  event (bayesian)
\item \textbf{Statistics}: the analysis and interpretation of
  data. Uses probability theory a lot. Stat is very similar to ML. But
  stat is about analysis/interpretation of data, but ML is about
  generalizing and using the analysis to make predictions.
  \begin{itemize}
  \item A lady drinking tea..
  \item model vs predictions
  \item model fit vs generalization
  \item explain the world vs predict the future
  \end{itemize}
\end{itemize}

\subsection{Decision Trees}
\begin{itemize}
\item Asks questions at nodes, edges are possible options.
\item Super efficient, the amount of time is the depth of the tree.
\item Histogram representation: in the absence of extra information
  (no info, no question), make a frequency histogram of the end result. i.e. 62\%
  play, 38\% no play.
\item Hope that you get about 62\% right. Assumption: relative frequency of
  play/no play is indicative of the real distribution of play/no play.
\item Try to ask one question at a time that's most benefitial.

\item \emph{example: is sunny?} =>\{no (play-80\%, noplay-20\%),
  yes(play-33\%, noplay-66\%)\}. If no, I'll guess play and expect to
  get 80\%, if yes, I'll guess no and expect about 2/3rds right.
\item On 5/8th of the days it's not sunny, 3/8th of the days is sunny.
\item $5/8\times 80\% + 3/8\times 66\% \approx 75\%$
\item \emph{example: is windy?} => \{ yes(p-50\%, np-50\%),
  no(p-75\%, np-25\%). Frequecy of windy days is 1/2.
\item So the training accuracy under this feature(question): $1/2(75\%) + 1/2(50\%) =5/8 = 62\% $
\item So, do I want to ask ``is windy'' or ``is sunny''? Is sunny!
\item For each of the featuers, we're going to compute the training
  accuracy, get the feature that maximizes the accuracy. Then make
  that the first question on the decision tree.
\item After the root of the tree is found, remove that feature,
  partition the data associated with the correct edge (not sunny days
  or sunny days) and repeat the step == The algorithm for decision
  tree:
  \begin{enumerate}
  \item \texttt{guess - get the most frequent answer}
  \item \texttt{base case: labels are umbiguous, no more features.}
  \item \texttt{else for all features, compute the score (training
    accuracy). Find the guy with max score, split the data, remove
    that guy, recurse on left and right}
  \end{enumerate}


\item A recursive function, so the depth of the recursion is the number
  of features, if the features are binary (other wise it's the number
  of possible feature values etc). So this will terminate.
\item Eventually you'll ask every question, so we'll hit the second
  base case only if there is an inconsistency (two examples with exact
  same features but with different labels)
\item \textbf{goal}: good future predictions given (label, example)
  pairs. A probability distribution, $D$, is the guy generating the
  examples of what the problem looks like. Is the God. Good future
  predictions means minimum test error.

\item $P_D(label, example)$. If I have learned well, I
  should be correctly assign labels to examples that $D$ thinks is likely.
\item test error=$\epsilon = E_{(x,y)~D}[f(x) \neq y]$, where $f(x)$
  is the prediction, $y$ is the true label.

\item $x$ may be reasonable, or not, because it may not fit the type
  of data for the problem.
\item We don't have $D$, but we have the training data $(x,y)$,
  examples drawn from $D$. Without $D$, minimizing $\epsilon$ is too hard.
\item \textbf{goal'}: good training predictions, i.e. minimize
  training error.
\item  $\hat{\epsilon} = 1/N \sum_{(x,y)\in train}[f(x)\neq y]$
\item Can the decision tree algorithm minimize $\hat{\epsilon}$? if
  there is no ambiguity in the training data, we can ask all the
  question (worst case), then we pinned down all examples (and there
  are no diff labels), and we can do this. If there is an
  ambiguity/inconsistency,  it's not possible to always get the 0 training error.
\item The best we can hope for is that whatever this minimum error is,
  it will achieve that. For this goal', the algorithm is optimum.
\item Does solving goal' help us help solve goal? if the assumption
  that test comes from the same $D$, we can.
\item Doesn't work when test data is small, and is not representative
  of the $D$.
\item Say there are spurious correlations between features and the labels in the
  training data. (looks useful in training, but not in real
  test). The decision tree will pick these features, it will make
  $\hat{\epsilon}$ low, but $\epsilon$ will be high. This is \textbf{overfitting}.
\item Solutions: constrain the height of the tree (you can only ask x
  questions and hope that it won't ask questions about this spurious feature), get more data.
\end{itemize}

\pagebreak
\section{September 8th Class 3}
\label{sec:class3}

Project 1 is online now. Implement basic classifiers. Start early! Due
27th September.

\subsection{HW1}
\label{sec:hw1}

\begin{enumerate}
\item Memorizing doesn't generalize, and overfits.
\item Look at \ref{sec:decisiontree}
\item Goal: minimize $\epsilon = \mathbf{E}_{(x,y)~D}[f(x)\neq y]$. We
  can calculate $\hat\eps =\frac{1}{N}\sum_n[f(x_n)\neq y_n]$. If
  overfit, we do too well on the training data so $\hat \eps$ will
  be tiny but not the $\eps$.
\item The algorithm should never look at the data because it'll overfit and we won't
  be able to say anything about the model's performance. We shouldn't
  look at it because we'll get a sense of what features will be
  important and fix the model accordingly.
\end{enumerate}

\subsection{Decision Tree}
\label{sec:decisiontree}
\textbf{Claim}: shallow trees are less prone to overfitting.\\
\noi
Example: two coins, A, B. Result of coin A will be the feature,$x$, that
of coin B will be the label,$y$ (they aren't correlated). Got $(1,1),
(0,0), (0,0), (0,1)$. Looks like the feature is very useful, but
not. The hope is that these accidental correlation will go down the
tree, and the actual features that work go up the tree.

The depth is a \emph{hyper parameter}. The goal of using
hyperparameters is to reduce overfitting. If we treat it as a
parameter, it'll just use max depth and lower $\hat
\eps$.  \emph{Parameters} are what you estimate on the training data.
  
\subsection{Geometry}
\label{sec:geometry}
We represent our inputs as vectors in high dimensional space. $y =
\pm1$, $x = (x_1, x_2, \cdots, x_n) \in \mathbf{R}^D$

\noi
\textbf{K-NN}: Look at $K$ nearest neighbors, label the new point as
the majority/mean of the neighbors. If $\inf$-NN, that's just taking the majority. This is
underfitting. $1$-NN would be overfitting. Choose odd $K$ so that
there won't be any ties.

\noi
\textbf{K-mediods Clustering}. 
\begin{enumerate}
\item Pick $K$ datapoints to be representatives
\item $\forall n$, put $n$ in the closest cluster (closest $K$ datapoints)
\item Choose a new prepresentative that minimizes the average dist to all
  cluster members
\item Go back to 2
\end{enumerate}
\noi
\textbf{K-means Clustering}. 
\begin{enumerate}
\item Pick $K$ datapoints randomly $\mu_1, \dots, \mu_k$
\item $\forall n$, $z_n = \arg\min_k |x_n - \mu_n|$. 
\item $\forall k$, $\mu_k = \frac{1}{n_k} \sum_{n; z_n = k}[x_n]$ (Make a new mean prepresentative who is an average of all cluster members.)
\item Go back to 2
\end{enumerate}
\noi
Does $K$-means always converge? 
Measure the quality of the solution by
the average distance from each point to its mean (score).
(Convergence as in this measure of cluster quality won't change.)\\
Observation 1:  Everytime step 2 or 3
is executed, the score goes down or stays the same. \\ 
Oberservation 2: If there are $n$ members and $k$ clusters, the
possible clustering assignment is finite, $k^n$.


\noi
Does $K$-means always find the optimal clustering?\\
No! Not necessarily. The initial point assignment is random and it
changes the result. The practical method is to do multiple
initialization and pick the smallest score.

\noi
How many iteration does it take to converge?\\
In theory it takes a time that's exponential to $k$. In practice, it
converges in like 4 iterations. Smooth analysis of why this converges
quickly. Only certain initial points take a long time. If it's taking a loong time, if you
perturbe the initial points and move it away (like how simplex is
really fast in real life).

$K=n$ ($\hat\eps=0$) or $K=1$ gives us no information about the data. As an implementation
note, some cluster centers will disappear and never come back. So be able to handle that situation.

\subsection{Curse of High Dimentionality}
\label{sec:curseofhighdimentionality}

as we make the dimention higher, the distance between points get
smaller. If every point is about the same distance from everything
else, the $k$-nn are just random points. Picking the majority of those
random points, is just another random guessing. As $D$ increases, the
distance between points concentrates to a point.
\begin{verbatim}
D = 100; hist(flatten(XtoD(rand(1000, D )) / sqrt(D)));
%XtoD takes the distance between points
\end{verbatim}

Seems like $K$-nn shouldn't work, but because the $D$ is uniform. It does work in real life,
because in real life the actual data is correlated.
\pagebreak
\section{September 15th Class 4}
\label{sec:class4}

\subsection{HW2}
\label{sec:hw2}

\begin{enumerate}
\item[2.] Decision boundary for a one nearest neighbor classifiers on
  two datapoints is a linear plane perpendicular to both points.
\item[3.] Clustering as in given labled data $(x_n, y_n)$, $\forall
  n\in N$ for each label run $K$-means on subset of data with this
  label, then run $KNN$ using $\mu_i$s. A lot faster on $KNN$ test
  time but more time on training,  in a sense clustering throws
  away bunch of data. This could avoid overfitting. (Assuming that
  we're working on relevant feature set).
\item[4.] in 2D space, the dot product, $<\bar w,\bar x>$, projects
  all the points, $X$, onto $\bar w$ and makes it 1D.\\
If $y=sign(w\cdot x)$, is the decision rule, then the boundary $B$ is $\{x: w\cdot x =
0\}$. The disadvantage is that it assumes the decision boundary lies
on the origin. This won't work if all the points are positive. So now,
modify $y$ to $y=sign(w\cdot x + b)$, s.t. $B=\{x:w\cdot x =
-b\}$. Bias is always $-b$, as $b\rarr -\infty$ , nothing can be
classified as positive.\\
Claim: using bias is the same as not using the bias but adding a new
feature. i.e. $x$ or $<1,x>=\tilde x$, where $x$ uses $w$ (to get
$w\cot x + b$) and $\tilde
x$ uses $\tilde w = <b w>$ so it's the same thing i.e. $w\cot x + b =
\tilde w \cdot \tilde x = <b,w>\cdot<1,x>$. So we can always write as
if there's not bias.
\end{enumerate}

\subsection{Perceptron}
\label{sec:perceptron}
General algorithm
\begin{verbatim}
for each (x,y)
   //if error, do an update
   if y(w\cdot x + b) \le 0: or if sign(y) == sign(w\cdot x + b):
      w = w + yx
      b = b + y
\end{verbatim}
Usually people scale the update with some constant $\gam < 1$, the
learning rate, so that it won't be so influenced by $x$. (each single
error);

\emph{Question:} If you add $\gam$ does it change?? Doesn't really change anything,
because after one update, the scale of $w$ changes, but the direction
is the same. With a bias, it'll shift it a lot of a little. 
After any number of updates, without using the learning rate, you'll
get $(w,b)$ in the end but with $\gamma$, all you'll get is $(\gam w,
\gam b)$., the decision boundary will have the same direction.\\
In the case of perceptrons, adding a $\gam$ makes no difference
(assuming we have infinite floating point). In the case that $w$ is a
unit vector, $b$ shifts the decision boundary $b$ units, if $w$ is not
unit, it shifts $b/||w||$ units. So if $||w||>>$, $b$ won't really
make that much difference.\\
More important point: the more you run, the less sentisitive the
algorithm will be for any update. Early on, the decision boundary
moves all over the place, but as the number of epoches increases, the
relative magnitude of the update is smaller than that of the beginning
updates. I.e. even if it won't converge, it'll slow down in the end.

\pagebreak
\section{September 22nd Class 6}
\label{sec:class6}

\subsection{Linear Seperability}
\label{sec:perceptron2}

Perceptrons can't do XOR, because 2D XOR is not linearly
seperable. But if you add a 3rd dimension, where $-$'s have 0 and
$+$'s have 1. Then you can have a hyperplane that seperates them.

You can add all possible combinations linearly, this is what
perceptrons do. But that's computationally expensive and makes the
feature very high dimension. This is prone to overfitting, and you
need more examples.

DT can combine few features but in a complicated way.\\
DT might fail on XOR because of all ties, features very slightly
correlated with the label..

\subsection{Evaluation/ROC}
\label{sec:roc}

Given a ratio of dataset where $1\%$ of them are positive and $99\%$
is negative, the classifier will say all of them are negative and
achieve $99\%$ accuracy.
Example for cancer detection, false negative is worse than false positive.
So there are different costs associated with different classification.
Perhaps my goal is to order the data to see what's interesting to look
at.

In biology, people look at \emph{Sensitivity vs Specificity}, we call
it \emph{Precision vs Recall}.
Precision says of all the $X$’s that you found, how many of them were
actually $X$’s? Recall asks: of all the $X$’s that were out there, how many
of them did you find? (should've found)

Choose the best threshold and get the harmonic mean, or their
\textbf{f-measure}:
$$F = \frac{2PR}{P+R}$$

\subsection{Cross-validation}
\label{sec:crossvalidation}

Get bunch of data, divide it in $n$ equal size. Say $n=10$, train on
$n_2\dots n_{10}$, and test on $n_1$, but now, you can train on $n_1
\dots n_9$, then test on $n_{10}$. You can do this $n$ times. Then
you'll get performance over all of the data I have, not just a single
slice.

Also you do this to tune your hyperparameters. For each hyperparameter
setting, you get the score of each, you want the hyperparameter that
does the best on average over all slices.

Say you run this on DT and get that depth = 17 is the best, so you
have $n$ trained classifier, get the one that does the best or,
retrain a new classifier on all $n-1$ training slices.
Usually learning a new classifier is a better idea, but it might be
slightly underfit, because you're regularizing (with depth=17), but
using more data.

4 choices: Number of slices $n=10$ if you're normal, you do $n=5$ if you're
running out of time, you do $n=2$, you'll make sure your two
classifiers aren't trained on the same data points, one more is train on everything except 1,
\emph{leave-one-out}, great for $knn$ but too expensive for others.
Leave-one-out is the least succeptable to underfitting.

\subsection{Statistical Significance}
\label{sec:statisticalsignificance}

Try to answer the question, ``given this new method, does it actually
work?''. You should use the most commen thing in your field. Two
examples we cover is \emph{Paired T-test} and \emph{Jacknifing/Bootstrapping}.

Given DT and Perceptron's performance, say perceptron always does better.
``If I were to do a new experiment, with what probability would
Perceptron beats DT''

This is what T-test does, fit gaussian to DT, fit gaussian to
Perceptron, and if the overlap is tiny, then there's small probability
that DT will beat perceptron, this gives us a $p$-value. 

Weakness is that there is a normal distribution assumption. LLN
argument, it should be kind of, but... not really because accuracy
lives between [0, 1], normal lives between $[-\infty, \infty]$.

So some people say that you should do binomial tests, but ppl still do
$p$-value.\\

\textbf{Jackknifing/Bootstrapping} (The word ``Bootstrapping'' is used
a lot everywhere). This is suited where your evaluation method is
holistic over the tneir data. Like $F$-measure.. Way around when T-test can't really be
applied. Doesn't make normality assumption, it's a
\emph{non-parametric} test.

If you have a big dataset, it's easier to seperate the two gaussian
curves (because they'll have more peaks), so you can get more
examples to get more statistical significance.

\pagebreak

\section{September 27th Class 7}
\label{sec:class7}

\subsection{HW3}
\label{sec:hw3}

\begin{enumerate}
\item About non-linearly seperable data and perceptron: Do the add one
  feature augmentation trick, why is this linearly seperable now? For
  example, $\vec w$ $D$ zeros and all indicators as corresponding
  $y_n$ the label.\\
Does this affect generalization? The above construction will not work
for test data at all. Generally it overfits, because it's like
cheating after the first couple of passes when you start using the
labels $y_n$ as the components of your new augmented $\vec w$\\
How long does it take to converge on this augmented data? Previously,
$||x||\le R$, but now, we have $||<x, 0,0,\dots,1,\dots>||?$. Square
it:
\begin{align*}
  ||<x, 0,0,\dots,1,\dots>||^2 &= ||<x,
  \dots,1,\dots>\dot<x,\dots,1,\dots>||\\
&= x\dot x + 1\\
&=||x||^2 + 1
\end{align*}
So $\hat R = \sqrt{R^2+1}$, but now $\gamma$ changes also. Using the
$\vec w$ above, the margin for this $\vec w$ is $\min_{(x,y)\in
  D}\frac{y}{||w||}w\dot x$
$||w|| = \sqrt{N}$, and max $w\dot x$ is $1$ so the new margin is at
least $\frac{1}{\sqrt{N}}$
\item Centering, Variance scaling vs DT, KNN, perceptron:\\
  \begin{itemize}
  \item \textbf{KNN} is not sensitive to centering but it is sensitive to
    variance scaling.
  \item \textbf{DT} is not sensitive to centering or scaling because the cut
    off is still the cut off.
  \item \textbf{Perceptron} is sensitive to centring and variance scaling, because if the
    feature value is huge, it'll move $\vec w$ a lot to one direction,
    it'll take more time because $R$ is huge. 
  \end{itemize}
\end{enumerate}

\subsection{Blue Box}
\label{sec:bluebox}

\begin{itemize}
\item In the realm of \emph{scale} of features, variance pruning might
  not work if done prior to scaling because it might be a good feature
  in the correct scale.
\item \emph{How do you get a confidence out of a DT or KNN?} For DT
  you can calclulate the probability. Each node is a conditional
  probability. For KNN, we can calculate the average within $k$
  neighbors and get a percentage (with $K=6$, 3 might be $VE$, 3 might
  be $-VE$, so 50\% chance
\end{itemize}

\subsection{Multiclass Classification}
\label{sec:multiclass}

\textbf{Reduction-based ML}: Is multiclass harder than binary? If I
could do binary, then I'll use that to solve multiclass.

\emph{Error-bounds}: If I can achieve $\eps$ error in binary
classification, we can have $\le f(\eps)$ error on multiclass
classification.

The big assumption is that we can achieve $\eps$ error.

\textbf{Definition:} Binary Classification.
\begin{itemize}
\item Assume some distribution $\mathbf{D}$ of $(x,y)$ pairs
\item get sample $D$ $(x_1,y_1),\dots,(x_n,y_n)$ drawn from $\mathbf{D}$
\item The goal is to compute $f:X\rarr \{-1, +1\}$
  s.t. $\mathbf{E}_{(x,y)~D}[f(x)\neq y]$ is small.
\end{itemize}

\textbf{Weighted binary classification}: Weight the negative sample less and
positive samples more, so that the classifier has to put more effort
to classify positive examples correctly. Now the goal is different it
is to minimize (arbitrarly assuming that possitive error costs more here):
$$\mathbf{E}_{(x,y)~D}
\begin{cases}
  \ah[f(x)\neq  1]& \text{ if y=1}\\
[f(x)\neq -1] &  \text{ if y=-1}\\
\end{cases}
$$
For some $\ah > 0 \in \mathbf{R}$. Above can be re-written as
$\mathbf{E}_{(x,y)~D}[\ah^{y=1}[f(x)\neq y]$
Algorithm to use: \textbf{Subsampling}. We end up with a ``balanced''
data. If the cost is $\ah=10$, I want 10th as many positive
examples. This is optimal for us.
What we want
\begin{enumerate}
\item given $\mathbf{D}^{WBC}$, we want to reduce it to $\mathbf{D}^{BC}$
\item learn a function $f$ on the binary classification problem.
\item given a test point $\hat x$, use $f$ to solve the weighted
  binary classification problem.
\end{enumerate}
The algorithm
\begin{enumerate}
\item[1a] Retain all positive examples
\item[1b] For each negative examples, sample $1/\ah$ of them.
\item[2]  $\hat y= f(\hat x)$ (i.e. 100 exmples, 98 -ve, 2ve, $\ah=2$,
  randomly select 49 negative examples.
\end{enumerate}
We want $\mathbf{E}_{(x,y)~D^{WBC}}[\ah^{y=1}[f(x)\neq y]]$, from
weighted $D$.
\begin{align*}
  \mathbf{E}_{(x,y)~D^{WBC}}[\ah^{y=1}[f(x)\neq y]]&=
 \sum_{(x,y)}\mathbf{D^{WBC}}(x,y)[\ah^{y=1}[f(x)\neq y]]\\
 &= \sum_x \begin{cases} \mathbf{D^{WBC}}(x, +1)[\ah [f(x)\neq 1]]& \\
     \mathbf{D^{WBC}}(x, -1)[f(x)\neq -1]\end{cases}\\
 &= \sum_x \begin{cases} \mathbf{D^{BC}}(x, +1)[\ah [f(x)\neq 1]]\\
     \ah\mathbf{D^{BC}}(x, -1)[f(x)\neq -1]\end{cases}\\
 &= \ah \sum_x\begin{cases} \mathbf{D^{BC}}(x, +1)[[f(x)\neq 1]]\\
     \mathbf{D^{BC}}(x, -1)[f(x)\neq -1]\end{cases}\\
&=\ah \eps
\end{align*}
Because we downselected negative examples, so to go from $D^{WBC}$ to
$D^{BC}$, we need to multiply it with $\ah$. 

Can we do better than $\ah \eps$ (on the weighted problem)? No, the
error grows linearly with the size of $\ah$. You could imagine to do
slightly better because of the sampling more of +ve but something
around there.

Sampling seems wasteful. SO instead, we can keep all negative
examples and put $\ah$ times more positive examples (replacement
of). This is \textbf{over-sampling}. Both gives us the same bounds,
but $\eps$ can be different. $\eps$ is the error rate of the BC
classifier. The big assumption is that we can achieve $\eps$, it's
easier to achive low $\eps$ in oversampling than undersampling. \emph{Even
though the bound for both sampling is $\ah\eps$, $\eps$ is smaller
from oversampling, generally.}

\subsection{Turning MC to BC}
\label{sec:MC2BC}

\begin{enumerate}
\item \textbf{OVA} (one-vs-all): with $k$ classifiers $f_1\dots f_k$, where $f_1$ seperates
  $y=1$ from $y\neq 1$ and $f_k$ seperates $y=k$ from $y\neq k$. Two
  bad things can happen:no one says yes, more than one says yes. Just
  pick at random for each, but these are the cases when the error
  occurs. Not great because of this. In practice, you pick the one
  with the highest confidence. Called brittle, because it takes only
  one error will shoot up the probability of error on MC. Any errors
 can cause grave harm, if each error is $\eps$, and you have $k$
 chances, it's pretty bad.
\item \textbf{AVA} (all-vs-all): $ k \choose 2 $ classifices where
  $f_{ij}$ seperates class $i$ from $j$.
\end{enumerate}

\pagebreak

\section{September 29th Class 8}
\label{sec:class8}

\subsection{HW 4}
\label{sec:hw4}

\begin{enumerate}
\item Task for squared error regression: Given: input space $\mathcal{X}$
  and an unknown distribution $\mathcal{D}$ over $\mathcal{X}\times\mathbf{R}$, Compute $f:X\rarr
  \mathbf{R}$, minimize $\mathbf{E}_{(x,y)~\mathcal{D}}[f(x)\neq y]$
\item \emph{To what types of error do these theorems apply?} All error
  in respect to data taken from the same distribution $\mathcal{D}$.
\item OVA vs AVA. If training for each classifier is $O(N)$, OVA:
  $O(NK)$, AVA:$O(K^2N/K) = O(K)$, if training for each classifier is
  $O(N^2)$, OVA: $O(N^2K)$, AVA: $O(K^2(N/K)^2)=O(N^2)$, so AVA is
  better! so the worst it is to train, the better and better AVA gets.
\item A ranking preference function $\omega$ that penalizes
  mispredictions linearly up to $K$: 
$$\omega(x,y) =
\begin{cases}
  min\{k, |i-j|\} & \text{ if $i\neq j$}\\
  0 & \text{ else}
\end{cases}$$
\end{enumerate}

\subsection{Continue on Multiclass classification}
\label{sec:multiclass}

There's OVA, AVA(the default), and \textbf{Tree-based}
classification. Idea is to divide and conquer, split all the classes
in two, then keep on splitting. At the leaf we make decisions. So if
we have $K$ classes, we have $K-1$ nodes and so we train $K-1$
classifiers. This divides the data, so we beat OVA's $O(K\eps^8)$, $K$
factor.

As soon as we make an error, we're done. The chance of making an error
is $\eps^b$, the number of times you can make an error is how many
nodes you go down (pf union bound: $P(A_1\cup A_2\cup \dots A_n) \le
\sum P(A_i)$, so at most the error caused by tree based is
$$\eps^{MC}\le O(\eps^blog(K))$$
If the number of features is not a power of 2, it might not be a full
binary tree with height $log(K)$, so more of the emphasis on $\le$.

But here we can't specify which order/the layout of the tree. Certain
layouts could make making a binary classifier easier. I.e. if data
were clustered in $1:(0,0), 2:(0,1), 3:(1,0), 4:(1,1)$, shouldn't
split classes $\{1,4\} vs \{2,3\}$.

\subsection{Ranking}
\label{sec:ranking}
\textbf{Bipartite Ranking}: Some are labeled good and some are
bad. The job is to rank the good, relevant ones before the bad
ones. Given nodes $a,b$, the labels is $1$ is $a$ is good and comes before $b$ and
should, $-1$ if $b$ comes before $a$.\\
We use $\omega(x,y)$ as the preference function just like binary classification's loss
function. Where $x$ is the true position, $y$ is my ranking.

\textbf{Objective}
\emph{Given} an input space $\mathcal{X}$, and an unknown distribution
$\mathbf{D}$ over $\mathcal{X}$, \emph{compute:} a function
$f:\mathcal{X}\rarr \Sigma_M$ minimizing $$\mathbf{E[\sum_{u\neq
    v}[sig_u<sig_v][\hat \sig_u < \hat \sig_v]\omega(\sig_u,\sig_v)}$$
Where $\hat sig = f(x)$. Basically did we swap $u$ adn $v$, if so, pay
$\omega(u,v)$.
$$f(x_{uv}) \begin{cases}
  +1& \text{ if } u<v\\
-1&\text{ if }v < u
\end{cases}$$
$f(x_{uv})$ answers ``is $u<_fv$?'' Now use this as a comparison
function and use any sorting algorithm, like quicksort. So it's just a
quicksort with a minor tweak:\\

Consider $f(x_{uv})$ as $P(u<v)$, so in quicksort, pick a pivot, is
this other number greater than the pivot or not? We answer this by
running $f$, and $f$ will tell us $x\%$ it's less. So it's like a
randomized quicksort.

So the weighted problem looks like:, given $(y=\{+1,-1\},x,w)$, (where
$w=\omega$, $x= x_{uv}$) we want to minimize
$$\eps^w = \mathbf{E}_{(x,y,w)~\mathcal{D}}[w(y\neq f(x))]$$
How would you solve this? Over sampling, add 2 more $(x,y)$ if
$w=2$. Undersampling, we want to keep if $w$ is high, so throw out
data with probability $1/w$. So if $w$ large, we most likely never
throw out the data. And we have $\eps^w \le \eps^b[\mathbf{E}_w[w]]$
is the best we can hope for.

\subsection{Collective Classification}
\label{sec:collectiveclassification}

Given a graph, where node is a feature, like a website, and if we were
to label each node as $+1$ not offensive, or $-1$ offensive, the idea
is that some information in the graph structure will tell us about the
label. i.e. offensive websites tend to link to offensive websites.

So train a classifier with a bigger feature, wher you include
information about the node itself plus the number of positive
neighbors and negative neighbors. Problem with this is that.......TODO!

The \textbf{Stacking algorithm}: Forget that we have a graph structure. First try
to classify each page based on its individual features alone. So
think: $f_0:\mathcal{X}\rarr \{+1, -1\}$, and now I have guesses for
all.
Now we can train a second round classifier, that takes an input from
the pages themselves and the neighbor information from the previous
classifier ( the new added info on the feautures are given by
$f_0$). So train $f_1:\mathcal{X}+f_0\text{'s output}\rarr
\{+1,-1\}$. Now, the hope is that $f_1$ wil get everything correct.
Now train $f_2$ that takes in $\mathcal{X}$ and $f_0$ and $f_1$'s
output. etc.

Problem: it never get's the true label of the neighbors, but only the
predicted data. This might be beneficial because at test time we never get to see
the true label of the neighbors. But the issue is overfitting: $f_0$
is trained and tested on the original data, so chances are it'll do
really good on the input data. So input to $f_1$ will be
unrealistically close to perfect. In the real world $f_0$ will do much
worse.

Concrete example: task for each 3D point, guess if it corresponds to a
tree or not, don't get info but angle and distance. It's nearly
impossible to classify the point just by using it's features, you
really need the entire graph structure. So at train time we do well,
but ... not in test..
\pagebreak
\section{Oct 4th Class 8}
\label{sec:class8}

\subsection{Linear Classifiers}
\label{sec:linear}

Linear decision boundary: $sign(\vec w \cdot \vec x + b)$. If $b=0$,
the plane passes through the origin. Computing the distance of this
plane from any point $\vec z$, is $\frac{\vec w \cdot \vec z +
  b}{||\vec w||}$.
 So from the origin is $\frac{b}{||\vec w||}$. If margin < 0,
loss is 1, if margin > 0, loss is 0.

Indicator function $\mathcal{1}(\cdot):
\begin{cases}
  1 & \text{ if argument is true }\\ 0
\end{cases}
$ Using 0-1 loss finding the decision boundary is NP-hard.

\subsection{Regularization}
\label{sec:regularization}

We're concerned about test error, not training error. Just minimizing
the training error will overfit. So Regularized Learning
(Tikhonov($\lam R(\vec w)$),
Ivanov(minimize the loss with constraints that $R(\vec w) \le \del$), Morozov
(minimize the regularizer with constraints on the loss function $le \del$)) All
three do the same thing, but most common is Tikhonov:
$$w = argmin_{w,b} \frac{1}{N}\sum_n\mathcal{1}(y_n(\vec w\cdot \vec
x_n + b) < 0) + \lam R(\vec w)$$
This balances empirical loss and complexity of the classifier -
prefers simpler one. $R(\vec w)$ is the regularizer for linear hyperplanes.
We want both the loss and regularizaer to be convex.

\subsection{Convexity}
\label{sec:convexity}

\textbf{Definition:}A set $S$ is convex if $\forall x,y\in S, \ah x + (1-\ah)y \in S$ where
$\ah\in [0,1]$. THe line segment joining $x$ and $y$ is contained in
$S$.
$S=\{x:x^2 > 2\}$ is non-convex because $x\ge \pm\sqrt{2}$, inside
it's empty
$S=\{x: \vec x^T \vec x < 1\}$. Show in two points norm will always be
< 1 with $\ah$
$S=\{U: U^T U = 1\}$, orthogonal matrices, is\emph{ not }convex. So
eigenvalue problems are non-convex.

Operations that preserve convexity:
\begin{itemize}
\item Intersetion
\item Affine function $f(x)=Ax+b)$
\end{itemize}

\textbf{Definition:} $f$ defined on a \emph{convex} set is a
\emph{convex} if 
$$f(\ah x + (1-\ah)y) \le \ah f(x) + (1-\ah)f(y) \forall \ah \in
[0,1]$$
The line is always above the function. Or equivalently, $f$ is convex if its \emph{epigraph} is a convex set:
$$E=\{(x,\mu):\mu \in \mathbf{R}, f(x)\le \mu\}$$ (region(set of points) that lies
above the graph of the function f.

Also, $f$ convex $\Rightarrow$ $-f$ concave.

Checking for convexity: First check if the domain is convex
\begin{enumerate}
\item Use definition (chord lies above hte function)
\item For differentiable $f$, function lies above all tangents i.e.
$$f(y) \ge f(x) + f'(x)(y-x)$$
\item For twice differentiable $f$, second derivative is
  non-negative. (These are positive definite matrices)
\end{enumerate}
If the inequalities are strict, then $f$ is \emph{stritctly convex}.
Operations that preserve convexity:
\begin{itemize}
\item Positive scaling/addition
\item Affine function composition: $f$ convex $\rarr$ $f(Ax+b)$ convex
\item Pointwise maximum: $f,g,$ convex $\rarr h(x) = max(f(x),g(x))$
  is convex
\item $f$ convex, $g$ convex non-decreasing => $h(x)=g(f(x))$ convex
\item $f$ concave, $g$ convex non-increasing $\rarr h(x) =g(f(x))$ convex

\end{itemize}
$exp$ good example of strictly convex, non-decreasing
Examples:
\begin{itemize}
\item $e^x,x,x^2,x^4$ take the second derivati
\item Any vector or matrix norm
\item $h(x) = max(|x|,x^2), x\in \mathbf{R}$
\item $exp(f(x))$, convex, by the 4th property
\item $\frac{1}{log(x)}, x>0$. $log(x)$ is concave, $1/x$
  is convex non-increasing in $\mathbf{R^+}$, so by 5 this is convex.
\end{itemize}

\subsubsection{Convex Loss Functions}
\label{sec:convexlossfunctions}

Hinge loss, exponential loss, and logistic loss
\begin{itemize}
\item Hinge loss: $L(y,\hat y) = max\{0, 1-y\hat y\}$
\item Exponential loss: $L(y,\hat y) = exp(-y\hat y)$ 
\item Logistic Loss:$L(y,\hat y) = log_2(1+exp(-y\hat y)$ 
\end{itemize}
Even if margin  is good, won't get 0 for exponential and logistic.

\subsection{Weight Regularization}
Why?
\begin{itemize}

\item  We need our weights to be small i.e. we DON'T want $\eps$ change in
input($\hat x=(x+\eps)$) to make $\hat y$ chnage a lot i.e. $\vec
w\eps$ will have a lot of influence if $||\vec w|| >>$.
\item Regularization is generalization, regularized learning is
  statistically stable and bounds generalization error. Stability here
  means that differences in $\eps$ is bounded..
\item We want regularizer to be convex for computational reasons
\end{itemize}

\subsubsection{Norm based Regularizers}
$l_p$ norm: $$||w||_p = (\sum_i|w_i|^p)^{1/p}, p\in[1,\infty]$$
if $p<1$, it won't satisfy triangle inequality and convexity.
We use $l_1$,$l_2$ usually. 
A unit ball in $l_p$ is a set $S=\{w:||w||_p \le 1\}$, so in $l_2$ it
will be a unit circle, with $l_1$, it will be a unit square rotated 90
degrees. As $p\rarr 0$, the sides approach the axis, $l_0$ ball looks
like a plus sign. As $p\rarr \infty$ the circle approaches
square. ($l_\infty(\vec w) = max \vec w$).

Properties:
\begin{itemize}
\item Solution often lies on the singularity (corners) of the ball
  when constrained enough. Why? (Why $l_1$ gives us sparsity, because
  the corners/edges are 4 or constant)
\item Rotational Invariance: $l_2$ is invariant to rotations
\item When to use $l_1$: less number of samples, lot of irrelevant features
\item When to use $l_2$: otherwise, leads to good generalization.
\end{itemize}

Now we our final objective is:
$$L(\vec w, b) = \frac{1}{N}\sum_n l(\hat y, y_n) +
\frac{\lam}{2}||w||_2^2$$

\subsubsection{Gradient Descent}
\label{sec:graddescent}
Gradient of a function $$\bigtriangledown f(x) = (\frac{\partial
  f(x)}{\partial x_1}, \frac{\partial
  f(x)}{\partial x_2},\dots, \frac{\partial
  f(x)}{\partial x_n}$$

\textbf{Directional derivative} Rate of change o fcuntion along a
direction $\vec v$. $D_v(f) = \bigtriangledown f\cdot \vec v$
The function increases the most along the direction of the
gradient. Why? because $\bigtriangledown f\cdot \vec v =
cos(\theta)|\bigtriangledown||\vec v|$, when the angle is 0,
$cos(0)=1$ and gives us the biggest increase.


Geometric interpretation:
\begin{itemize}
\item \textbf{Level sets}: for function $f(x)$, the level set
  containing a point $a$ is a set of points $S=\{\vec x: f(\vec x) =
  f(\vec a)\}$. Function admits same value at all points in the level
  set $S$
\item Gradient at a point $\vec y$ is perpendicular to the level set
  containing $\vec y$. i.e. a gradient at point $(x,y)$ will be
  perpendicular ot the level set going through $(x,y)$. (from
  definition, gradient is always normal to the tangent going through
  the point)
\item i.e. $f(x,y) = \sqrt{x^2 + y^2}$, level set is a cone, for
  $f(x,y,z) = \sqrt{x^2 + y^2+z^2}$, the level set is a sphere.
\end{itemize}

\textbf{Gradient Descent}:
\begin{itemize}
\item Move in the negative direction of gradient to minimize a
  function.
\item Steepest radient descent: To minimize a function $f$, start wiht
  some initial guess $x^(0)$ with update rule:
$$x^{(t)} = x^{(t-1)} -\eta^{(t)}\bigtriangledown f(x^{(t-1)})$$
\end{itemize}
\pagebreak

\section{October 11th Class 10}
\label{sec:class10}

\subsection{Probabilistic Modeling}

\textbf{Joint:}$p(x,y)$, \textbf{Conditional}: $p(y|x)$ (some ppl
think this is the discriminative).

\subsubsection{Joint Modeling Setup}
\label{sec:joint}

Assumption $\mathcal{D}~(x,y)$, $f(\hat x) = argmax_y \mathcal{D}(\hat
x, y)$ (pick the most likely thing). If there's unique $y$ for every
$x$ with probability 1 there will be no error.

Training Data: $(x_n, y_n)_n ~ \mathcal{D}$, we want to estimate $\hat
p(x,y)\approx \mathcal{D}$, where $f(\hat x) = argmax_y \hat p(\hat x,
y)$

Typical Distributions:
\begin{itemize}
\item For labels with $\{1, -1\}$ (coin flips), when $p(h)=\beta$,
  $p(t) =1-\beta$, use \emph{Bernoulli} distribution.
\item For labels in the reals, people use \emph{Gaussians}.
\end{itemize}

People approach joint modeling by factorizing $p(x,y)$ into
$p(y)p(x|y)$ (chain rule). Backwards of what you think about
classification. First decide if there's a bird or not, then draw the
picture depending on if i decided if there's a bird or not.

$x$s are usually feature vectors, so it's sometimes to come up with a
good model over $p(\vec x| y)$.

Example: label: play or not, features: wind, sunny, temp => many
combinations. For each, given $Y$, assign probabilities to each
combinations. where sum of all combinations is 1. But when the number
of features grow, it's just a lot of combinations and there might be
not enough data.

\textbf{Naive Bayes Assumption}
Make smaller tables, condition on play for each feature, so
$p(W|P)=.6$, then $p(\not W|P)=.4$, and if $p(W|\not P)=.2$, then
$p(\not W|\not P)=.8$
Using $$p(x_1, \dots, x_n|y)=\prod_d p(x_d|x_1, \dots, x_{d-1}, y)
\approx \prod p(x_d|y)$$
i.e. \emph{Features are independent given the label.}

Note: In the real world it's clear that $p(temperature | outlook)\neq p(temp)$. But naive
bayes is not this bad, because $p(temp|outlook, play) = p(temp|play)$,
we are conditioning on the label. Not the best assumption but not
totally wrong.

So we have, $p(x,y) = p(y)p(x|y)\approx p(y)\prod_d p(x_d|y)$

Telling this fictional tale about where the data came from, the close
the tale is the better my model will be.

Probabilistic assumption = inductive bias.

\subsection{Primary BUilding model in Naive Bayes}

Short hand for if $x=1$, probability is $\theta$, else
$1-\theta$: $$\theta^{(x=1)}(1-\theta)^{(x\neq 1)}$$

\subsubsection{Parameter Estimation}: Goal is given data, estimate
$\mathcal{D}$. Best way is \textbf{Maximum Likelihood}
\begin{itemize}
\item Pick parameters $\thet$ that maximize $P_\thet(D)$ where that's
  the likelihood of $\thet$ (not likelihood of data, probability of data
  given $\thet$ ($P(D|\thet)$)).
\end{itemize}
Goal: Maximize $P((x_1,y_1), \dots,(x_n, y_n)|\thet)$
Make a first big assumption, that $x$s are i.i.d.
Then we get $max_\thet \prod_n p(x_n, y_n|\thet)$. The data isn't really
iid because yesterday it was raining so it probably affects your
samples.

Now using naive bayes, assume that we have 
\begin{align*}
&\approx max_\thet \prod_n p(y_n|\thet)\prod_d p(x_{n, d}|\thet, y_n)\\
&\approx max_\thet \prod_n \thet_0^{(y_n=1)}(1-\theta_0)^{(y_n\neq 1)}\prod_d \thet_{+1,d}^{(y_n=1, x_{n,d}=1)}(1-\theta_{+1,
  d})^{(y_n=1, x_{n,d}\neq 1)}\thet_{-1,d}^{(y_n\neq 1,
  x_{n,d}=1)}(1-\thet_{-1,d})^{(y_n\neq 1, x_{n,d}\neq 1)}
\end{align*}
($\thet_0$ is how many days i play)
Given Play=1, outlook=1 is $\thet_{+1,out}$, so  play=1, outlook=0 is
$(1-\thet)_{+1,-out}$, and so forth for each feature and it's label
combination.
So $\thet_{+1,d}^{(y_n=1, x_{n,d}=1)}$ is $\thet_{+1, out}$.
(with D many features, we
get 2D+1 coins and $\thet_0$ is the guy who choses the label, then for
each feature you'll get $2D$ possibilities) 

\emph{examples:}
Given 50+ve, 150 -ve examples, $\thet_0 = 50/(50+150) = 1/4 = p(y)$.
Take 50+ve ex and look at the outlook feature. Say 30 have outlook=1
and 20 have outlook=0. This is $\thet_{+1, outlook} = 30/50 = 3/5$. So
training naive bayes is very easy, all you need to do is count. Very
fast.

\emph{Pf:}
Just talking about labels so $\thet_0=\thet$
Super easy case, get bunch of data
$p(y_1, \dots, y_{200}) = \prod_n p(y_n) = \prod_n
\thet^{(y_n=1)}(1-\thet)^{(y_n\neq 1)}$
Now take the log so that it's easier to maximize. We can take the log
because it's monotonically increasing.
So:
\begin{align*}
 &= \prod_n\thet^{(y_n=1)}(1-\thet)^{(y_n\neq 1)}\\
&= \sum_n([y_n=1]log(\thet) + [y_n\neq 1] log(1-\thet))\\
\frac{\partial}{\partial \thet}&= \sum_n( \frac{y_n=1}{\thet} +
\frac{-[y_n\neq 1]}{(1-\thet)}) = 0\\
&= \frac{1}{\thet}\sum[y_n=1] &= \frac{1}{1-\thet}\sum [y_n\neq 1]\\
(1-\thet)\sum[y_n=1] &=\theta \sum[y_n\neq 1]\\
\sum_n[y_n=1]&= \theta(\sum[y_n \neq 1] + \sum [y_n=1])\\
\theta &= \frac{\sum_n[y_n=1}{N}
\end{align*}
This is what we wanted!!!

\subsubsection{a}
Objective: 
$$\max_\thet \prod_\thet \prod_k \thet_k^{[y_n=k]} \text{ subject to
  }\sum_k \theta_k = 1$$
Now rolling a die instead of the coin. Without the constraints,
$\theta_k$ will go to infinity. Take the log of that and
get $$g(\theta) = \sum_n\sum_k[y_n=k]log(\theta_k) \text{ subject to
} \sum_k\theta_k-1 = 0$$
This is concave, with the restriction, it's still concave around the
line that sums ot one (y=-1), it's concave in the whole space and over
my restriction, so if we maximize we'll get the the most.

Now construct the lagurangian $\mathcal{L}(\thet, \lam)$
For every constraint, add a $\lambda$, a lagurange multiplier. So here we only have
one constraints and we get
$$\mathcal{L}(\thet, \lam) = g(\thet) + \lam(\sum_k\theta_k - 1)$$
Then, 
$$\max_\thet \min_\lam \mathcal{L}(\thet, \lam) = \max_\theta
g(\thet) \text{ subject to constraints}$$
(What ever i'm trying to do $\lam$ tries to do the opposite.)
Now we can do $g(\theta)$ without the constraints by solving
$\min_\lam \max_\thet \mathcal{L}(\thet, \lam)$.

Idea: All i care is making $g(\thet)$ big. But then there's $\lam$
who's trying to make $\mathcal{L}$ small. Say I find $\sum_n \thet_n =
2$, and makes $g(\thet)$ happy. But then $\lam$ will be set to
$-\infty$, because $\sum_n\thet_n - 1 = 1$, so now $\lam$ has full
control on making $\mathcal{L}$ small.
If $\sum_n \thet_n = 0.5$, then $\sum_n \thet_n -1 = -0.5$, so $\lam$
will go to $\infty$. The only thing I can do is to actually satisfy my
constraints $\sum_n  \thet_n -1 = 0$. i.e. if we get $-\infty$, that
means we didn't satisfy the constraints, if we do get something other
than that we know we've satisfied it.
\pagebreak

\section{Class 11 October 13th}
\label{sec:class11}

HW
\begin{enumerate}
\item $$\max_\theta \prod_d \theta_d^{x_d} s.t. \sum_d \theta_d^2 = 1,
  (\theta_d\le 0)$$ $\theta_d^2 \le 0$ will always happen but
  technically need to include it. Since we're maximizing, we want to
  insure that the function is concave, and log is concave (second
  derivative should be non-positive). Make sure to check this! The
  overall goal is to $\max_\thet$ and $\lam$ plays the other role. Take the log:
  \begin{align*}
    &=\max_\thet \sum_d x_d log(\theta_d) s.t. \sum_d \theta_d^2 = 1\\
\mathcal{L}(\thet, \lam)&=\sum_d x_d log(\theta_d)+ \lam(\thet_d^2
-1)\\
&=\max_\thet \min_\lam \mathcal{L}(\thet, \lam)
  \end{align*}
The order of $\max_\thet \min_\lam$: I pick $\thet$ then $\lam$ picks,
if I pick $\thet$ that doesn't satisfy the constraint, $\lam$ will
pick $-\infty$. If we had $\min_\lam \max_\thet$, $\lam$ picks $\le 0$, then
$\theta$ will be $\infty$, if $\lam > 0$, I will still pick bad
$\theta$... You need to chose $\thet$ first, $\min_\lam
\mathcal{L}(\thet, \lam)$ is a function of $\theta$, to figure out
what $\lam$ is, need to choose $\thet$ first. That's why the order
matters..? (unresolved) Whatever, pick $\theta$ first!
Now differentiate with respect to $\thet_d$
\begin{align*}
  \partial \mathcal{L}/\partial \thet_d &= x_d/\thet_d + 2\lam
  \thet_d\\
-x_d/\thet_d &= 2\lam \thet_d\\
2\lam \thet_d^2 &= - x_d\\
\thet_D &= \sqrt{\frac{-x_d}{2\lam}}
\end{align*}
Now solve for $\lam$. Substitute the constraint $\sum_d\thet_d^2 = 1$
\begin{align*}
  \sum_d \frac{x_d}{2\lam}&= \frac{-1}{2\lam}\sum_{d}x_d = 1\\
\lam &= -1/2 \sum_d x_d
\end{align*}
This means, $\thet_d = \sqrt(\frac{x_d}{\sum_d x_d})$. Need to make
sure that the positive solution is the right solution (unless there's
a constraint that $\thet_d \le 0$.
\end{enumerate}

\subsection{Maximum Likelihood}
\label{sec:maxli}

Just to remind ourselves, we were trying to maximize the joint
probability $p(x,y)$ i.e. $f(x) = \arg\max_y p(x,y)$.
The generative story is:
\begin{enumerate}
\item For each example $n=1\dots N$
  \begin{enumerate}
  \item choose $y_n\sim Ber(\beta)$
  \item for each $d=1\dots D$, choose $x_{n,d} \sim N(\mu_{y_n,d},
    \sig_{y_n,d}^2)$ or $Ber(\thet_{y_n,d})$ any reasonable
    distribution. For each class and each feature there is a
    distribution.
  \item 
  \end{enumerate}
\end{enumerate}
This says if I know my model and the parameters, $\mu$s and $\sig$, we can, so the idea
is to reverse engineer these parameters.
Example in matlab:
\begin{verbatim}
y = rand < 0.6
mu = zeros(2); mu(1,1) = 8; % where 1 is pos, 2 is neg, give random number
mu(1,2) = 11; mu(2, 1) = -4;  mu(2,2) = 10;
mu(y+1, :) % ans = 8 11 
X = repmat(mu(y+1, :), 100, 1) + randn(100,2))
X2 = repmat(mu(2, :), 100, 1) + randn(100,2))
plot(X(:,1), X(:,2), 'b.') hold on;
plot(X2(:,1), X2(:,2), 'r*')
mean(X) % solution/estimate, gaussian with these two mean 
mean(X2) % => pretty close to the actual mean 
\end{verbatim}

Turns out that if we want the decision boundary, it just becomes the
linear classification. 
\textbf{LLR}: 
Re-write as $\log \frac{p(y=+1|x}{p(y=-1|x} = \vec x \cdot \vec w +
b$. The decision boundary is just linear. This is assuming bernoulli
model on the features. If we did the gaussian case, it works out
too. But not with any distribution. Point is that this is a linear
model. The parameters of the model $\thet_{\pm 1, d}$ you can  compute
by counting. (If gaussian model, find the $\mu,\sig$ of data for each
feature and each label). Constrasting with perceptron or other iterative
classifiers, this we get the closed form.\\
We have a linear model and closed form solution, but there's no
guarantee about seperability or large margins.

\subsection{Generative vs Conditional}
Generative is trying to model $p(x,y)$, but all we care about is
$p(y|x)$ looks like. This is the conditional model.
The big question in the generative was what probability distribution
do we chose, then estimate the parameters. Same in conditional model,
choose a distribution for $p(y|x)$, then estimate the parameters.

We like linear models, so maybe we want $\vec w \cdot \vec x + b$,
wehre $\vec w$ is the parameters of the model. Since this is in the
$\mathbf{R}$, we need to squish it down to interval $[0,1]$
$$\text{sigmoid: } \sig(z) = \frac{1}{1+e^{-z}}$$
If $wx+b>>$, $\sig = 1$, if $wx+b = 0$, $\sig = 0.5$, if $xw+b << $,
$\sig = 0$. 
So we have $$p(y=+1|x) = \sig(\vec w \cdot \vec x + b)$$
$$p(y=-1|x) = 1-\sig(\vec w \cdot \vec x + b) = \sig(-(\vec w \cdot \vec x + b))$$ Since $1-\sig(z) =
\sig(-z)$ (symetric property on sig)
So $$p(y|x) = \sig(y(\vec w \cdot \vec x + b))$$
We're saying the distribution over the label is a function of the
margin, which makes sense. Any function that maps $[-\infty, \infty]$
to $[0,1]$ is good.

What we really care about is $\log(p(y|x)) = \log \sig(y(wx+b))$
Since $\log(\sig(z)) = -log(1+e^{-z})$, 
$$\log p(y|x) = -\log (1+e^{-y(wx+b)}$$ The \emph{logistic loss
  function}!!

So if you optimize a linear classifier under logistic loss function,
it's the same as optimizing $p(y|x)$.

\subsection{Discriminative Models:SVM}
Generative and Conditional models are always probabilistic, for
Discriminative there's no probability. We're explicitly optimizing the
decision boundry for discriminative.
Conditional models optimizes the probability of label given data.
Generative models optimizes predicting both the label and the data.

If you care about probabilities, you should do generative, conditional
models. When to prefer either or? 

\subsubsection{Current State of Generative vs Conditional}
\begin{enumerate}
\item Assume the generative model is ``correct''. (Tell the story
  about how the data got generated and it's actually true, always
  false but have it for now). Then,
  \begin{enumerate}
  \item as $N\rarr \infty$, $\eps^{gen} \rarr \eps^{Bayes}$,
    $\eps^{cond} \rarr \eps^{Bayes}$ Asymptotic

  \item fininte $N$, $\eps^{gen} \rarr \eps^{Bayes}$ faster than  $\eps^{cond}
    \rarr \eps^{Bayes}$ 
  \end{enumerate}
\item Assume model not correct
  \begin{enumerate}
  \item as $N\rarr \infty$, $\eps^{gen} > \eps^{cond}$, Conditional is
    better because if the model is even slightly wrong, if we have
    infinite amount of data, it blows up.
  \item $N$ finite, $\eps^{gen} \rarr $ it's optimum faster than
    $\eps^{cond}\rarr$ it's optimum. We'll never get to
    $\eps^{Bayes}$. It's no longer asymptote at the Bayes line, and
    they asymptote at different places, and conditonal's optimum might
    be better, but generative goes to its optimum faster.
  \end{enumerate}
\end{enumerate}
Hall's heuristic is that if $N>2D$, use conditional, $N<D$ use generative.
Generative adds more information, where data comes from, to
the model. So when $N$ is small, it's very good. But when $N$ is
large, it relies too much on your inductive bias about the data.

Conditional and Discrimnative only makes sense for supervised.

\pagebreak

\section{Class 11 October 18th 2011}

\subsection{Lagrangian}
\emph{Goal:} $P(y|x)$, given examples $(x_i, y_i), i=1,\dots,N$ One
thing to do is want my feature's distribution to match the data. My
model has some $E_{(x,y)\tilde p}[y=1 \wedge sunny] =
\frac{1}{N}\sum_{(x_n,y_n)}[y=1 \wand sunny]$ 
i.e. want $E_P[f_i(x,y)]$ to match the empirical data. So:
$E_P[f_i(x,y)] = E_D[f_i(x,y)]$ $P$ what i'm going to
learn, $D$ is the training data.

if we store the training data and say $p(y|x) =
\begin{cases}
  1/N & \text{ if }(x,y)\in D\\
0 & \text{ otherwise}
\end{cases}$ it wil work, but this is overfitting.

Extreme case of underfitting is uniform distribution. Want something
in between, so want to match the data, but we also want $p(y|x)$ to be
uniform as possible.

\textbf{Entropy:} $H(q) = -\sum_z q(z)log(q(z))$

So we want to maximize of the entropy of $p$. so 
$$\max_{p(y|x)}-\sum_yp(y|x)\log(p(y|x)) s.t. E_P[f_i(x,y)] =
E_D[f_i(x,y)]\forall i$$
This is maximizing over a function, which is scary. But from ``Calculus of
variations'' that study functions over functions, the magic is that
everything we know holds.

Now remove the - to change max to min, and add lagrange multipliers
and get:
$$\min_{p(y|x)}\max_\lambda\sum_{x\in D}\sum_yp(y|x)\log p(y|x) + \sum_i \lambda_i[E_p(f_i) -
E_D(f_i)]$$
$E_p[f_i]$ is $\sum_{x\in D}\sum_y p(y|x)f_i(x,y)$, $E_D[f_i]$ is some
number.

Now take the derivative with respect to $p(y|x)$:
\begin{align*}
\frac{\partial}{\partial p(y|x)}&= \sum_{x\in D}\sum_{y}[\log p(y|x)
+ 1] + \sum_i\lambda_i\sum_{x\in D}\sum_yf_i(x,y) = 0\\  
\sum_{x\in D}\sum_{y}[\log p(y|x)
+ 1] &= - \sum_i\lambda_i\sum_{x\in D}\sum_yf_i(x,y)\\
\sum_{x\in D}\sum_{y}[\log p(y|x)
+ 1] &= - \sum_{x\in D}\sum_y\sum_i\lambda_if_i(x,y)\\
\end{align*}
Then we need to show $\log p(y|x) = -\sum_i \lam_if_i(x,y) - 1 \forall
x,y$
Now take the exponential:
\begin{align*}
  p(y|x) &= \exp[-\sum_i \lam_if_i(x,y) - 1 ] = \exp[-\sum_i
  \lam_if_i(x,y)]/e
\end{align*}
This is like $\sig(w\cdot x) = 1/z \exp(w\cdot x)$, the sigmoid linear
regression! it's linear model
again, where $\lam$s are the $\vec w$, even though we didn't start off
with a linearity expectation.

\subsection{Neural Networks}
\label{sec:neuralnetworks}
Dealing with non-linearlity..

Each of the feature of $x$ will be an input, and the goal is to get
$y\in \mathbf{R}$ from this. A linear model has weight on each edge
from the input to the last node, where the value of the last node is
the sum of $w_ix_i$. This can't solve XOR, so we introduce
\emph{hidden units}.

Connect each input to each hidden units, a fully connected graph,
where each edge will have a weight $w_{d,i}$, where it goes from input
$x_d$ to hidden node $i$, and hidden nodes will have weights $u_i$ on
edges that connect to the output unit.
$y=\sum_iu_i(\sum_dw_{d,i}x_d)$, but this is still linear! To
introduce non-linearity, we'll introduce a link function, $f$, usually
sigmoid, $f(z) = \tanh(z)$

Let's do XOR:
OR: $y=x_1 \vee x_2$, where $(1,1),(-1,1),(1,-1)\rarr  y=1$, and
$y=-1$ for $(-1,-1)$. Here you want the bias to be positive so that the
only way you can get -1 is if both are negative. Let $b=1000$, then if
$w_1=w_2=1000$, then we'll get the OR behavior.
Construct AND, then using OR and AND (with a bias), we can do XOR.

Three parts involved training, architecture selection, and selection
of link functions.

\subsubsection{Forward and Back propagation}
\label{sec:forwardback}
Forward: given $\vec x$, get $y$. 
Backward: $\partial L/\partial u $ is how much the error on my final prediction
changes as $u$ changes and so forth.

\subsubsection{Architecture}

\emph{Terminology:}
\begin{itemize}
\item  no hidden units $\rarr$ one layer network, the linear model, no xor
\item 1 set of hidden units $\rarr$ 2 layer network, the universal
  function approxmiator (essentially infinite representation power),
  but exponentially many hidden units.
\item ``deep'' ($\ge 3$), also a universal function approximator, the hope
  is fewer hidden units.
\end{itemize}




\end{document}
