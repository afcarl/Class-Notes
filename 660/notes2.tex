\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\kap}{\kappa}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\Rarr}{\Rightarrow}
\newcommand{\Larr}{\Leftarrow}

\newcommand{\ol}{\overline}
\newcommand{\dagg}{\dagger}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}
% for cross product
\newcommand{\lc}{\langle} %<
\newcommand{\rc}{\rangle} %>
% linear alg
\newcommand{\inv}{^{-1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
% order 
\newcommand{\cO}{\mathcal{O}}
% gradient
\newcommand{\grad}{\nabla}
\newcommand{\ddx}{\frac \partial {\partial x}}

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}
\parskip 5pt
\parindent 0pt

\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,algorithmic}
\begin{document}
\title{Scientific Computing CS660 Fall '11\\Notes after the midterm}
\author{Angjoo Kanazawa}
\maketitle
\section{Class 13 October 25th 2011}
\label{sec:class13}
\subsection{HW2 answers}
\textbf{Problem 1b}: to compute many $N$, compute $S_N = \phi(x_1) + \cdots +
\phi(x_n)$, take $N$ samples, divide by $N$ will give us $I_N$. THe
error is $E_N = I-I_N$. TO generate $I_{N+1}$, take $S_{N+1} = S_N +
\phi(x_{N+1})$ to compute $I_{N+1}$. But this way $E_{N+1}$ is not
independent from $E_N$. (Left top figure on the answers).

(Doing it entirely independently (how i did it) is the lower figure)
Plotted $\log_N$ vs $\log(\frac{\sig}{\sqrt{N}}$
The observation was that it somewhat follows the bound. 

Our bound, $\frac{\sig}{\sqrt{N}}$, comes from chebychev
$P(|\frac{S_N}{N} - \mu| \ge c) \le \frac{\sig^2}{Nc^2}$.
So if we want this $P$ to be 95\%, then $c\le
\frac{\sig}{\sqrt{0.5N}}$. So loosely speaking, the error $E_N = I-I_N
\approx \frac{\sig}{\sqrt{0.5N}}$. The point of the question was that
it's not exact.

On Page 3 of the solution, there's the definition of histogram for
question 1c.

\textbf{Problem 2}: 
In P1, the error was $E_N = \frac{\sig^2}{\sqrt{N}}$
If the variance is $\sig^2$, std $\sig$, then log of $E_N$ is $\log
\sig - \frac{\log(0.5N)}{2}$. 

In here, we expected the error to be $\frac{\sig^2}{\sqrt{N}}$. If
that were to happen, same derivation would give $\log(E_N)\approx
2\log(\sig) - \frac{\log(0.5N)}{2}$.
The blue line (boundary) would've been $2\log(\sig) -
\frac{\log(N)}{2}$ and $E_N$ will hover around there. But it didn't
work!! Because $sin(\pi X) = sin(\pi(1-X))$, nothing changed. Function
had to be monotone.

\textbf{midterm:}
Equally distributed, 3 topics: floating pt/round off errors, mc,
matrix factorization (big picture). (no newton's method, no machine
representation of numbers)

\subsection{Eigenvalue Hessemberg}
Why do we want to reduce the matrix to Hessemberg form: Because doing
$QR$ on Hessenberg is $\mathcal{O}(n^2)$ instead of
$\mathcal{O}(n^3)$.

How to make a hessemberg matrix:
$A$ is a big full matrix. Now $QA$ will make everything below the
second entry of first column to 0. You can do that with Householder
transformation, or givens rotation. 

However we do this, $QAQ^T$ will not mess up the 0s.
\begin{align*}
 QAQ^T &= [Q(QA)^T]^T\\
\end{align*}
Applying to $Q$ to $(QA)^T$ will leave the first row alone, so
$[Q(QA)^T]^T$ still keeps the first column's entry below 2 to 0.

Exercise, go back, do: can we perform $QAQ^T$ where $Q$ is a
householder transformation s.t. $QAQ^T$ is a first step hessemberg
matrix. The reason why this works is because householder is in the
$n-1$ subblock, so it doesn't do anything row 1. (It's like the 2nd
household transformation in the QR reduction)


$QR$ iteration for the eigenvalue problem:
\begin{enumerate}
\item[0] Reduce $A$ to hessenberg form, formally, $PAP^T = H$, where $P$
  orthogonal. (do $n-1$ householder transformation $P_2$).$A_0 = H$
\item[1-k] Iterate, $A_K = Q_KR_K$, $A_{K+1} = R_KQ_K$
\end{enumerate}
Doing this $A=QR$ is $\mathcal{O}(n^3)$, but to do this until
conversion (see $\eps$ in below diagonal, is $n$, so total
$\mathcal{O}(n^4)$ (without making $A$ into upper hessenberg).
If $A$ wer hessenberg, doing $QR$ is $\mathcal{O}(n^2)$ instead of
$\mathcal{O}(n^3)$, so the total cost is $\mathcal{O}(n^3) +
\mathcal{O}(nn^2) = \mathcal{O}(n^3)$ with hessemberg.

\pagebreak

\section{Class 14 November 1st}
\label{sec:class14}
Midterm: median 22, mean 20.8, max:29

\subsection{Midterm}

\textbf{Problem 1}:
Given $Ax=b$, $A\hat x = b + \delta b$: $A(x-\hat x) = b -b \delta b$
so 
\begin{align*}
x-\hat x &= A^{-1}(\delta b)\\
||x-\hat x|| &\le ||A^{-1}||||\delta b||\\
\end{align*}
And
\begin{align*}
  b&=Ax\\
||b|| &\le ||A|| ||x||\\
\frac{1}{||x||} &\le \frac{||A||}{||b||}\\
\end{align*}
So
$$
\frac{||x-\hat x||}{||x||} \le ||A|| ||A^{-1}|| \frac{||\delta b||}{||b||}
$$
\noi
\textbf{Problem 2}: 
\begin{enumerate}
\item[a] QR factorization by gram-schimid
\item[b] Making $r_{ik} = <\hat q_k, q_i>$ from $r_{ik} =<a_k, q_i>$,
  doesn't change anything. Because
  \begin{align*}
    r_{23} = \lc\hat q_3, q_2\rc \\
&= \lc a_3 - r_{13}, q_2\rc\\
&= \lc a_3, q_2 \rc - r_{13}\lc q_1, q_2 \rc
\end{align*}
\item[c] Real question is: Will $q_3$ be orthogonal to $q_2$ and
  $q_1$? Look at $\hat q_3$ after $i$th loop is done. $\hat q_3 = \lc
  a_3 - r_{13}q_1 - r_{23}q_2, q2 \rc = \lc a_3, q_2\rc - r_{13}\lc
  q_1, q_2\rc = -r_{23}\lc q_2, q_2 \rc$
Problem is $\lc q_1, q_2\rc$ could be non-zero because of round-off
error.
But if we did the second way, $\lc \hat q_3, q_2 \rc = \lc a_3 - r_{13}q_1, q_2 \rc -
r_{23}\lc q_3, q_2 \rc$, is a little bit less-sensitive to round-off
errors. Called the modified-Gram Schmid. (This can't be run parallel)
\end{enumerate}

\subsection{Singular Value Decomposition}
\label{sec:SVD}

Let $A\in \mathbf{R}^{m\times n}$. $A$ can be factored as $$A=U\Sigma
V^T$$, $U$ is $m$ by $m$, sigma is $m$ by $n$, a diagonal matrix, where
the entries are $sig_1, \dots, sig_n$, everywhere else 0. $sig_i$'s
are called the singular values, and $\sig_1 \le sig_2 \le \cdots \le
\sig_n \le 0$.
$V^T$ is $n$ by $n$. Where $U^TU = I_m$ and $V^TV = I_n$.

How to use:
\textbf{Least Squares}: minimize $||b-Ax||_2$, find $x$. 
  \begin{align*}
    ||b-Ax||_2^2 &= \lc b-Ax, b-Ax \rc \\
&= \lc b-U\Sigma V^Tx, b-U\Sigma V^Tx \rc \\
&\text{ let $\hat x = V^Tx$}\\
&= \lc U(U^Tb-\Sigma\hat x), U(U^Tb-\Sigma\hat x) \rc \\
&= \lc U^TU(U^Tb-\Sigma\hat x), U^TU(U^Tb-\Sigma\hat x) \rc \\
&= \lc U^Tb-\Sigma\hat x, U^Tb-\Sigma\hat x \rc \\
&\text{ let $\hat b = U^Tb$}\\
&= \lc \hat b-\Sigma\hat x,  \hat b-\Sigma\hat x \rc \\
&= || \hat b - \Sigma \hat x ||^2_2
  \end{align*}
This is $$
\begin{pmatrix}
 \hat b_1 \\ \hat b_2 \\ \vdots \\ \hat b_m
\end{pmatrix}
-
\begin{pmatrix}
 \sig_1 \hat x_1 \\ \vdots \\ \sig_n\hat x_n \\ 0 \\ \vdots 0
\end{pmatrix}
$$
So to minimize, set $\sig_k\hat x_k = \hat b_k$, or $$\hat x_k =
\frac{\hat b_k}{\sig_k}$$

The norm of minimum norm solution is $\sqrt{\hat b^2_{n+1} + \cdots +
  \hat b_m^2}$, provided $\sig_n > 0$. If all $\sig_j > 0$, then $\hat
x, x$ are unique. aka $A$ is of full rank. If some of them are zero,
then $\exists$ multiple solutions $\hat x$ and $x$. Rank of $A$ is the
index of smallest nonzero $\sig_j$s.

To solve this problem, compute $U, V$ and $\Sigma$, fing $\hat b$,
find $\hat x$, find $x=V\hat x$.

\textbf{Remember}: least squares solution can be found from $A^TAx =
A^Tb$:
\begin{align*}
  A^TAx &=A^Tb\\
V\Sigma^TU^T U\Sigma V^T x &= V \Sigma^T U^Tb\\
V[\Sig_1 0][\Sig_1; 0]V^T x&= V \Sigma^T U^Tb\\
V\Sig_1^2 V^T x &= V[Sig_1 0][U^Tb]\\
\Sig_1^2 V^Tx &=\Sig_1[U^Tb]\\
\end{align*}
This is just like the one before where $V^Tx = \hat x$,
$\hat b = U^Tb$.

\subsection{Pseudo-inverse}
Of a rectangular matrix $A$, is written:
$$A^\dagger = (A^TA)^{-1}A^T$$
$(A^TA)^{-1}$ is square times wide rectangle, so a wide rectangle. 
Property: $A^\dag A = (A^TA)^{-1}A^A = I$.

Given $A = U\Sig V^T$, 
\begin{align*}
  A &= U\Sig V^T\\
A^TA &= V\Sig_1^2 V^T\\
(A^TA)^{-1} &= (V\Sig_1^2 V^T)^{-1} = (V^T)^{-1}(\Sig_1^2)^{-1}V^{-1}\\
&=V\Sig_1^{-2}V^T\\
\end{align*}
So 
\begin{align*}
A^\dagger = (A^TA)^{-1}A^T &= V\Sig_1^2 V^T V([\Sig_1 0]) V^T\\  
&=V([\Sig_1^{-1} 0])U^T
\end{align*}

\section{Class 16 November 3rd 2011}
\label{sec:class16}

\subsection{Continue on SVD}
\label{sec:svd2}

Given $A\in \mathbf{R}^{m\times n}$, $m>n$, $A=U\Sig V^T$, $A$ full
rank.
$A = [U_1 U_2][\Sig_1 ; 0] V^T$ wher $U_1$ is m by n, $U_2$ is m by
$m-n$.
And
\begin{align*}
  U^TU &= [U_1^T; U_2^T][U_1 U_2]\\
&=
\begin{pmatrix}
U_1TU_1  & U_1^TU_2\\
U_2^TU_1 & U_2^TU_2
\end{pmatrix}
&=I =
\begin{pmatrix}
  I & 0 \\ 0 & I
\end{pmatrix}
\end{align*}
Because $U_1^TU_1 = I_n, U_2^TU_2 = I_{m-n}$, and $U_1^TU_2=0$

$A^T = V[\Sig_1 0][U_1^T; U_2^T] = V\Sig_1U_1^T$.
Consider $w\in \mathbf{R}^m$, look at $A^Tw = V\Sig_1U_1^Tw$. If $w\in
range(U_2)$, $w = U_2z$ for some $z$, so
$$A^Tw = V\Sig_1U_1^TU_2z = 0$$
i.e. $range(U_2) \subseteq null(A^T)$, $\supseteq$ is also true.
So $$null(A^T) = range(U_2).$$

\textbf{Pf}[$range(U_2) \supseteq null(A^T)$]
Suppose $w \in null(A^T)$, i.e. $A^Tw = 0$. 
$w = U_z$, for some $z$
\begin{align*}
  w &= U_z\\
&= U_1z_1 + U_2z_2
A^Tw &= V\Sig_1 U_1^T (U_1z_1+U_2z_2)
&=V\Sig_1 z_1 + 0
\end{align*}
If $z_1 \neq 0 \Rarr \Sig_1z_1 \neq 0$ $\contra$ because the
assumption $range(U_2)\subseteq null(A^T)$, $A^Tw$ should be 0. So it
has to be that $z_1 = 0$.  $\therefore w \in range(U_2)$

Another point:
COlumns of $U_1$ span the range of $A$. Range of $A:=\{Av | v\in
\mathbf{R}^n\}$.
Given $Av = U_1(\Sig_1 V^Tv)\subseteq range(U_1)$

We can see that $range(A)$ and $null(A^T)$ are orthogonal to each
other. (This is a known fact in linear algebra, but SVD makes it intuitive)

end of matrix factorization!
\section{New topic:Optimization}
Given $f: \mathbf{R}^n \rarr \mathbf{R}$, we want to find $x\in
\mathbf{R}^n s.t. f(x)$ is minimal (i.e. $-f(x)$ is maximal).
We may want to find:
\begin{itemize}
\item Global minimum $\hat x s.t. f(\hat x) le f(x) \forall x$
\item Local  minimum $\hat x s.t. f(\hat x) le f(x) \forall x$ near $\hat x$
  (rigorously: near means in some radius $r$.).
\end{itemize}
In this class we're looking for a local minimum.

If $x$ is a local minimum value, consider $\phi(\ah) = f(x+\ah d) $, where $d\in
\mathbf{R}^n$, some other vector, $\ah \in \mathbf{R}$.
Now look $\phi(\ah)$'s taylor series: $  \phi(0) + \phi'(0)\ah + \cO(\ah^2)$
\begin{align*}
\phi'(\ah) &= [\grad f(x+\ah d)]^Td\\
\phi'(0) &= [\grad f(x+\ah d)]^Td\\
\end{align*}
(Where $\grad f =
\begin{pmatrix}
  \frac{\partial f}{\partial x_1} &\vdots &  \frac{\partial f}{\partial x_n}
\end{pmatrix}$)

Suppose $\grad f(x) \neq 0$. \emph{Claim}: $\exists d s.t. [\grad
f]^Td < 0$.
Does such $d$ exist? yes, take $d = -\grad f(x)$. There's at least
one.

For any such $d$, $\phi(x+\ah d) = \phi(x) + [(\grad f(x))^Td]\ah + \cO(\ah^2)$
$\phi(x)$ is $f(x)$, and $(\grad f(x))^Td < 0$

When $\ah \ll$, positive, $\cO(\ah^2)$ is negligible incomparison to
$[(\grad f(x))^Td]\ah$. So $\phi(x+\ah d) < f(x) \forall$ small
positive $\ah$. 
$\contra$ because we started off with a local minimum! $\Rarr \grad
f(x) = 0$.

Summary: A necessary condition for $x$ to be a minimizer is that
$\grad f(x) = 0$. (not sufficient): called the 1st order necessary condition.
This can be solved by doing newton's method on $F=\grad f(x)$.

Check the second derivative, if concave (second derivative positive), then we have a local minima.

Look at 3-term taylor series.
$\phi''(\ah) = d^T\grad^2f(x+\ah d)d$, where $\grad^2f(x+\ah
d)$ is a matrix with $(i,j)$ entry given by $H = \frac{\partial^2
  f}{\partial x_i \partial x_j}(x+\ah d)$. This is the
\textbf{Hessian} matrix.
So $\phi''(0) = d^T\grad^2 f(x) d = d^THd$
So:
\begin{align*}
\phi(\ah) &= f(x+\ah d) = \phi(0) + \phi'(0)\ah + \frac{1}{2}\phi''(0)\ah^2 +
\cO(\ah^3)\\
&= f(x) + [\grad f(x)]^Td]\ah + \frac{1}{2}d^THd \ah^2 + \cO(\ah^3)
(\text{ because $x$ is a local minimum, } \grad f(x)]^Td == 0, \text{
  and } d^THd < 0 \forall d)\\  
\end{align*}

(if $d^THd < 0$, then $f(x+\ah d) < f(x)\forall \ah \ll$ ( $\contra$ just
like the other reasoning)).

So another necessary conditions is $H = \grad^2f(x)$ has to be positive
semidefinite, this is calloed the 2nd order necessary condition.

Note that these together are not sufficient.

\emph{Example:} $n=2$, $f(x) = \frac{1}{2}x_1^2 + x_1 + x_2^3$
then $\grad f =
\begin{smallmatrix}
x_1+1 & 3x_2^2  
\end{smallmatrix} = \begin{smallmatrix}
0 & 0
\end{smallmatrix}$ set $x_1 = -1, x_2 = 0$.
So $\grad^2 f =
\begin{smallmatrix}
  1 & 0 \\ 0 & 6x_2
\end{smallmatrix} =
\begin{smallmatrix}
  1 & 0 \\ 0 & 0
\end{smallmatrix}$ at $x=(-1,0)$

Condition for a matrix to be positive definite is that all its
eigenvalues are strictly positive. Semi-definite is all its
eigenvalues are $le 0$. So this is $H$ that satisfies the condition
and we have two necessary conditions satisfied at $x=(-1, 0)$.
But it's not because we can find a point $\tilde x=(-1, x_2)$
$f(\tilde x) = \frac{1}{2} + x_2^3 < f(x) = -\frac{1}{2} \forall x_2
le 0$.

\textbf{Sufficient conditions}:
\begin{enumerate}
\item $\grad f(x) = 0$
\item $\grad^2 f(x) = H$ is positive-definite. (i.e. in 2-D function
  is concave at this point)
\end{enumerate}

If we have some $x$ not a minimizer, algorithm will chose a $d$
s.t. $f(x+\ah d) < f(x)$. One condition for $d = -\grad f(x)$.

Another approach: try to find a root of the equation $F(x) = \grad
f(x) = 0$.

Recall \emph{Newton's method}: In 1-D, $x_{n+1} = x_{n} +
\frac{-f(x_n)}{f'(x_n)}$.
In N-D, it's $J_F(x_n)d = -F(x_n)$ (not the same $d$)

\pagebreak
\section{Class 17,18 November 15th 2011} Missed for grace hopper 
\section{Class 19 November 15th 2011}
\label{sec:class19}

\subsection{Optimization}
$f:\mathbf{R}^n \rarr \mathbf{R}$ Iteration $x_{k+1} \larr x_k +\ah_k
d_k$, where $d_k$ is the descent direction $d_k^Tg_k < 0$, and $\ah_k$
is the step length obtained from line search.

From last time: Choose $\ah_k$ s.t. $$f(x_k+\ah_kd_k) \le f(x_k) + \mu
\ah_kd_k^Tg_k$$ where $\mu$ is a fixed parameter. Called the \emph{Armijo condition}.
Refer to the graph on the notebook, but $\phi(\ah) = f(x_k+\ah d_k)$,
$\phi(0) = f(x_k)$, $\phi'(0) = \grad f (x_k)^Td_k = d_k^Tg_k$, and
$\phi_\mu(\ah) = f(x_k) + \mu\ah d_k^Tg_k$.

Another way to do line search: try to find $\ah$ near the value that
minimizes $\phi(\ah)$.
Where $\phi'(\ah) = [\grad f (x_k+\ah d_k)]^Td_k$, such minimizer of $\phi$ (if it
exists) satisfies $\phi'(\ah) = 0$.

Strategy: Choose $\ah_k$s.t. $\phi'(\ah)$ is not too large. That is,
require $$|\phi'(\ah)| \le \eta |\phi'(0)|$$ for some fixed constant
$\eta$ (example $\eta = .9$).

Guess $\ah$ by evaluating $|\grad f (x_k+\ah d_k)]^Td_k|$ compare it
to $|g_k^Td_k|$. Called the \emph{Wolfe Condition}.
This way is more expenisve, because we need to compute the $\grad f$,
which involves $n$ computations, as opposed to computing $f$ is just
single real value. 
But this is prefered because: we are requiring the derivative to be
smaller, it force us to chose a bigger step sizes and move closer to
the minimum sooner. (Because say we have a candidate $a$ very close to
$0$, the slope of $\phi'(a)$ will be very close to $\phi(0)$, so it
will prevent us from chosing $\ah$ that's too close to 0.

So far we've seen
\begin{itemize}
\item steepest descent $d_k = -g_k$ (slow but cheaper)
\item Newton's method $d_k$ solves $H_kd_k = -g_k$, called the
  Hessian. $\grad^2 f(x_k)$ is the descent direction iff $H_k$ is
  positive-definite. ($d_k^Tg_k < 0 \rarr -g_k^TH_kg_k$ is guaranteed to be $< 0$ if $H_K$
positive-definite, but may happen by luck so we want the condition
example: $g_k=[1,0], H_k=(\begin{smallmatrix}  1 & 0 \\ 0 & -1 \end{smallmatrix})$).

Evaluating the hessian and solving system is very expensive.
\end{itemize}

\subsection{Quasi-Newton Methosd}
\label{sec:quasi-newton}

This method will construct $B_K$, an approximation to $H_k$ somehow.
Algorithm:
\begin{algorithmic}
  \STATE{ Start with an arbitrary $x_0$, $B_0$ ($=I$, for example), $g_0 = \grad f(x_0)$}
  \FOR{ $k=0,1,2,\dots$ until convergence}
    \STATE{solve $B_kd_k = -g_k$}
    \STATE{update $x_{k+1} = x_k + \ah_kd_k$}
    \STATE{compute $g_{k+1} = \grad f(x_{k+1})$}
    \STATE{update $B_{k+1} = B_k - $ (some update)}
  \ENDFOR
\end{algorithmic}
Q: How to choose $B_{k+1}$??

One approach: Consider $H(x_k)$ when $f$ is a quadratic
function. $$f(x) = \frac{1}{2}x^TQx + b^Tx + c$$, a typical quadratic
function.
$\grad f(x) = Qx+b$ (exercise)
$\grad f(x+d) = Qx + Qd + b$, and $\grad f(x+d) - \grad f(x) = Qd$.
ALso, $\grad^2 f = Q$. This implies that $$Hd = \grad^2f d = Qd = g(x+d) - g(x)$$ We
will try to mimic this relationship for general $f$. 

Try to eforce this conversation as follows:
Choose $B_{k+1}$ to satisfy $B_{k+1}d_k = g_{k+1} - g_k$. But this is
not enough to get a good estimate so in addition,
$B_{k+1}v = B_kv$ $\forall v$ orthogonal to $d_k$ (because the first
condition is just 1-D, this make sure that we don't do anything to
whatever that's not in the same direction).

Consider
$$B_{k+1} = B_k - \frac{(B_kd_k - (g_{k+1} - g_k))d_k^T}{d_k^Td_k}$$
Check: 
\begin{align*}
B_{k+1}d_k &= B_kd_k - \frac{(B_kd_k - (g_{k+1} -
  g_k))d_k^Td_k}{d_k^Td_k}\\  
&= B_kd_k - (B_kd_k - (g_{k+1} - g_k))\\  
&= g_{k+1} - g_k
\end{align*}
Also 
\begin{align*}
  B_{k+1}v &= B_kv - \frac{(B_kd_k - (g_{k+1} -
    g_k))d_k^Tv}{d_k^Tv}\\  
&= B_kv - 0 \text{ for $v \perp d_k$}
\end{align*}
This is called the \emph{Broyden's method}, a rank-1 update. Way
cheaper than evaluating the hessian.

If $H$ is a hessian, it would be a symmetrix matrix (it's a second
derivative $\frac{\partial^2 f}{\partial x_i\partial x_j}$). But $B_k$
by Broyden's method is not symmetric.

So \emph{Symmetric variant:}
Let $y_k = g_{k+1} - g_k$, $$B_{k+1} = B_K +
\frac{(y_k-B_kd_k)(y_k-B_kd_k)^T}{(y_k-B_kd_k)^Td_k}$$
This works satisfies the two requirements.
\pagebreak

\section{Class 20 November 17th 2011}
\label{sec:class20}
Review of the HW3 \#2:
$\hat A_c = A_c + E$, we want $A$. THere is a linear relation between
$A$ and $A_c$. Unravel $A,A_c,\hat A_c$ column wise to get $a,a_c,\hat
a_c$.
The naive computation was to solve $Ka = \hat a_c$ but this runs out
of memory, or takes way too long.

$K$ has the form $B \otimes C$. Question was how do you take advantage
of this and solve $Ka = \hat a_c$.  Supposed we wanted to solve $(B
\otimes C)x = y$
$B \otimes C =
\begin{pmatrix}
  b_{11}C & b_{12}C & \cdots & b_{1n}C\\
  \vdots &  & \cdots & \vdots \\
  b_{11}C & b_{12}C & \cdots & b_{1n}C\\
\end{pmatrix}
$
Change $x$ s.t. it's $m$ by $n$.
THen, the first block is
\begin{align*}
C(b_{11}x_1 + b_{12}x_2 + \cdots + b_{1n}x_m)  &=C[x_1 , x_2, \cdots, x_m]
\begin{pmatrix}
  b_{11} \\ b_{12} \\ \vdots \\ b_{1n}
\end{pmatrix}
&= \text{ transpose of 1st row of $B$ is 1st col of $B^T$}
\end{align*}
So
$$CXB^T=Y$$ where $X$ and $Y$ are reshaped versions of $x$ and $y$ in
blocks.
So the solution is $X=C\inv Y B^{-T}$. Let $C= U_c\Sig_cV_c^T$ and
$B=U_b\Sig_bV_b^T$, then $X$ is $V_c\Sig_c\inv U_c^TY(U_b\Sig_b\inv V_b^T)$

\subsection{Quasi-Newton Methods}
\label{sec:quasi-newton}
\begin{algorithmic}
  \STATE{ Start with $x_0$, $B_0$ ($=I$, for example), $g_0 = \grad f(x_0)$}
  \FOR{ $k=0,1,2,\dots$ until convergence}
    \STATE{solve $B_kd_k = -g_k$}
    \STATE{update $x_{k+1} = x_k + \ah_kd_k$}
    \STATE{compute $g_{k+1} = \grad f(x_{k+1})$}
    \STATE{update $B_{k+1} = B_k - $ (some update)}
  \ENDFOR
\end{algorithmic}


\end{document}
