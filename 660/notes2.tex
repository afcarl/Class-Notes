\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\kap}{\kappa}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\dag}{\dagger}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}
% for cross product
\newcommand{\lc}{\langle} %<
\newcommand{\rc}{\rangle} %>

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}
\parskip 5pt
\parindent 0pt

\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,algorithmic}
\begin{document}
\title{Scientific Computing CS660 Fall '11\\Notes after the midterm}
\author{Angjoo Kanazawa}
\maketitle
\section{Class 13 October 25th 2011}
\label{sec:class13}
\subsection{HW2 answers}
\textbf{Problem 1b}: to compute many $N$, compute $S_N = \phi(x_1) + \cdots +
\phi(x_n)$, take $N$ samples, divide by $N$ will give us $I_N$. THe
error is $E_N = I-I_N$. TO generate $I_{N+1}$, take $S_{N+1} = S_N +
\phi(x_{N+1})$ to compute $I_{N+1}$. But this way $E_{N+1}$ is not
independent from $E_N$. (Left top figure on the answers).

(Doing it entirely independently (how i did it) is the lower figure)
Plotted $\log_N$ vs $\log(\frac{\sig}{\sqrt{N}}$
The observation was that it somewhat follows the bound. 

Our bound, $\frac{\sig}{\sqrt{N}}$, comes from chebychev
$P(|\frac{S_N}{N} - \mu| \ge c) \le \frac{\sig^2}{Nc^2}$.
So if we want this $P$ to be 95\%, then $c\le
\frac{\sig}{\sqrt{0.5N}}$. So loosely speaking, the error $E_N = I-I_N
\approx \frac{\sig}{\sqrt{0.5N}}$. The point of the question was that
it's not exact.

On Page 3 of the solution, there's the definition of histogram for
question 1c.

\textbf{Problem 2}: 
In P1, the error was $E_N = \frac{\sig^2}{\sqrt{N}}$
If the variance is $\sig^2$, std $\sig$, then log of $E_N$ is $\log
\sig - \frac{\log(0.5N)}{2}$. 

In here, we expected the error to be $\frac{\sig^2}{\sqrt{N}}$. If
that were to happen, same derivation would give $\log(E_N)\approx
2\log(\sig) - \frac{\log(0.5N)}{2}$.
The blue line (boundary) would've been $2\log(\sig) -
\frac{\log(N)}{2}$ and $E_N$ will hover around there. But it didn't
work!! Because $sin(\pi X) = sin(\pi(1-X))$, nothing changed. Function
had to be monotone.

\textbf{midterm:}
Equally distributed, 3 topics: floating pt/round off errors, mc,
matrix factorization (big picture). (no newton's method, no machine
representation of numbers)

\subsection{Eigenvalue Hessemberg}
Why do we want to reduce the matrix to Hessemberg form: Because doing
$QR$ on Hessenberg is $\mathcal{O}(n^2)$ instead of
$\mathcal{O}(n^3)$.

How to make a hessemberg matrix:
$A$ is a big full matrix. Now $QA$ will make everything below the
second entry of first column to 0. You can do that with Householder
transformation, or givens rotation. 

However we do this, $QAQ^T$ will not mess up the 0s.
\begin{align*}
 QAQ^T &= [Q(QA)^T]^T\\
\end{align*}
Applying to $Q$ to $(QA)^T$ will leave the first row alone, so
$[Q(QA)^T]^T$ still keeps the first column's entry below 2 to 0.

Exercise, go back, do: can we perform $QAQ^T$ where $Q$ is a
householder transformation s.t. $QAQ^T$ is a first step hessemberg
matrix. The reason why this works is because householder is in the
$n-1$ subblock, so it doesn't do anything row 1. (It's like the 2nd
household transformation in the QR reduction)


$QR$ iteration for the eigenvalue problem:
\begin{enumerate}
\item[0] Reduce $A$ to hessenberg form, formally, $PAP^T = H$, where $P$
  orthogonal. (do $n-1$ householder transformation $P_2$).$A_0 = H$
\item[1-k] Iterate, $A_K = Q_KR_K$, $A_{K+1} = R_KQ_K$
\end{enumerate}
Doing this $A=QR$ is $\mathcal{O}(n^3)$, but to do this until
conversion (see $\eps$ in below diagonal, is $n$, so total
$\mathcal{O}(n^4)$ (without making $A$ into upper hessenberg).
If $A$ wer hessenberg, doing $QR$ is $\mathcal{O}(n^2)$ instead of
$\mathcal{O}(n^3)$, so the total cost is $\mathcal{O}(n^3) +
\mathcal{O}(nn^2) = \mathcal{O}(n^3)$ with hessemberg.

\pagebreak

\section{Class 14 November 1st}
\label{sec:class14}
Midterm: median 22, mean 20.8, max:29

\subsection{Midterm}

\textbf{Problem 1}:
Given $Ax=b$, $A\hat x = b + \delta b$: $A(x-\hat x) = b -b \delta b$
so 
\begin{align*}
x-\hat x &= A^{-1}(\delta b)\\
||x-\hat x|| &\le ||A^{-1}||||\delta b||\\
\end{align*}
And
\begin{align*}
  b&=Ax\\
||b|| &\le ||A|| ||x||\\
\frac{1}{||x||} &\le \frac{||A||}{||b||}\\
\end{align*}
So
$$
\frac{||x-\hat x||}{||x||} \le ||A|| ||A^{-1}|| \frac{||\delta b||}{||b||}
$$
\noi
\textbf{Problem 2}: 
\begin{enumerate}
\item[a] QR factorization by gram-schimid
\item[b] Making $r_{ik} = <\hat q_k, q_i>$ from $r_{ik} =<a_k, q_i>$,
  doesn't change anything. Because
  \begin{align*}
    r_{23} = \lc\hat q_3, q_2\rc \\
&= \lc a_3 - r_{13}, q_2\rc\\
&= \lc a_3, q_2 \rc - r_{13}\lc q_1, q_2 \rc
\end{align*}
\item[c] Real question is: Will $q_3$ be orthogonal to $q_2$ and
  $q_1$? Look at $\hat q_3$ after $i$th loop is done. $\hat q_3 = \lc
  a_3 - r_{13}q_1 - r_{23}q_2, q2 \rc = \lc a_3, q_2\rc - r_{13}\lc
  q_1, q_2\rc = -r_{23}\lc q_2, q_2 \rc$
Problem is $\lc q_1, q_2\rc$ could be non-zero because of round-off
error.
But if we did the second way, $\lc \hat q_3, q_2 \rc = \lc a_3 - r_{13}q_1, q_2 \rc -
r_{23}\lc q_3, q_2 \rc$, is a little bit less-sensitive to round-off
errors. Called the modified-Gram Schmid. (This can't be run parallel)
\end{enumerate}

\subsection{Singular Value Decomposition}
\label{sec:SVD}

Let $A\in \mathbf{R}^{m\times n}$. $A$ can be factored as $$A=U\Sigma
V^T$$, $U$ is $m$ by $m$, sigma is $m$ by $n$, a diagonal matrix, where
the entries are $sig_1, \dots, sig_n$, everywhere else 0. $sig_i$'s
are called the singular values, and $\sig_1 \le sig_2 \le \cdots \le
\sig_n \le 0$.
$V^T$ is $n$ by $n$. Where $U^TU = I_m$ and $V^TV = I_n$.

How to use:
\textbf{Least Squares}: minimize $||b-Ax||_2$, find $x$. 
  \begin{align*}
    ||b-Ax||_2^2 &= \lc b-Ax, b-Ax \rc \\
&= \lc b-U\Sigma V^Tx, b-U\Sigma V^Tx \rc \\
&\text{ let $\hat x = V^Tx$}\\
&= \lc U(U^Tb-\Sigma\hat x), U(U^Tb-\Sigma\hat x) \rc \\
&= \lc U^TU(U^Tb-\Sigma\hat x), U^TU(U^Tb-\Sigma\hat x) \rc \\
&= \lc U^Tb-\Sigma\hat x, U^Tb-\Sigma\hat x \rc \\
&\text{ let $\hat b = U^Tb$}\\
&= \lc \hat b-\Sigma\hat x,  \hat b-\Sigma\hat x \rc \\
&= || \hat b - \Sigma \hat x ||^2_2
  \end{align*}
This is $$
\begin{pmatrix}
 \hat b_1 \\ \hat b_2 \\ \vdots \\ \hat b_m
\end{pmatrix}
-
\begin{pmatrix}
 \sig_1 \hat x_1 \\ \vdots \\ \sig_n\hat x_n \\ 0 \\ \vdots 0
\end{pmatrix}
$$
So to minimize, set $\sig_k\hat x_k = \hat b_k$, or $$\hat x_k =
\frac{\hat b_k}{\sig_k}$$

The norm of minimum norm solution is $\sqrt{\hat b^2_{n+1} + \cdots +
  \hat b_m^2}$, provided $\sig_n > 0$. If all $\sig_j > 0$, then $\hat
x, x$ are unique. aka $A$ is of full rank. If some of them are zero,
then $\exists$ multiple solutions $\hat x$ and $x$. Rank of $A$ is the
index of smallest nonzero $\sig_j$s.

To solve this problem, compute $U, V$ and $\Sigma$, fing $\hat b$,
find $\hat x$, find $x=V\hat x$.

\textbf{Remember}: least squares solution can be found from $A^TAx =
A^Tb$:
\begin{align*}
  A^TAx &=A^Tb\\
V\Sigma^TU^T U\Sigma V^T x &= V \Sigma^T U^Tb\\
V[\Sig_1 0][\Sig_1; 0]V^T x&= V \Sigma^T U^Tb\\
V\Sig_1^2 V^T x &= V[Sig_1 0][U^Tb]\\
\Sig_1^2 V^Tx &=\Sig_1[U^Tb]\\
\end{align*}
This is just like the one before where $V^Tx = \hat x$,
$\hat b = U^Tb$.

\subsection{Pseudo-inverse}
Of a rectangular matrix $A$, is written:
$$A^\dagger = (A^TA)^{-1}A^T$$
$(A^TA)^{-1}$ is square times wide rectangle, so a wide rectangle. 
Property: $A^\dag A = (A^TA)^{-1}A^A = I$.

Given $A = U\Sig V^T$, 
\begin{align*}
  A &= U\Sig V^T\\
A^TA &= V\Sig_1^2 V^T\\
(A^TA)^{-1} &= (V\Sig_1^2 V^T)^{-1} = (V^T)^{-1}(\Sig_1^2)^{-1}V^{-1}\\
&=V\Sig_1^{-2}V^T\\
\end{align*}
So 
\begin{align*}
A^\dagger = (A^TA)^{-1}A^T &= V\Sig_1^2 V^T V([\Sig_1 0] V^T\\  
&=V([\Sig_1^{-1} 0])U^T
\end{align*}

\end{document}
