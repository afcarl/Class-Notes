\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}


\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb}
\begin{document}
\title{Scientific Computing CS660 Fall '11}
\author{Angjoo Kanazawa}
\maketitle
\section{September 1st Class 1}
\subsection{Logistics}
\label{sec:logistics}
Prof. Howard Elman CSI 2120 TR 2pm-3:15pm
class url:http://www.cs.umd.edu/~elman/660.11/syl.html
\begin{itemize}
\item Scientific Computing puts heavier emphasis on computing, Numerical
  Analysis is more about proofs/theories.

\item 4-6 hw asg: \textbf{35\%} Penalty on late assignments (-15\%
  after 24 hrs, -30\% after 48 hrs. 
\item in-class midterm: \textbf{25\%}
\item final project: \textbf{40\%}
\end{itemize}

\subsection{Content} 
\label{sec:content}
\textbf{Newton's Method}: Root finding. 
Objective: Fine $x$ s.t. $f(x) = 0$, where $f$ is a scalar,
$f:\mathbf{R}\Rightarrow \mathbf{R}$
function. Where does the function cross 0 (x-axis)?

Given $x_n$, some guess, find where the line through $(x_n, f(x_n))$
tangent to the solution curve intersects the $x$-axis. Call that
pt of intersection $x_{n+1}$

The equation of the tangent line: $$\frac{y-f(x_n)}{x-x_n} = f'(x_n)$$
Set $y=0$: then
\begin{align*}  \frac{0-f(x_n)}{x_{n+1}-x_n} &= f'(x_n)\\
  \frac{x_{n+1}-x_n}{-f(x_n)} &= 1/f'(x_n)\\
  x_{n+1} &= x_n - f(x_n)/f'(x_n)\\
\end{align*}

\textbf{Another Derivation}
Consider the taylor series $f(x_n+(x-x_n)) = f(x_n) + f'(x_n)(x-x_n) +
1/2f''(x_n)(x-x_n)^2 + \text{ etc}$. This is a function of some
variable $x$. Approximate it just by using the first two terms (linear
approximation).
So above becomes a new function $f(x_n+(x-x_n))~= f(x_n) +
f'(x_n)(x-x_n) = l(x)$.
Find where $l(x) = 0$. That is: $x_{n+1} = x_n - f(x_n)/f'(x_n)$

This is not guaranteed to work, i.e. when the tangent doesn't cross
the $x$-axis.


\textbf{Problem}: given $\alpha \in \mathbf{R} > 0$ Find $1/\alpha$ without
doing any division.
First thing you need to do if identify (concoct) a $f(x)$ whose root
is $1/\alpha$.
Naive: try $f(x) = x - 1/\alpha$. But this won't work, because this
requires division.
Howabout: $f(x) = \alpha x - 1$. $f'(x) = \alpha$, so in the newton
iteration.. 
\begin{align*}
x_{n+1} &= x_n - \frac{\alpha x - 1}{\alpha}\\
&= x_n - x_n + 1/\alpha \\
&= 1/\alpha
\end{align*}
No! because you need to divide.

Answer: $f(x) = \alpha - 1/x$. Not transparent, because intuitively it
looks like there's a divide into it.
$f'(x) = - -1/x^2 = 1/x^2$. Then, $f/f' = \alpha x^2 - x$. So given $x_n$,
the iteration goes $x_{n+1} = x_n - (\alpha x_n^2 - x_n) = 2x_n -
\alpha x_n^2 = x_n(2-\alpha x_n)$.

Numerical example in matlab: let $\alpha = 2$, solve with this method. Use $x_0
= 0.1$. Notice that $err(i)/err(i-1)^2$ is constant and \emph{is}
$\alpha$.
\begin{align*}
  \frac{|x-x_{n+1}}{x-x_n}^2 &\approx 1/2\frac{f''(x_n)}{f'(x_n)}\\
\end{align*}
Notice the $err(i)$ decreases faster as iteration moves on, this is
the super linear convergence property of Newton's method.
With $\alpha = 0.25$, same thing.\\

\textbf{Analysis of the Newton's Method}:
$|x_{x_{n+1}}|/|x-x_n|^2 \approx 1/2 | f''(x_n)/f'(x_n)| \approx 1/x$
So @ $x = 1/\alpha$, this turns out to be $\alpha$, just for this
example.

The ratio was derived from the idea to find a patter in the errors
s.t. $e_{n+1} ~ c e_n^2$, for some $c\in \mathbf{R}$

The question is to find trends in data, and this relationship $e_{n+1}
~ ce_n^p$ is useful to tell us the rate of convergence as the solution
approches the optimal one. (I think $p$ goes down to the golden
ratio). Our goal is to find what $p$ is for a specific problem.
\pagebreak
\section{September 6th class 2}
\label{sec:class2}

\subsection{Process of Scientific Computing}

\begin{itemize}
\item Start with a mathematical model. (In general we don't have an analytic solution, so we get
  insight from numerical computation)
\item Example with heat conduction in a bar:
  \begin{itemize}
  \item A 1-D object $\in [0, 1]$, $u(x)=$temperature in the bar, with
  $u(0)=0, u(1) = 0$. $q$=heat flow induced by a heater of intensity $f$.
  \item We want what the temperature will be given $q$.
  \item get some models: \emph{Fourrier's Law}: $q= -ku'$, $k$=conductivity
    coefficient, transfer of heat in direction of decreasing
    temperature (hence the -). \emph{Conservation of energy}: $q' = f$.
  \item \textbf{Goal}: find $u$. \textbf{Equation of interest}: $-(ku')' = f$, the 1D diffusion
    equation.
  \item Typical strategy: lay down a grid $x_o=0, x_1, \dots, x_n,
    x_n+1=1$. Compute a discrete solution, $\bar{u}$, vector of size $n$. $\bar{u} = [u_1, \cdot, u_n]^T$, where $u_i \approx u(x_i)$
  \item \textbf{Claim}: we can find the discrete sol, $\bar{u}$ by
    solving an algebraic system of equations. For this example, this
    system is a matrix equation $$A\bar{u} = \bar{b}$$

  \end{itemize}
  \item No matter how hard I try, we're never going to get the exact
    solution. This process $A\bar{u}=\bar{b}$ leads to errors
  \item \textbf{Sources of error}:
    \begin{enumerate}
       \item Modeling error: we may not know $k$ exactly.
       \item Discretization error/Truncation error: difference between the discrete and
         the continuous values (from the approximation on a discrete
         set of points)
       \item Representation error: we don't have the entire
         $\mathbf{R}$, we only have a finite set, in floating point
         format. ($A$ and $b$ may have error)
       \item Additional error: from the computation of $\tilde{u}$,
         will get something else $\hat{\tilde{u}}\neq \tilde{u}$\\
        \textbf{we can show}: $$\frac{||\hat{\tilde{u}} -
           \tilde{u}||}{\hat{\tilde{u}}||} \le K(A)\mu$$ That is if we
         solve the problem appropriately (like being careful about
         pivoting etc).
         
    \end{enumerate}
  \item We're really trying to solve $\tilde{A}\tilde{u} =
    \tilde{b}$., where $\tilde{A}\approx A$, $\tilde{u} =
    \bar{u}$. Typically $\approx$ is machine precision, $10^{-16}$

  \item In the end, we want $u(x)$, and would be happy with $u(x_j)$,
    $j=1,\dots, n$. That will get $\tilde{u_j}$
  \item The moral is that when we do this stuff, we're not just doing
    mathematics. 
\end{itemize}

\subsection{Floating Point Arithmetic}
\label{sec:floating}

decimal numbers (base 10).
Consider the example 6522 and 10.31
\begin{itemize}
\item $6522 = 6(10^3) + 5(10^2) + 2(10^1)+ 2(10^0)$
\item $10.31 = 1(10^1) + 0 + 3(10^-1)+ 1(10^-2)$
\item Normalize the numbers! then,,
\item $6522 = 6.522 \times 10^3$ so we can express numbers with one digit to the
  left o fthe decimal point.
\item $10.31 = 1.031 \times 10^1$
\item For any number but 0, it has a fomr $z\times 10^p$, where
  $z\in[1, 10)$.
\end{itemize}

Computers use binary representation. Example: $3_{10} = 1(2^1) + 1(2^0) =
11$ or $1.1000 \times 2^1$.

$23_{10} = 16 + 4 + 2 + 1 = 1(2^4) + 0(2^3) + 1(2^2)+ 1(2^1)= 10111_2$
or $1.0111 \times 2^4$ \emph{normalized}. Here, normalized means
$z\times 2^p$, where $z\in [1, 2)$

\pagebreak
\section{September 8th Class 3}
\label{sec:class3}

\subsection{Floating Point Operations}

Example: addition using d=5 binary digits

Add $3+23$.\\ \noi
Normalized 5-digit binary expressions:

$3 = 2^1 + 2^0 = 1.1000 \times 2^1$

$23 = 16 + 4 + 2 + 1 = 1.0111 \times 2^4$

To add these, shift the smaller number so that the exponents agree.

\begin{align}
1.1000 \times 2^1 & = 0.11000 \times 2^2 \\
& = 0.01100 \times 2^3 \\
& = 0.00110 \times 2^4
\end{align}



May need to round the result: 
3+34
32+2 = 

Shift smaller number
\begin{align*}
3 &==> 0.000110 \times 2^5\\
34 &==> 1.00010 \times 2^5\\
---&-------------\\
37 &==> 1.00101  \\
\end{align*}
\noi
The way the arithmetic is done: take 2 floating point numbers. The computer
hardware will take the correct result (stored temporarily), but will always
round to $d$ digits. In our examples we are using only $d=5$. in the real world we are usually dealing with $d=27$ or more. number will always be rounded to closest representation-- can be either up or down. 

so in this case, the stored result is rounded to $1.0011 \times 2^5$ which is $32+4+2 = 38$. 

Exercise: what two number would have the property whose sum's \textbf{correct}
result is 1.001001? Answer: 32+4+0.5. (Note: $2.5$ in binary is $1.0100 \times 2^1 = 2 + 1/2$). 

In general, given the real $x$, $fl(x) = $ closest floating point
number to $x$. $|x = fl(x)|$ is called the rounding error. For
operations op = $\{+,-,\times,\div\}$, if $x$ and $y$ are floating
point numbers, then $fl(x\text{ op }y) =$ floating point number closest to $x$ $op$ $y$. 

It is possible that both inputs are perfectly good FP numbers, but that the
result of the operation is not able to be represented in the floating point
system (eg. d is too small). For example, in our single precision example,
$d=23$, $m = -126 \le p \le M = 127$. Then, if $x = 1 \times 2^{100}$ and $y = 1 \times
2^{100}$ then $x*y = 1\times2^{200}$ which results in an overflow. 

\textbf{Definition} \emph{Machine Precision} or \emph{Machine Epsilon}
is the smallest floating point number $\mu$ such that $fl(1+\mu) \neq 1$ $(> 1)$

In IEEE arithmetic, for any real $x \neq 0$:

$$
\frac{|x - fl(x)|}{|x|} \le \mu
$$

Example: $d=5$. Find $\mu$. 

\begin{align*}
1 &= 1.0000 \times 2^0\\
\text{Add }z &= 0.00001 \times 2^0\\
--&------------\\
&=1.00001  
\end{align*}
=> which gets rounded to 1.0001. 

$fl(1+z) \neq 1$

Next smallest number to $z$. Normalized, $z = 1.0000\times 2^{-5}$. Next smaller number is $z = 1.1111 \times 2^{-6}$. 
\begin{align*}
1&= 1.00000\\
 +z&=0.0000011111\\
--&---------------\\
&=1.0000011111
\end{align*}
 

which gets rounded to 1.

In general, in $d$-digit binary arithmetic, the machine precision $\mu$ is
$2^{-d}$. 

For IEEE single precision, this is $2^{-23} \approx 1.2 x 10^{-7}$
For IEEE double precision, this is $2^{-53} \approx 1.1 x 10^{-16}$

Does IEEE arithmetic support the existence of numbers like $2^m$ or $2^M$?
\subsection{Relative Error}

We have a number $x$, and a representation of $x \approx \hat{x}$.

The relative error is given by

\[
\frac{||x - \hat{x}||}{||x||}
\]

And the absolute error is given by $||x - \hat{x}||$. 

The relative error is more important than the absolute error. Consider
for example a census, x\% absolute error in the number of ppl in class
compared to the same x\% absolute error in the population of nyc does
not mean the same thing.


\subsection{Forward vs. Backward Error Anlysis}

Generically speaking: we are seeking $y = f(x)$, and get $\hat(y) \neq y$. We want insight into $\frac{||y - \hat{y}||}{||y||}$. 

\textbf{Definition:} \emph{Forward error analysis} tries to keep track of errors as the computation proceeds. 

Example:

Solve $Ax = b$. We get $\hat{x}$ instead. And we are interested in $\frac{||x -
\hat{x}||}{||x||}$. Forward error analysis (egf. for gaussian elimination) is
not possible. (Well, it is possible, but the estimates tend to be wildly
inaccurate). 

\textbf{Definition:}\emph{Backward error analysis} makes the claim that $\hat{x}$ is the solution of a perturbed problem, $\hat{A}\hat{x} = b$ such that (if the solution is done right):

$$
\frac{||x - \hat{x}||}{||x||} \lessapprox \mu (p(n) \mu)
$$

where $p(n)$ is a slowly growing function of $n$ = problem size. 

We use this observation:
$Ax = b$
$\hat{A}\hat{x} = b$

and 

\begin{align}
A(x - \hat{x}) &= Ax - A\hat{x} \\
& = Ax - \hat{A}\hat{x} + \hat{A}\hat{x} - A\hat{x} \\
& = b - b + (\hat{A} - A)\hat{x} \\
& = 0 + E
\end{align}

where $E$ is our error. 

\begin{align}
& \rightarrow x - \hat{x} = A^{-1}E\hat{x} \\
& || x - \hat{x}|| \le ||A^{-1}|| ||E|| ||\hat{x}|| \\
& ||A^{-1}|| ||A||  ||E|| / ||A|| ||\hat{x} ||
\end{align}

$||A^{-1}|| ||A||$ is called $\kappa(A)$, and $||E||/||A|| \lessapprox \mu$

this $\rightarrow ||x - \hat{x}|| / ||\hat{x}|| \lessapprox \kappa(A) \mu $

$\mu \approx 10^{-16}$

$\kappa(A)$ depends on $A$, the statement of the problem.. 

We claim, that with a bit more work, we could put $||x||$ in the denominator. 
\pagebreak
\section{September 13rd Class 4}
\label{sec:class4}
\subsection{Intro to Probability}
\label{sec:probability}

\textbf{Discrete models}: Consider a game with a finite or countable
number of outcomes, $\Omega$, the \emph{sample space}.
\textbf{Definition:} \emph{Probability} for each $\omega_j \in
\Omega$, we have a number $p_j(\omega_j) = p_j$ s.t. $p_j \in [0, 1]$,
and $\sum_{j=1}^{\infty} p_j = 1$

Examples:
\begin{enumerate}
\item Roll a fair die outcomes: $\Omega=\{1,2,3,4,5,6\}$, $p_j = 1/6$, $j=1,2,\dots,6$
\item Roll two fair dice, outcomes: $\Omega=\{(1,1), (1,2), \cdots, (1,6),
  (2,1), \cdots, (6,6)\}$, $p_j = 1/36$, $j=1,\dots,36$.
\item put $n$ distinct balls into $N$ urns $N>n$. 1st ball has $N$ choices, so does the 2nd ball, etc, so
  the total number of outcomes is $N^n$.
\item A die is rolled until a six appears. possible outcomes:
  \begin{enumerate}
  \item Get a 6 on 1st trial: 6, $p(\omega_1)=1/6$
  \item something other than 6 on 1st trial: 16,26,36,46,56 (5 ways),
    $p(\omega_2) = 5/6(1/6) = 5/36$
  \item etc $p(\omega_i) = (1-p(\omega_1))^{i-1})p(\omega_1) = (5/6)^{i-1}(1/6)$
  \end{enumerate}
Here $\Omega$ is countably infinite. The sum:
\begin{align*}
\sum_{j=1}^\infty p_j &= 1/6 + (5/6)(1/6) + (5/6)^2(1/6) + \cdots\\
&=1/6\sum_{l=0}^\infty (5/6)^l\\
 &= 1/6(\frac{1}{1-5/6} = 1/6(\frac{1}{1/6}) = 1
\end{align*} 

 \textbf{Definition:} An \emph{event} is a subset $A \subseteq
 \Omega$. Notation $P(A) = \sum_{\omega_J \in A}
 p(\omega_j)$. Properties: $\P(\Omega) =1, P(\empty)=0,
 P(A^c)=1-P(A)$.
If $A_1, A_2, \dots$ are pairwise disjoint, then $P(\cup_i A_i) =
\sum_i P(A_i)$.

\textbf{Definition:} A \emph{random variable} $X$ is a function
$X:\Omega \rightarrow \mathbf{R}$. $\forall E\in \mathbf{R}$, define
an \emph{event} $\{X\in E\} = \{\omega \in \Omega | X(\omega) \in E\}$
Can talk about probability of such an event as $P(X\in E)$

\textbf{Definition:} A \emph{distribution} of a discrete random
variable $X$ is a collection of distinct real numbers, or a set of
values $\{m_j\}\in [0,1]$, s.t. $\sum m_j = 1$, and $P(X=x_j) = m_j$.

Example: \emph{Binomial distribution}. Consider a random experiment
with two possible outcomes. Psobability of success = $p$, failure
$q=1-p$.
With coins, $p=1/2$. Perform this experiment $n$ times, $X=$\# of
successes. $X$ has a $Binom(p,n)$. The probability of exactly $l$
sucesses is $P(X=l) = {n \choose k} p^kq^{n-k}$
\pagebreak
\section{September 15th Class 5}
\label{sec:class5}

\subsection{Probability Cont.}
\label{sec:probability}
Experiment: three tosses of a coin $|\Omega|=2^3$

\textbf{Definitino: } The distribution of discrete random variables is
a collection of real positives $\{x_j\}^{\infty}_{j=1}$, s.t.  $P(X=x_j) = m_j$


\textbf{Definition: } A \emph{mass function} of a discrete random
variable $X$ is $m:\mathbf{R}\rightarrow [0, 1]$ s.t. $m(x)=P(X=x)\forall x
\in \mathbf{R}$.

\textbf{Definition:} The binomial distriubtion is a random experiment with two outcomes where
$p$ is the probability of success, $1-p$ is the probability of
failure. It performs the experiment $n$ times, and $X$ denotes the number of total
successes.

Q: what is the probability of exactly $k$ successes? ${n \choose k}
p^k(1-p)^{n-k}$, let's check $\sum_{j=1}^\infty m_j =1$. This is
$\sum_{k=0}^np^k(1-p)^{n-k} = (p + (1-p))^n = 1^n = 1$ so yes.

\textbf{Definition:} Continuous random variables and
distributions. ex: Average weight of 100 randomly selected
, where $\Omega$=$\{$100 tuples of weights $\}$, $X(\omega) =1/100$
(sum entries of $\omega$). Random number uniformly chosen from $[a,b]$.

\textbf{Definition:} A function $f$ on $\mathbf{R}$ is called a
\emph{density function} if it's non-negative and integrable and
$\int_{-\infty}^\infty f(x)dx = 1$.

A random variable with a density function $f$
means $$P(\{ \omega \in \Omega | X(\omega) \in A \}) = \int_Af(x)dx$$
for $A\subseteq \mathbf{R}$.

example: we say $X$ is uniformly distributed on $[a,b]$ if it has
density function 
$$f(x)=
\begin{cases}
  \frac{1}{b-a} & \text{ for } x\in [a,b]\\
  0 & \text{ otherwise }\\
\end{cases}
$$

Alternatively, $X$ is normally distributed with mean $\mu$ and
variance $\sigma^2$ if 
$$f(x) = \frac{1}{\sqrt{2\pi \sig^2}}e^{-\frac{(x-\mu)^2}{2\sig^2}}$$
Exercise: check $\int_{-\infty}^{\infty} f(x)dx=1$

\textbf{Defintion:} \emph{mean}, $\mathbf{E}(x)=\mu$,  of a random variable. Discrete case:
$\mu = \sum_{x\in \mathbf{R}}x\times m(x)$, continuous case:
$\mu = \int_{-\infty}^{\infty}x\times f(x)dx$
$$
\begin{cases}
  \sum_{x\in \mathbf{R}} x\times m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty x\times f(x) & \text{ Continuous } 
\end{cases}
$$

\textbf{Definition:} \emph{Distribution of a continuous r.v.}: $F(x) =
P(X(\omega) \le x) = \int_{-\infty}^xf(\xi)d\xi$. So if $F(x)$ is
normal with $\mathcal{N}(\mu, \sig)$, $\mu$ is at the tip of the
gaussian mountain, and $\mu$ has val $0.5$ in the CDF, $F(x)$.


Let $\Phi: \mathbf{R} \rarr \mathbf{R}$. \emph{Claim:} 
$$E(\Phi(x)) =
\begin{cases}
  \sum_{x\in \mathbf{R}} \Phi(x)m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty \Phi(x)f(x)dx & \text{ Continuous }  
 
\end{cases}
$$
This is not obvious.

Now how to define R.V, $X$, formally. $\Omega \rarr \textbf{R}$. Using
the example of 3 coin tosses, $\Omega =
\{000,001,\dots,111\}$. $X(\omega) =$ decimal version of binary value
of $\omega$. Then, $X(\omega)= [1, 7] \in \textbf{N}$. Now, we can do:
$\forall x\in \textbf{R}$, define $m(x)$ by $$
m(x)=
\begin{cases}
1/8 & \text{ if } 1\ge x\le 7 \\
0 & \text{ otherwise}
\end{cases}
$$

Let us call 1 or more consecu, tive tosses of same type as \emph{run}s. This is a
new R.V. where $\Omega$ is the same, and $Y(\omega) = $number of
runs.

i.e. $Y(000) = 1 = \Phi(0), Y(001) =2=\Phi(1), Y(010)=3=\Phi(2),
Y(011)=2, \dots$. Note $Y=\Phi(X)$. So $m_y(y) = P(Y=y)$, and $m_y(1)
= 1/4, m_y(2)=1/2, m_y(3)=1/4, m_y(\text{else}) =0$. Then,
\begin{align*}
\mathbf{E}(Y) &= \sum_{y\in R}ym_y(y)\\
& = 1/4 + 2(1/2) + 3(1/4) = 2  
\end{align*}

This is the same as 
\begin{align*}
\sum_{x\in R}\Phi(x)m(x) &= (1+2+3+2+3+2+1)(1/8)\\
&=16/8=2
\end{align*}


\end{enumerate}


\end{document}
