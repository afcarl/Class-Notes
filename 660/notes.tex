\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}
\parskip 5pt
\parindent 0pt

\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb}
\begin{document}
\title{Scientific Computing CS660 Fall '11}
\author{Angjoo Kanazawa}
\maketitle
\section{September 1st Class 1}
\subsection{Logistics}
\label{sec:logistics}
Prof. Howard Elman CSI 2120 TR 2pm-3:15pm
class url:http://www.cs.umd.edu/~elman/660.11/syl.html
\begin{itemize}
\item Scientific Computing puts heavier emphasis on computing, Numerical
  Analysis is more about proofs/theories.

\item 4-6 hw asg: \textbf{35\%} Penalty on late assignments (-15\%
  after 24 hrs, -30\% after 48 hrs. 
\item in-class midterm: \textbf{25\%}
\item final project: \textbf{40\%}
\end{itemize}

\subsection{Content} 
\label{sec:content}
\textbf{Newton's Method}: Root finding. 
Objective: Fine $x$ s.t. $f(x) = 0$, where $f$ is a scalar,
$f:\mathbf{R}\Rightarrow \mathbf{R}$
function. Where does the function cross 0 (x-axis)?

Given $x_n$, some guess, find where the line through $(x_n, f(x_n))$
tangent to the solution curve intersects the $x$-axis. Call that
pt of intersection $x_{n+1}$

The equation of the tangent line: $$\frac{y-f(x_n)}{x-x_n} = f'(x_n)$$
Set $y=0$: then
\begin{align*}  \frac{0-f(x_n)}{x_{n+1}-x_n} &= f'(x_n)\\
  \frac{x_{n+1}-x_n}{-f(x_n)} &= 1/f'(x_n)\\
  x_{n+1} &= x_n - f(x_n)/f'(x_n)\\
\end{align*}

\textbf{Another Derivation}
Consider the taylor series $f(x_n+(x-x_n)) = f(x_n) + f'(x_n)(x-x_n) +
1/2f''(x_n)(x-x_n)^2 + \text{ etc}$. This is a function of some
variable $x$. Approximate it just by using the first two terms (linear
approximation).
So above becomes a new function $f(x_n+(x-x_n))~= f(x_n) +
f'(x_n)(x-x_n) = l(x)$.
Find where $l(x) = 0$. That is: $x_{n+1} = x_n - f(x_n)/f'(x_n)$

This is not guaranteed to work, i.e. when the tangent doesn't cross
the $x$-axis.


\textbf{Problem}: given $\alpha \in \mathbf{R} > 0$ Find $1/\alpha$ without
doing any division.
First thing you need to do if identify (concoct) a $f(x)$ whose root
is $1/\alpha$.
Naive: try $f(x) = x - 1/\alpha$. But this won't work, because this
requires division.
Howabout: $f(x) = \alpha x - 1$. $f'(x) = \alpha$, so in the newton
iteration.. 
\begin{align*}
x_{n+1} &= x_n - \frac{\alpha x - 1}{\alpha}\\
&= x_n - x_n + 1/\alpha \\
&= 1/\alpha
\end{align*}
No! because you need to divide.

Answer: $f(x) = \alpha - 1/x$. Not transparent, because intuitively it
looks like there's a divide into it.
$f'(x) = - -1/x^2 = 1/x^2$. Then, $f/f' = \alpha x^2 - x$. So given $x_n$,
the iteration goes $x_{n+1} = x_n - (\alpha x_n^2 - x_n) = 2x_n -
\alpha x_n^2 = x_n(2-\alpha x_n)$.

Numerical example in matlab: let $\alpha = 2$, solve with this method. Use $x_0
= 0.1$. Notice that $err(i)/err(i-1)^2$ is constant and \emph{is}
$\alpha$.
\begin{align*}
  \frac{|x-x_{n+1}}{x-x_n}^2 &\approx 1/2\frac{f''(x_n)}{f'(x_n)}\\
\end{align*}
Notice the $err(i)$ decreases faster as iteration moves on, this is
the super linear convergence property of Newton's method.
With $\alpha = 0.25$, same thing.\\

\textbf{Analysis of the Newton's Method}:
$|x_{x_{n+1}}|/|x-x_n|^2 \approx 1/2 | f''(x_n)/f'(x_n)| \approx 1/x$
So @ $x = 1/\alpha$, this turns out to be $\alpha$, just for this
example.

The ratio was derived from the idea to find a patter in the errors
s.t. $e_{n+1} ~ c e_n^2$, for some $c\in \mathbf{R}$

The question is to find trends in data, and this relationship $e_{n+1}
~ ce_n^p$ is useful to tell us the rate of convergence as the solution
approches the optimal one. (I think $p$ goes down to the golden
ratio). Our goal is to find what $p$ is for a specific problem.
\pagebreak
\section{September 6th class 2}
\label{sec:class2}

\subsection{Process of Scientific Computing}

\begin{itemize}
\item Start with a mathematical model. (In general we don't have an analytic solution, so we get
  insight from numerical computation)
\item Example with heat conduction in a bar:
  \begin{itemize}
  \item A 1-D object $\in [0, 1]$, $u(x)=$temperature in the bar, with
  $u(0)=0, u(1) = 0$. $q$=heat flow induced by a heater of intensity $f$.
  \item We want what the temperature will be given $q$.
  \item get some models: \emph{Fourrier's Law}: $q= -ku'$, $k$=conductivity
    coefficient, transfer of heat in direction of decreasing
    temperature (hence the -). \emph{Conservation of energy}: $q' = f$.
  \item \textbf{Goal}: find $u$. \textbf{Equation of interest}: $-(ku')' = f$, the 1D diffusion
    equation.
  \item Typical strategy: lay down a grid $x_o=0, x_1, \dots, x_n,
    x_n+1=1$. Compute a discrete solution, $\bar{u}$, vector of size $n$. $\bar{u} = [u_1, \cdot, u_n]^T$, where $u_i \approx u(x_i)$
  \item \textbf{Claim}: we can find the discrete sol, $\bar{u}$ by
    solving an algebraic system of equations. For this example, this
    system is a matrix equation $$A\bar{u} = \bar{b}$$

  \end{itemize}
  \item No matter how hard I try, we're never going to get the exact
    solution. This process $A\bar{u}=\bar{b}$ leads to errors
  \item \textbf{Sources of error}:
    \begin{enumerate}
       \item Modeling error: we may not know $k$ exactly.
       \item Discretization error/Truncation error: difference between the discrete and
         the continuous values (from the approximation on a discrete
         set of points)
       \item Representation error: we don't have the entire
         $\mathbf{R}$, we only have a finite set, in floating point
         format. ($A$ and $b$ may have error)
       \item Additional error: from the computation of $\tilde{u}$,
         will get something else $\hat{\tilde{u}}\neq \tilde{u}$\\
        \textbf{we can show}: $$\frac{||\hat{\tilde{u}} -
           \tilde{u}||}{\hat{\tilde{u}}||} \le K(A)\mu$$ That is if we
         solve the problem appropriately (like being careful about
         pivoting etc).
         
    \end{enumerate}
  \item We're really trying to solve $\tilde{A}\tilde{u} =
    \tilde{b}$., where $\tilde{A}\approx A$, $\tilde{u} =
    \bar{u}$. Typically $\approx$ is machine precision, $10^{-16}$

  \item In the end, we want $u(x)$, and would be happy with $u(x_j)$,
    $j=1,\dots, n$. That will get $\tilde{u_j}$
  \item The moral is that when we do this stuff, we're not just doing
    mathematics. 
\end{itemize}

\subsection{Floating Point Arithmetic}
\label{sec:floating}

decimal numbers (base 10).
Consider the example 6522 and 10.31
\begin{itemize}
\item $6522 = 6(10^3) + 5(10^2) + 2(10^1)+ 2(10^0)$
\item $10.31 = 1(10^1) + 0 + 3(10^-1)+ 1(10^-2)$
\item Normalize the numbers! then,,
\item $6522 = 6.522 \times 10^3$ so we can express numbers with one digit to the
  left o fthe decimal point.
\item $10.31 = 1.031 \times 10^1$
\item For any number but 0, it has a fomr $z\times 10^p$, where
  $z\in[1, 10)$.
\end{itemize}

Computers use binary representation. Example: $3_{10} = 1(2^1) + 1(2^0) =
11$ or $1.1000 \times 2^1$.

$23_{10} = 16 + 4 + 2 + 1 = 1(2^4) + 0(2^3) + 1(2^2)+ 1(2^1)= 10111_2$
or $1.0111 \times 2^4$ \emph{normalized}. Here, normalized means
$z\times 2^p$, where $z\in [1, 2)$

\pagebreak
\section{September 8th Class 3}
\label{sec:class3}

\subsection{Floating Point Operations}

Example: addition using d=5 binary digits

Add $3+23$.\\ \noi
Normalized 5-digit binary expressions:

$3 = 2^1 + 2^0 = 1.1000 \times 2^1$

$23 = 16 + 4 + 2 + 1 = 1.0111 \times 2^4$

To add these, shift the smaller number so that the exponents agree.

\begin{align}
1.1000 \times 2^1 & = 0.11000 \times 2^2 \\
& = 0.01100 \times 2^3 \\
& = 0.00110 \times 2^4
\end{align}



May need to round the result: 
3+34
32+2 = 

Shift smaller number
\begin{align*}
3 &==> 0.000110 \times 2^5\\
34 &==> 1.00010 \times 2^5\\
---&-------------\\
37 &==> 1.00101  \\
\end{align*}
\noi
The way the arithmetic is done: take 2 floating point numbers. The computer
hardware will take the correct result (stored temporarily), but will always
round to $d$ digits. In our examples we are using only $d=5$. in the real world we are usually dealing with $d=27$ or more. number will always be rounded to closest representation-- can be either up or down. 

so in this case, the stored result is rounded to $1.0011 \times 2^5$ which is $32+4+2 = 38$. 

Exercise: what two number would have the property whose sum's \textbf{correct}
result is 1.001001? Answer: 32+4+0.5. (Note: $2.5$ in binary is $1.0100 \times 2^1 = 2 + 1/2$). 

In general, given the real $x$, $fl(x) = $ closest floating point
number to $x$. $|x = fl(x)|$ is called the rounding error. For
operations op = $\{+,-,\times,\div\}$, if $x$ and $y$ are floating
point numbers, then $fl(x\text{ op }y) =$ floating point number closest to $x$ $op$ $y$. 

It is possible that both inputs are perfectly good FP numbers, but that the
result of the operation is not able to be represented in the floating point
system (eg. d is too small). For example, in our single precision example,
$d=23$, $m = -126 \le p \le M = 127$. Then, if $x = 1 \times 2^{100}$ and $y = 1 \times
2^{100}$ then $x*y = 1\times2^{200}$ which results in an overflow. 

\textbf{Definition} \emph{Machine Precision} or \emph{Machine Epsilon}
is the smallest floating point number $\mu$ such that $fl(1+\mu) \neq 1$ $(> 1)$

In IEEE arithmetic, for any real $x \neq 0$:

$$
\frac{|x - fl(x)|}{|x|} \le \mu
$$

Example: $d=5$. Find $\mu$. 

\begin{align*}
1 &= 1.0000 \times 2^0\\
\text{Add }z &= 0.00001 \times 2^0\\
--&------------\\
&=1.00001  
\end{align*}
=> which gets rounded to 1.0001. 

$fl(1+z) \neq 1$

Next smallest number to $z$. Normalized, $z = 1.0000\times 2^{-5}$. Next smaller number is $z = 1.1111 \times 2^{-6}$. 
\begin{align*}
1&= 1.00000\\
 +z&=0.0000011111\\
--&---------------\\
&=1.0000011111
\end{align*}
 

which gets rounded to 1.

In general, in $d$-digit binary arithmetic, the machine precision $\mu$ is
$2^{-d}$. 

For IEEE single precision, this is $2^{-23} \approx 1.2 x 10^{-7}$
For IEEE double precision, this is $2^{-53} \approx 1.1 x 10^{-16}$

Does IEEE arithmetic support the existence of numbers like $2^m$ or $2^M$?
\subsection{Relative Error}

We have a number $x$, and a representation of $x \approx \hat{x}$.

The relative error is given by

\[
\frac{||x - \hat{x}||}{||x||}
\]

And the absolute error is given by $||x - \hat{x}||$. 

The relative error is more important than the absolute error. Consider
for example a census, x\% absolute error in the number of ppl in class
compared to the same x\% absolute error in the population of nyc does
not mean the same thing.


\subsection{Forward vs. Backward Error Anlysis}

Generically speaking: we are seeking $y = f(x)$, and get $\hat(y) \neq y$. We want insight into $\frac{||y - \hat{y}||}{||y||}$. 

\textbf{Definition:} \emph{Forward error analysis} tries to keep track of errors as the computation proceeds. 

Example:

Solve $Ax = b$. We get $\hat{x}$ instead. And we are interested in $\frac{||x -
\hat{x}||}{||x||}$. Forward error analysis (egf. for gaussian elimination) is
not possible. (Well, it is possible, but the estimates tend to be wildly
inaccurate). 

\textbf{Definition:}\emph{Backward error analysis} makes the claim that $\hat{x}$ is the solution of a perturbed problem, $\hat{A}\hat{x} = b$ such that (if the solution is done right):

$$
\frac{||x - \hat{x}||}{||x||} \lessapprox \mu (p(n) \mu)
$$

where $p(n)$ is a slowly growing function of $n$ = problem size. 

We use this observation:
$Ax = b$
$\hat{A}\hat{x} = b$

and 

\begin{align}
A(x - \hat{x}) &= Ax - A\hat{x} \\
& = Ax - \hat{A}\hat{x} + \hat{A}\hat{x} - A\hat{x} \\
& = b - b + (\hat{A} - A)\hat{x} \\
& = 0 + E
\end{align}

where $E$ is our error. 

\begin{align}
& \rightarrow x - \hat{x} = A^{-1}E\hat{x} \\
& || x - \hat{x}|| \le ||A^{-1}|| ||E|| ||\hat{x}|| \\
& ||A^{-1}|| ||A||  ||E|| / ||A|| ||\hat{x} ||
\end{align}

$||A^{-1}|| ||A||$ is called $\kappa(A)$, and $||E||/||A|| \lessapprox \mu$

this $\rightarrow ||x - \hat{x}|| / ||\hat{x}|| \lessapprox \kappa(A) \mu $

$\mu \approx 10^{-16}$

$\kappa(A)$ depends on $A$, the statement of the problem.. 

We claim, that with a bit more work, we could put $||x||$ in the denominator. 
\pagebreak
\section{September 13rd Class 4}
\label{sec:class4}
\subsection{Intro to Probability}
\label{sec:probability}

\textbf{Discrete models}: Consider a game with a finite or countable
number of outcomes, $\Omega$, the \emph{sample space}.
\textbf{Definition:} \emph{Probability} for each $\omega_j \in
\Omega$, we have a number $p_j(\omega_j) = p_j$ s.t. $p_j \in [0, 1]$,
and $\sum_{j=1}^{\infty} p_j = 1$

Examples:
\begin{enumerate}
\item Roll a fair die outcomes: $\Omega=\{1,2,3,4,5,6\}$, $p_j = 1/6$, $j=1,2,\dots,6$
\item Roll two fair dice, outcomes: $\Omega=\{(1,1), (1,2), \cdots, (1,6),
  (2,1), \cdots, (6,6)\}$, $p_j = 1/36$, $j=1,\dots,36$.
\item put $n$ distinct balls into $N$ urns $N>n$. 1st ball has $N$ choices, so does the 2nd ball, etc, so
  the total number of outcomes is $N^n$.
\item A die is rolled until a six appears. possible outcomes:
  \begin{enumerate}
  \item Get a 6 on 1st trial: 6, $p(\omega_1)=1/6$
  \item something other than 6 on 1st trial: 16,26,36,46,56 (5 ways),
    $p(\omega_2) = 5/6(1/6) = 5/36$
  \item etc $p(\omega_i) = (1-p(\omega_1))^{i-1})p(\omega_1) = (5/6)^{i-1}(1/6)$
  \end{enumerate}
Here $\Omega$ is countably infinite. The sum:
\begin{align*}
\sum_{j=1}^\infty p_j &= 1/6 + (5/6)(1/6) + (5/6)^2(1/6) + \cdots\\
&=1/6\sum_{l=0}^\infty (5/6)^l\\
 &= 1/6(\frac{1}{1-5/6}) = 1/6(\frac{1}{1/6}) = 1
\end{align*} 

 \textbf{Definition:} An \emph{event} is a subset $A \subseteq
 \Omega$. Notation $P(A) = \sum_{\omega_J \in A}
 p(\omega_j)$. Properties: $\P(\Omega) =1, P(\empty)=0,
 P(A^c)=1-P(A)$.
If $A_1, A_2, \dots$ are pairwise disjoint, then $P(\cup_i A_i) =
\sum_i P(A_i)$.

\textbf{Definition:} A \emph{random variable} $X$ is a function
$X:\Omega \rightarrow \mathbf{R}$. $\forall E\in \mathbf{R}$, define
an \emph{event} $\{X\in E\} = \{\omega \in \Omega | X(\omega) \in E\}$
Can talk about probability of such an event as $P(X\in E)$

\textbf{Definition:} A \emph{distribution} of a discrete random
variable $X$ is a collection of distinct real numbers, or a set of
values $\{m_j\}\in [0,1]$, s.t. $\sum m_j = 1$, and $P(X=x_j) = m_j$.

Example: \emph{Binomial distribution}. Consider a random experiment
with two possible outcomes. Psobability of success = $p$, failure
$q=1-p$.
With coins, $p=1/2$. Perform this experiment $n$ times, $X=$\# of
successes. $X$ has a $Binom(p,n)$. The probability of exactly $l$
sucesses is $P(X=l) = {n \choose k} p^kq^{n-k}$
\pagebreak
\section{September 15th Class 5}
\label{sec:class5}

\subsection{Probability Cont.}
\label{sec:probability}
Experiment: three tosses of a coin $|\Omega|=2^3$

\textbf{Definitino: } The distribution of discrete random variables is
a collection of real positives $\{x_j\}^{\infty}_{j=1}$, s.t.  $P(X=x_j) = m_j$


\textbf{Definition: } A \emph{mass function} of a discrete random
variable $X$ is $m:\mathbf{R}\rightarrow [0, 1]$ s.t. $m(x)=P(X=x)\forall x
\in \mathbf{R}$.

\textbf{Definition:} The binomial distriubtion is a random experiment with two outcomes where
$p$ is the probability of success, $1-p$ is the probability of
failure. It performs the experiment $n$ times, and $X$ denotes the number of total
successes.

Q: what is the probability of exactly $k$ successes? ${n \choose k}
p^k(1-p)^{n-k}$, let's check $\sum_{j=1}^\infty m_j =1$. This is
$\sum_{k=0}^np^k(1-p)^{n-k} = (p + (1-p))^n = 1^n = 1$ so yes.

\textbf{Definition:} Continuous random variables and
distributions. ex: Average weight of 100 randomly selected
, where $\Omega$=$\{$100 tuples of weights $\}$, $X(\omega) =1/100$
(sum entries of $\omega$). Random number uniformly chosen from $[a,b]$.

\textbf{Definition:} A function $f$ on $\mathbf{R}$ is called a
\emph{density function} if it's non-negative and integrable and
$\int_{-\infty}^\infty f(x)dx = 1$.

A random variable with a density function $f$
means $$P(\{ \omega \in \Omega | X(\omega) \in A \}) = \int_Af(x)dx$$
for $A\subseteq \mathbf{R}$.

example: we say $X$ is uniformly distributed on $[a,b]$ if it has
density function 
$$f(x)=
\begin{cases}
  \frac{1}{b-a} & \text{ for } x\in [a,b]\\
  0 & \text{ otherwise }\\
\end{cases}
$$

Alternatively, $X$ is normally distributed with mean $\mu$ and
variance $\sigma^2$ if 
$$f(x) = \frac{1}{\sqrt{2\pi \sig^2}}e^{-\frac{(x-\mu)^2}{2\sig^2}}$$
Exercise: check $\int_{-\infty}^{\infty} f(x)dx=1$

\textbf{Defintion:} \emph{mean}, $\mathbf{E}(x)=\mu$,  of a random variable. Discrete case:
$\mu = \sum_{x\in \mathbf{R}}x\times m(x)$, continuous case:
$\mu = \int_{-\infty}^{\infty}x\times f(x)dx$
$$
\begin{cases}
  \sum_{x\in \mathbf{R}} x\times m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty x\times f(x) & \text{ Continuous } 
\end{cases}
$$

\textbf{Definition:} \emph{Distribution of a continuous r.v.}: $F(x) =
P(X(\omega) \le x) = \int_{-\infty}^xf(\xi)d\xi$. So if $F(x)$ is
normal with $\mathcal{N}(\mu, \sig)$, $\mu$ is at the tip of the
gaussian mountain, and $\mu$ has val $0.5$ in the CDF, $F(x)$.


Let $\Phi: \mathbf{R} \rarr \mathbf{R}$. \emph{Claim:} 
$$E(\Phi(x)) =
\begin{cases}
  \sum_{x\in \mathbf{R}} \Phi(x)m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty \Phi(x)f(x)dx & \text{ Continuous }  
 
\end{cases}
$$
This is not obvious.

Now how to define R.V, $X$, formally. $\Omega \rarr \textbf{R}$. Using
the example of 3 coin tosses, $\Omega =
\{000,001,\dots,111\}$. $X(\omega) =$ decimal version of binary value
of $\omega$. Then, $X(\omega)= [1, 7] \in \textbf{N}$. Now, we can do:
$\forall x\in \textbf{R}$, define $m(x)$ by $$
m(x)=
\begin{cases}
1/8 & \text{ if } 1\ge x\le 7 \\
0 & \text{ otherwise}
\end{cases}
$$

Let us call 1 or more consecu, tive tosses of same type as \emph{run}s. This is a
new R.V. where $\Omega$ is the same, and $Y(\omega) = $number of
runs.

i.e. $Y(000) = 1 = \Phi(0), Y(001) =2=\Phi(1), Y(010)=3=\Phi(2),
Y(011)=2, \dots$. Note $Y=\Phi(X)$. So $m_y(y) = P(Y=y)$, and $m_y(1)
= 1/4, m_y(2)=1/2, m_y(3)=1/4, m_y(\text{else}) =0$. Then,
\begin{align*}
\mathbf{E}(Y) &= \sum_{y\in R}ym_y(y)\\
& = 1/4 + 2(1/2) + 3(1/4) = 2  
\end{align*}

This is the same as 
\begin{align*}
\sum_{x\in R}\Phi(x)m(x) &= (1+2+3+2+3+2+1)(1/8)\\
&=16/8=2
\end{align*}
\end{enumerate}
\pagebreak
\section{September 20th Class 5}
\label{sec:class5}

\subsection{Continue on Probability}
\label{sec:prob3}

Given random variable $X$ and $E(x)=\mu = \int_{-\infty}^{\infty}xf(x)dx$,
the variance is $var(x)=\sigma=E[(X-\mu)^2] = E(X^2)-E(X)^2$
If $x$ has a desity function $f(x)$, then
$E(\phi(x))=\int_{-\infty}^\infty \phi(x)f(x)dx$

\subsection{Monte Carlo Integeration}
\label{sec:montecarlo}
Let $X$ be a uniformly distributed random variable on $[0,1]$. If
$\phi:[0,1]\rarr \mathbf{R}$, suppose we want an approximation to
$\int_0^1\phi(x)dx = I(\phi)$.

Sample $X$ from $[0,1]$, and get $x_1, x_2, \dots, x_n$. We'll
approximate $I(\phi)$ by the average
$\frac{1}{N}\sum_{i=1}^N\phi(x_i)$. This is the quadrature rule
$Q_{MC, N}(\phi)$.

Notice: $\phi(x)$ is a random variable. The expected value of
$\phi(x)$ is $E(\phi(x)) = \int_{0}^1 \phi(x)dx = I(\phi)$, because $X$
is uniform. (so $f(x) =
\begin{cases}
1 & \text{ if }  x\in [0,1]\\
0 & \text{ otherwise}
\end{cases}
$)

The approximation $Q_{MC,N}(\phi) = Q_N(\phi)$ is called the sample
mean of $\phi(x)$. Approximates the mean of $\phi$, which is $I(\phi)$
or $\mu(\phi)$. i.e. a trivial example if $\phi(x) = x$, then $I(\phi)
= \int_0^1xdx = \frac{1}{2}x^2|_0^1 = 1/2$

Contrast this with standard ways to do quadrature.
\emph{examples:}
\begin{itemize}
\item  the trapezoidal rule: area under the curve is approximated by
  the trapezoid bewteen $f(a)$ and $f(b)$. $\int_a^b\phi(x)dx \approx 1/2(b-a)(\phi(a)+\phi(b))$
\item the simpson's rule: $\int_a^b\phi(x)dx\approx 1/6(\phi(a) +
  4\phi(\frac{a+b}{2}) + \phi(b))(b-a)$
\item Composite trapezoidal rule on [0, 1]: $\approx h/2\phi(x_0) +h\sum_i^{N-1}f(x_i) + h/2\phi(x_N)$
\end{itemize}
\subsubsection{Monte Carlo history}
\label{sec:mchistory}

From high energy physics by  John von Newmann and Stanislaw
Ulam. Monte Carlo, an island in the meditteranian where people gamble.

\subsection{Error Analysis}
\label{sec:erroranalysis}
Simpson's rule is better. The error analysis of sympson's rule
is: $I(\phi) - Q_s(\phi) \le ch^4 = c1/N^4 = O(1/N^4)$

With trapezoid it's $O(1/N^2)$. Is this better than monte carlo? \\In
$1D$, monte carlo does worse.


\emph{Claim:} error $|I(\phi) - Q_{MN, N}(\phi)| "\le"
O(1/\sqrt{N})$. Suppose $N=10000$, so $1/100$, but in simpson's rule's
error, $O(1/10^{16})$. Simpson's does way better.

 From probability analysis, using the Law of Large Numbers, if we let
 $S_N = \phi(x_1) + \cdots + \phi(x_N)$, and the sample mean $S_N/N=\mu_N$,
 (which is our approximated integral), by LNN,
 $\lim_{n\rarr\infty}P(|\frac{S_N}{N} - I(\phi)| < \eps) = 1$, where
 $I(\phi)$ is the real $\mu$. That is $P(|\frac{S_N}{N} -
 I(\phi)| > \eps) \rarr 0$ as $N\rarr \infty$. In words, it's ``very
 unlikely'' that $Q_{MC,N}(\phi)$ is different from $I(\phi)$ when $N$
 is very large.

\textbf{Chebyshev's inequality}: Given $c>o$, $$P(|\frac{S_N}{N} -
\mu|\ge c) \le \frac{\sig^2}{Nc^2}$$ Commentary: suppose we want this
probability to correspond to 95\% confidence, that is the probability on the left
to be $.05$, then  we require $.05 \le \frac{\sig^2}{Nc^2}$, solve for
$c$ and we get $c^2\le \frac{\sig^2}{.05N}$ or $c\le
\frac{\sig}{\sqrt{.05N}}$. So with $95\%$ confidence, $error\le
\frac{\sig}{\sqrt{.05}}\frac{1}{\sqrt{N}}$
That's what we can say about the error. If we want $99\%$ confidence,
then $\sqrt{.05}$ becomes $\sqrt{.01}$

Two cons: convergence is quite a bit slower, we're not as confident as
the other quadrature methods (the others give us a guarantee)

Suppose instead of uniform, say we had two box, $N$ samples of
2-tuples, then with double integral and think about monte carlo. The
procedure is the same, sample, evaluate the value at that sample, then
take the average. Suppose we have a cube, same thing. Nothing of monte carlo is
tied to the underlying domain. The analysis using LLN and chebyshev is
always the same.

Consider 2D. Simpsons rule on $[a,b]\times[c,d]$. 
\begin{align*}
\int_0^\phi\int_a^b\phi(x,y)dxdy &\approx \text{let } \Phi(y) = \frac{b-a}{6}(\phi(a,y)+4\phi(\frac{a+b}{2}, y) + \phi(b,y))\\
&\approx \frac{d-c}{6}(\Phi(c) + 4\Phi(\frac{c+d}{2}) + \Phi(d))
\end{align*}, where $\Phi(\frac{c+d}{2}) = \phi(\frac{a+b}{2}, \frac{c+d}{2})$
Total number of points $N=n^2$, $h=1/n = 1/\sqrt{N}$. The error is
$O(h^4)= O(1/n^4) = O(1/N^2)$

In 3D, the error on simpsons is still $O(h^4)$, but $h=1/n, N=n^3$, so
this is $O(1/N^{4/3})$, in $D$ dimensions, it's $O(1/N^{4/d})$)

In monte carlo, the error analysis is the same in any dimension. Has
nothing to do with the integral. so the big pro is that we can never
be certain but we're always working with the same error around
$\frac{\sig}{\sqrt{a}}\frac{1}{\sqrt{N}}$.

Now, which technique is better? If we ignore the uncertainty, we're
asking when is $\frac{1}{N^{4/d}} < \frac{1}{N^{1/2}}$?

\begin{align*}
\frac{1}{N^{4/d}} &< \frac{1}{N^{1/2}}?\\
N^{1/2} &< N^{4/d}\\
\frac{1}{2} &<\frac{4}{d}\\
d &< \frac{4}{1/2} = 8
\end{align*}
So when $d<8$, simpson is better, else, MC is better. (Although we're
ignoring the constant and the uncertainty factor so it won't be exact
like this but still around there)

In model of particle physics, the number of dimensions is basically is
equal to the number of particles, which could be hundreds. It's not
unusual to have large $d$.

\emph{Example:} of high dimensional integral (in the text/wiki). In
particle physics, a \emph{partition function} $Z(\beta)$ describes the
statistical properties a system of $d$ particles, thermodynamic
equilibrium, \\$Z(\beta) = \int exp(-\beta H(p_1, \cdots, p_d, x_1,
\cdots, x_d)d^3p_1\cdots d^3p_dd^3x_1\cdots d^3x_d$, \\where this
integral is a $3d$ integral. where $d=$ number of particles, and
$\beta = \frac{1}{\alpha\tau}$, a Boltzmann constant, $\tau$ is
temperature, and $H$ is hte hamiltonian function. And the point is
that people care about this. This is about a 3 million dimensional
integral.

\pagebreak
\section{September 22 Class 6}
\label{sec:class6}

\subsection{Continue on Monte Carlo}
\label{sec:montecarlo2}
Monte Carlo Integration: Given a r.v. $Y$, $P(Y\subseteq
A)=E(I_A(Y))$, where $I_A(Y) =
\begin{cases}
 1 & \text{ if } Y\in A\\
 0 & \text{ otherwise} 
\end{cases}
$. 
And $E(I_A(Y)) = \int_AI_a(y)dy$, where the integral can be
multi-dimensional. One way to deal with high-dimensional integral is
to use Monte Carlo Integration.\\

\noi
\emph{Review}: Monte Carlo gives us the approximation $\int_{\mathbf{R^D}}\phi(x)dx =
I(\phi)$, where $I(\phi) \approx 1/N\sum_{n=1}^NQ(x_n) = Q_{MC,N}(\phi)$ where $\{x_n\}$
are samples of randome variable $X$.
\\
We showed that the error was $|I(\phi) - Q_{MC,N}(\phi)|"\le"
\frac{\sig}{c\sqrt{N}}$

\noi
If the goal is to make this bound $\le \tau$ tolerance,
i.e. $\frac{\sig}{c\sqrt{N}} \le \tau$, then we need 
\begin{align*}
\sqrt{N} &\le \frac{\sig}{c\tau}\\
N &\le  \frac{\sig^2}{c^2\tau^2}  
\end{align*}
many samples.
\subsection{Variance Reduction Methods}
\label{sec:variancereduction}
The aim is to reduce $\sig$
2 examples of idea to do this

\subsubsection{Antithetical variables}
Assume $\phi \in [0,1]$, and $I(\phi)
  =\int_0^1\phi(x)dx = E(\phi(x))$ (remember !!) where $X$ is a r.v. from a uniform
  distribution on $[0,1]$. ($E(x) = \int_0^1xdx = 1/2x^2|_0^1 = 1/2$,
  and we approximate $I(\phi)$ with
  $1/N\sum_n=1^NQ(x_n)$).\\ Antithetical variables are $x$, and $x^c$
  s.t. $x+x^c = 1$ or  $x^c= 1-x$, $x = \mu + d$, $x^c = \mu - d$

Assume $\sig$ small. Write $d = \sig \hat d$ ($d$ will not grow too
much because they're from a bounded distribution..) Let $D = X - \mu$,
another r.v., and let $\hat D = D/\sig$ also a r.v. Then,
\begin{align*}
E(\hat D) &= \frac{1}{\sig}E(D)\\
&= \frac{1}{\sig}E(x-\mu)  \\
&=\frac{1}{\sig}[E(x) - E(\mu)] \\
&=\frac{1}{\sig}[\mu-\mu] = 0
\end{align*}
Now, consider $\phi(x)$. (he used 1/2 as $\mu$ following the example on board)
\begin{align*}
\phi(x) &= \phi(\mu + d) = \phi(\mu + \sig\hat d)\\
&= \phi(\mu) +   \phi'(\mu)\sig \hat d + \phi''(\xi)\sig^2 \hat d^2
\text{ by taylor series}\\
&\approx \phi(\mu) +   \phi'(\mu)\sig \hat d + O(\sig^2)\\
\end{align*}
And $\phi(x^c)$
\begin{align*}
  \phi(x^c) &= \phi(\mu - d) = \phi(\mu - \sig \hat) \\
&\approx \phi(\mu) - \phi'(\mu)\sig\hat d +  O(\sig^2)
\end{align*}
SO, 
$$\frac{\phi(x)+\phi(x^c)}{2} = \phi(\mu) + O(\sig^2)$$
Now look at $I(\phi) = E(\phi(x)) = \int_0^1\phi(x)dx$. This is
\begin{align*}
 &=E[\phi(\mu) + \phi'(\mu)\sig \hat D + O(\sig^2)]\\
&=\phi(\mu) + \phi'(\mu)\sig E[\hat D] + O(\sig^2)\\
&=\phi(\mu) + 0 + O(\sig^2)
\end{align*}

Now a new integration method. Smaple $X$ $N$ times and get $x_1,
\dots, x_N$, also let $x^c_1,\dots, x^c_N$. Let the complementary
quantities:
$$
Q^c_{MC,N}(\phi) =  \frac{1}{2N}(\phi(x_1) + \phi(x_1^c) + \cdots +
\phi(x_n) + \phi(x_n^c)
$$

For each $n$,
\begin{align*}
 \phi(x_n) &= \phi(\mu + \sig \hat d_n) = \phi(\mu) + \phi'(\mu)\sig
 \hat d_n + O(\sig^2)\\
 \phi(x_n^c) &= \phi(\mu - \sig \hat d_n) = \phi(\mu) - \phi'(\mu)\sig
 \hat d_n + O(\sig^2)\\
1/2(\phi(x_n) + \phi(x_n^c)) &= \phi(\mu) + O(\sig^2)\\
\Rightarrow Q^c_{MC,N}(\phi) &= \frac{1}{N}(N\phi(\mu) +
N(\phi(\sig^2)) = \phi(\mu) + O(\sig^2)\\
\end{align*}
Now the difference: $I(\phi) - Q_{M,N}^c(\phi) = \phi(\mu) - \phi(\mu)
+ O(\sig^2)$
So \textbf{the error is 1/4th the size} or $\sig^2$ instead of
$\sig$.
Smmmary:
\begin{enumerate}
\item[Method 1] Take $N$ samples, error ~ $\frac{\sig}{\sqrt{N}}$
\item[Method 1] Take $2N$ samples, error ~ $\frac{\sig}{\sqrt{2N}}$
\item[Method 2] Take $N+N_{\text{antithetical}}$ samples, error ~ $\frac{\sig^2}{\sqrt{N}}$
\end{enumerate}

\subsubsection{Importance Sampling}
\label{sec:importancesampling}
$I(\phi) = \int_0^1\phi(x)dx$, written in a different way
is $$\int_0^1 \frac{\phi(x)}{p(x)}p(x)dx$$
Where $p$ is a positive function on $[0,1]$ s.t. $\int_0^1 p(y)dy=1$
. Notice $p(x)$ is a density function.

Suppose we can randomly sample variable $x$ from the distribution defined by
$p$. This means $P(X\in A) = \int_A p(x)dx$.

\noi \textbf{Rule:}
\begin{enumerate}
\item Sample $x_1, x_2, \dots, x_n$
\item Form the MC cost estimate, $I_N(\phi) = \frac{1}{N}\sum_{n=1}^N\frac{\phi(x_n)}{p(x_n)}$
\end{enumerate}

The variance of this process is $$\hat \sig^2 = \int_0^1
(\frac{\phi(x)}{p(x)} - I(f))^2p(x)dx$$
Error is proportional to $\frac{\hat\sig}{\sqrt{N}}$
We want $p$ s.t. $\hat \sig < \sig_{\phi}$
$I(f)$ is just a number. We would like $p$ close to
$\frac{\phi}{I(\phi)}$. But we don't know what $I(\phi)$ is.

\pagebreak{}
\section{September 27th Class 7}
\label{sec:class7}

\subsection{Continue on Importance Sampling}
\label{sec:importancesampling}

For variance reduction, we're computing $\int_0^1\phi(x) dx$, MC says
that this is approximated by $\frac{1}{N}\sum_i^N\phi(x_i)$. We'll write the
integral as $I(\phi) = \int_0^1\frac{\phi(x)}{p(x)}p(x)dx$, where $p$ is
positive and $\int_0^1p(x) = 1$, i.e. $p(x)$ is a density function.\\

We know that the error for MC is
$~\frac{\sig_{\phi}^2}{\sqrt{N}}$. Supposition is that we can sample
$\{x_i\}$ from a distributionwith density $p(x)$, and sampling will
approximate $I(\phi)$ by
$\frac{1}{N}\sum_i^N\frac{phi(x_i)}{p(x_i)}$. The variance of this
process: $\hat \sig^2 = \int_0^1(\frac{\phi(x)}{p(x)} -
I(\phi))^2p(x)dx$, and the error $~ \frac{\hat \sig^2}{\sqrt{N}}$, and
$\hat \sig < \sig_p$. We need $p$ and be able to sample from it to do
this.


We need a way to define $p(x)$ and a technique to sample from it. 

\subsubsection{Technique: Accept/Reject sampling method}
Supposed we have $q(x)\ge p(x)$ ($q$ could be a constant too).
Let $\hat q(x)= \frac{q(x)}{\int_0^1q(x)dx} = \frac{q(x)}{I(q)}$, and
so $I(\hat q) = 1$ (scaled version of q). (in the hw $\hat q$ is just
a constant)

\emph{Algorithm} to draw random numbers with density $p(x)$. 
\begin{enumerate}
\item Pick two random numbers $x'$ with density $\hat q$ and $y$ from
  uniform distribution [0,1].
\item Then make a decision: Accept $x'$ as one of my samples if $y\le \frac{p(x')}{q(x')}$,
  reject otherwise.
\item \emph{claim} the number of accepted samples is approximately $\frac{1}{I(q)}$
\end{enumerate}

Why does this work? \\Consider a function $C(\xi) = C_\ah(\xi) =
\mathbf{X}(\xi \le \ah)
\begin{cases}
  1&\text{ is }\xi \le \ah \\
0 & \text{ otherwise}
\end{cases}$. For $\ah\in [0,1]$, $\int_0^1C_\ah(\xi)d\xi =$ area under line from 0
to $\ah$, which is $\ah$. Now..use
\begin{equation}
  \label{eq:Cxi}
C(\xi) =
C_{\frac{p(x)}{q(x)}}(\xi)
\end{equation}
where $x$ is given.

We want to know if 
\begin{equation}
  \label{eq:wewant}
\mathbf{E}(X) \approx \int_0^1xp(x)dx =
\frac{1}{N}\sum_j^Nx_j
\end{equation}
 (we can take out $p(x)$ because of the way we
sampled $x_j$). Notice $p(x) = \frac{p(x)}{q(x)}q(x) =
\frac{p(x)}{q(x)}\hat q(x)I(q)$, now using \ref{eq:Cxi}, this is the
same as $\int_0^1C_{\frac{p(x)}{q(x)}}(\xi)d\xi \hat q(x)I(q)$.
Then,
\begin{align*}
\int_0^1xp(x)dx &=
\int_0^1xdx \int_0^1C_{\frac{p(x)}{q(x)}}(\xi)d\xi \hat q(x) I(q)\\
&=\int_0^1\int_0^1x C_{\frac{p(x)}{q(x)}}(\xi)\hat q(x)d\xi dx I(q)\\
&\approx \frac{1}{N'}\sum x_i C_{\frac{p(x)}{q(x)}}(\xi_i)I(q)\text{
  this step was doing MC to estimate}\\
\end{align*}
Using $N'$ samples from the distribution for $x$ taken from $\hat q$,
and $\xi$, taken from uniform dist. $C(\xi_i)=1$ if $\xi_i \le
\frac{p(x)}{q(x)}$, 0 otherwise. So the entire thing is equal to 
$$
\frac{1}{N}\sum_{\text{indicies of accepted samples}}x_i
$$
Using the claim that $\frac{N=\text{\# accepted}}{N'=\text{\# sampled}}
= \frac{1}{I(q)}$
($\frac{I(q)}{N'} = \frac{1}{N}$) So this is what we wanted \ref{eq:wewant}.

\textbf{Proof of the claim}: Rule is accept if $y\le
\frac{p(x')}{q(x')}$, which is $yq(x')\le p(x')$.
For a given $y$, $P(\text{resulting experiment leads to accept }x')$ is deteremined by integrating
$y\int_0^1q(x')dx' \le \int_0^1p(x')dx'$. So
$$y \le \frac{I(p)}{I(q)} = \frac{1}{I(q)}$$ because $p$ is a density
function.

Then $P(\text{experiment leads to acceptance}) = P(\text{sample $y$
  from uniform }\le \frac{1}{I(q)}) = \frac{1}{I(q)}$ (since
  $\int_0^{\frac{1}{I(q)}}dy = \frac{1}{I(q)}$

This probability is approximately just $\frac{N}{N'} \approx
\frac{1}{I(q)}$, which is our claim! (Reference R. Caflisch Acta
Numerica 1:49, 1998 Monte Carlo and quasi-Monte Carlo methods)

\subsection{Matrix Factorization Methods}
\label{sec:matrixfactorization}
Four basic algorithms:
\begin{enumerate}
\item Gaussian Elimination (solving $Ax=b$ where $A$ is nonsingular)
\item QR Factorization (same, or solving least squares min$||Ax-b||$
  where $A$ is a long thin $m$by$n$ matrix, $m>>n$)
\item Singular value decomposition (also for least squares)
\item Eigenvalue decomposition (SVD and this are used to solve eigen
  value problems, $Av= \lambda v$, find $v, \lambda$
\end{enumerate}

\pagebreak
\section{October 4th Class 8}
\label{sec:class8}

Corrections for hw 2:
\begin{itemize}
\item Problem 1(a): $f(X) \in [0,1]$, not $[-1,1]$
\item Problem 3: should read $\rho(x) = \frac{4}{10}\hat \rho(x)$
  because we need $\int_0^1\rho(x)dx = 1$. Also $q(x)=1.6$
  constant. When sampling, $q(x)$ induces a density function $\hat
  q(x)$ by $\hat q = \frac{q}{\int q(x)dx}=\frac{1.6}{1.6}$, which means that we
  still sample from $U(0,1)$, but when you compare the ratio $p/q$, we
  use 1.6.
\item For histogram \& density estimation: If we have a pdf $p(x)$,
  the $\int_{D~X}p(x)dx = 1$. For r.v. $X$ with density $p$, then
  $P(\ah \le X\le \beta) = \int_\ah^\beta p(x)dx$. 

If we're sampling $\{x_i\}$ from such an $X$(using reject/accept), then the number of $\text{num of
}x_i \in [\ah,\beta]/\text{num of samples} \approx \int_\ah^\beta
p(x)dx$. This is called the kernel density estimate. The goal here is
to approximate $p(x)$ by some constant $p_c(x)$ on $[\ah, \beta]$. If
we had $p_c$, then $\int_\ah^\beta p(x) \approx p_c[\beta-\ah]$. Turn
this around to define $p_c$ as 
$$p_c = \frac{\text{\# of }x_i\in[\ah, \beta]}{\text{\# of
    samples}}\frac{1}{\beta-\ah}$$
Given $x$, let $[\ah_j, \beta_j]$ be the interval containing $x$,
define $p_c(x)=$ number of $x_i\in[\ah_j,\beta_j]$/number of
samples$\frac{1}{\beta_j-\ah_j}$, and this is the histogram he's looking for.
Does it come out the way it should?

In matlab use $hist$, and if you really want to compare/plot $p_c$,
specify bin set \texttt{hist(X, [1/8,3/8/,5/8,7/8])}, it will give you
exactly what you want (same centers). Better yet, let that hist be $z$
and do a bar graph \texttt{bar(z, centers)}.
\item For prob 1.b, do $log(\text{error})$vs$log(N)$ scale. Second
  graph is to ask whether things really look clean (no, not really bc
  of the probabilistic nature). Because if we were
  to use trapezoid, we'll get a very clean ratio between the error.
\item For prob 1.c, 3 sets, each of 4 pictures.. sample $E_N$ $n$
  times, try different sets of $n$s. (I did 1000, use that for
  $\frac{\sig}{n}$..?, but try different samples).
\end{itemize}

\subsection{Matrix Factorization}
\label{sec:matrix}

\subsubsection{Gaussian Elimination}
\label{sec:gaussian}
Solve $Ax=b$, where $A$ $NbyM$ is non-singular. Procedure also called $LU$
factorization.
\begin{enumerate}
\item Add multiple of first row of $A$ and first entry of $b$ to the
  second to the last rows to produce zeroes on the first column of $A$
  except the first entry. We get $\hat A = L_1A$
\item repeat this, then $L_{n-1}\cdots L_1=U$ the upper triangle.
\end{enumerate}

\emph{Cost}: number of floating point multiplication and divisions:
\begin{itemize}
\item $n-1$ for column of $L_1$.
\item $(n-1)(n-1)$  to produce $\hat A$
\item Just the operation on $A$ is $n(n-1)$ for step 1.
\item plus $(n-1)(n-2)$ for step 2
\item $\cdots$ and we get $(n-(n-2))(n-(n-1)) = 2$ at step $n$
\end{itemize}
So total is 
\begin{align*}
  \sum_{j=1}^{n-1}j(j+1)&= \sum_{j=1}^{n-1}j^2 + \sum_{j=1}^{n-1}j\\
&=\sum_{j=1}^{n-1}j^2 + \frac{n(n-1)}{2}\\
&= O(\frac{1}{3}n^3) + n^2 = O(n^3)
\end{align*}
Exercise: Identify how to get $x$, \emph{back substitution} and what's
the cost? ($O(n^2)$)

What happens if $a_{00} == 0$?
Since $A$ nonsingular, we know that there must be at least one
$a_{j1}\neq 0$. Take the $j$ with largest $|a_{j1}|$ and switch it
with $a_{11}$. Formally, the switch is done by multiplying the system
by $P_1$, a matrix with all diagonal values $1$ but $a_{11}$ and
$a_{jj}$ and all 0 but $a_{0j}$ and $a_{j1}$., the permutation matrix.

So result is $L_1P_1Ax = L_1P_1b$, where $L_1$ is taken from
$P_1A$. This is called \emph{pivotting}.
\pagebreak

\section{October 6th Class 9}
\label{sec:class9}

\subsection{More Linear Algebra}
\label{sec:linearAlgebra}
\subsubsection{Cont on Gaussian Elimination}
Solving $Ax=b$, $A$ nonsingular. Without pivoting, $L_{n-1}\cdots L_1A
= U$, where $L_{n-1}\cdots L_1 = L^{-1}$ Claim: $L^{-1}$ and $L$ are
lower triangulare
\begin{itemize}
\item product of lower triangular matrices is a lower triangular. 
\item Inverse of lower triangular matrices is lower triangular.
\end{itemize}
So we have $$A = LU$$
Formally, solving $Ax=b$ consists of forming $U=L^{-1}A$ (step 1a) and $\hat b = L^{-1}b$
(step 1b) and solving $Ux=\hat b$ for $x$ (step 2).

We showed last time that (1a) costs $O(n^3/3)$. Claim: cost of (1b) is
$O(n^2)$ and cost of (2) is $O(n^2)$.

Step 1 is $x_n=b_n/U_{n,n}$
Idea of solving $Ux=\hat b$ to do row $j$
\begin{verbatim}
for j=n:-1:1
   x_j = (\hat b_j - \sum_{l=j+1}^n U_{j,l}x_l)U_{j,j}
end
\end{verbatim}
For each step j, cost is:  $n-j + 1$ plus 1 for divide
So the total cost is $\sum_{j=1}^n(n-j+1)$, $n+1 - 1+n+1, -2+\cdots +
n+1-n = n+n-1 \cdots +1 $ so total $\frac{n(n+1)}{2}$

One thing we missed is what's the entries of matrix $L$ look like. We
know $U$, it's what's been left behind. \emph{Claim}: the entries of
$L$ are - cost of $L_i$'s that are constructed. i.e. first column is
$\begin{smallmatrix}
1\\ a_{21}/a_{11}\\a_{31}/a_{11}\\ \vdots \\ a_{nn}/a_{11}
\end{smallmatrix}$

To do this correctly, we may need to \emph{pivot}. When we pivot in
floating point, we compute a solution $\hat x$ that satisfies
$(A+\delta A)\hat x=b$, where $\frac{||\delta A||}{||A||}$ is
small. What we looked at in week 1, $\frac{||x-\hat x||}{||x||} =
||A||||A^{-1}||\frac{||\delta A||}{||A||} = \kappa(A)$

Now, look at the relative error $$\frac{||x-\hat
  x||}{||x||}=\frac{||x-\hat x||}{||\hat x||}\frac{||\hat
  x||}{||x||}$$
\begin{align*}
Ax&=b \\
(A+\delta A)\hat x &= b\\
A\hat x&= b-(\delta A)\hat x\\
\hat x &= A^{-1}b - A^{-1}(\delta A)\hat x\\
&= x -  A^{-1}(\delta A)\hat x\\
||\hat x|| &\le ||x|| + ||A^{-1}(\delta A)\hat x|| \text{ take the
  norm}\\ 
&le  ||x|| + ||A^{-1}|| ||(\delta A)||||\hat x|| 
(1-||A^{-1}||||(\delta A)|| )||\hat x|| &\le ||x||\\
1-\kappa(A)\frac{||\delta A||}{||A||}||\hat x|| &\le ||x||
\end{align*}
$\kappa(A)$ is a property of our matrix A. In numerical computing, we
can only work with A with reasonable conditioning number.\\
Put things together we have
$$\frac{||x-\hat
  x||}{||x||} \le \frac{ \kappa(A)\frac{||\delta A||}{||A||}}{1-\kappa(A)\frac{||\delta A||}{||A||}}$$
The way to think bout hte denominator:$\frac{||\delta A||}{||A||}$
small means aroung $10^{-12}$~$10^{-14}$~$10^{-16}$. Suppose
$\kappa(A)=500$, then the denominator is
$1-500(10^{-12})=1-5(10^{-10})$ not that bad at all. In fact, we can
deal up till like $5000$ can handle it until $10^{10}$ (for this
proble)

\subsection{Least Squares Problem}
\label{sec:leastsquares}

Given data $\{(x_i,y_i)\}$, find a simple function $p(x)$
s.t. $p(x_i)\approx y_i$. Example of such a function is
$p(x)=\ah_0+\ah_1x$ a linear polynomial. 

\noi
A least squares fit is to find $\ah_0$ and $\ah_1$ s.t. sum of squares
of $y_i-(\ah_0+\ah_1x_i)$ is minimized. $min
\sum_{i=1}^n[y_i-(\ah_0+\ah_1x_i)]^2$
Note: if we tried to make $p(x_i)=y_i$ for all $i$, we have
\begin{align*}
\ah_0+\ah_1x_1 &= y_1\\  
\ah_0+\ah_1x_2 &= y_2\\ 
&\vdots\\
 \ah_0+\ah_1x_m &= y_m\\  
\end{align*}
$m$ equations and $2$ unknowns. Write in matrix form:
$$\begin{pmatrix}
  1&x_1\\
  1&x_2\\
\vdots\\
  1&x_m\\
\end{pmatrix}
\begin{pmatrix}
  \ah_0\\ \ah_1
\end{pmatrix} = \begin{pmatrix}
  y_1\\ \vdots \\ y_m
\end{pmatrix}$$
Write this $Aa=y$. In general, there is no solution. Gvien any $\ah =
\begin{smallmatrix}
  \ah_0\\ \ah_1
\end{smallmatrix}$, let $r=y-Aa$, a vector of length $m$.

Notice $r_i=y_i-[Aa]_i=y_i-(\ah_0+\ah_ix_i)$. We can restate the goal
as to minimize $\sum_i^n r_i^2 = ||r||_2^2$

\end{document}
