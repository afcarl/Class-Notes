\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\kap}{\kappa}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}
% for cross product
\newcommand{\lc}{\langle} %<
\newcommand{\rc}{\rangle} %>

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}

\newcommand{\noi}{\noindent}
\parskip 5pt
\parindent 0pt

\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,algorithmic}
\begin{document}
\title{Scientific Computing CS660 Fall '11}
\author{Angjoo Kanazawa}
\maketitle
\section{September 1st Class 1}
\subsection{Logistics}
\label{sec:logistics}
Prof. Howard Elman CSI 2120 TR 2pm-3:15pm
class url:http://www.cs.umd.edu/~elman/660.11/syl.html
\begin{itemize}
\item Scientific Computing puts heavier emphasis on computing, Numerical
  Analysis is more about proofs/theories.

\item 4-6 hw asg: \textbf{35\%} Penalty on late assignments (-15\%
  after 24 hrs, -30\% after 48 hrs. 
\item in-class midterm: \textbf{25\%}
\item final project: \textbf{40\%}
\end{itemize}

\subsection{Content} 
\label{sec:content}
\textbf{Newton's Method}: Root finding. 
Objective: Fine $x$ s.t. $f(x) = 0$, where $f$ is a scalar,
$f:\mathbf{R}\Rightarrow \mathbf{R}$
function. Where does the function cross 0 (x-axis)?

Given $x_n$, some guess, find where the line through $(x_n, f(x_n))$
tangent to the solution curve intersects the $x$-axis. Call that
pt of intersection $x_{n+1}$

The equation of the tangent line: $$\frac{y-f(x_n)}{x-x_n} = f'(x_n)$$
Set $y=0$: then
\begin{align*}  \frac{0-f(x_n)}{x_{n+1}-x_n} &= f'(x_n)\\
  \frac{x_{n+1}-x_n}{-f(x_n)} &= 1/f'(x_n)\\
  x_{n+1} &= x_n - f(x_n)/f'(x_n)\\
\end{align*}

\textbf{Another Derivation}
Consider the taylor series $f(x_n+(x-x_n)) = f(x_n) + f'(x_n)(x-x_n) +
1/2f''(x_n)(x-x_n)^2 + \text{ etc}$. This is a function of some
variable $x$. Approximate it just by using the first two terms (linear
approximation).
So above becomes a new function $f(x_n+(x-x_n))~= f(x_n) +
f'(x_n)(x-x_n) = l(x)$.
Find where $l(x) = 0$. That is: $x_{n+1} = x_n - f(x_n)/f'(x_n)$

This is not guaranteed to work, i.e. when the tangent doesn't cross
the $x$-axis.


\textbf{Problem}: given $\alpha \in \mathbf{R} > 0$ Find $1/\alpha$ without
doing any division.
First thing you need to do if identify (concoct) a $f(x)$ whose root
is $1/\alpha$.
Naive: try $f(x) = x - 1/\alpha$. But this won't work, because this
requires division.
Howabout: $f(x) = \alpha x - 1$. $f'(x) = \alpha$, so in the newton
iteration.. 
\begin{align*}
x_{n+1} &= x_n - \frac{\alpha x - 1}{\alpha}\\
&= x_n - x_n + 1/\alpha \\
&= 1/\alpha
\end{align*}
No! because you need to divide.

Answer: $f(x) = \alpha - 1/x$. Not transparent, because intuitively it
looks like there's a divide into it.
$f'(x) = - -1/x^2 = 1/x^2$. Then, $f/f' = \alpha x^2 - x$. So given $x_n$,
the iteration goes $x_{n+1} = x_n - (\alpha x_n^2 - x_n) = 2x_n -
\alpha x_n^2 = x_n(2-\alpha x_n)$.

Numerical example in matlab: let $\alpha = 2$, solve with this method. Use $x_0
= 0.1$. Notice that $err(i)/err(i-1)^2$ is constant and \emph{is}
$\alpha$.
\begin{align*}
  \frac{|x-x_{n+1}}{x-x_n}^2 &\approx 1/2\frac{f''(x_n)}{f'(x_n)}\\
\end{align*}
Notice the $err(i)$ decreases faster as iteration moves on, this is
the super linear convergence property of Newton's method.
With $\alpha = 0.25$, same thing.\\

\textbf{Analysis of the Newton's Method}:
$|x_{x_{n+1}}|/|x-x_n|^2 \approx 1/2 | f''(x_n)/f'(x_n)| \approx 1/x$
So @ $x = 1/\alpha$, this turns out to be $\alpha$, just for this
example.

The ratio was derived from the idea to find a patter in the errors
s.t. $e_{n+1} ~ c e_n^2$, for some $c\in \mathbf{R}$

The question is to find trends in data, and this relationship $e_{n+1}
~ ce_n^p$ is useful to tell us the rate of convergence as the solution
approches the optimal one. (I think $p$ goes down to the golden
ratio). Our goal is to find what $p$ is for a specific problem.
\pagebreak
\section{September 6th class 2}
\label{sec:class2}

\subsection{Process of Scientific Computing}

\begin{itemize}
\item Start with a mathematical model. (In general we don't have an analytic solution, so we get
  insight from numerical computation)
\item Example with heat conduction in a bar:
  \begin{itemize}
  \item A 1-D object $\in [0, 1]$, $u(x)=$temperature in the bar, with
  $u(0)=0, u(1) = 0$. $q$=heat flow induced by a heater of intensity $f$.
  \item We want what the temperature will be given $q$.
  \item get some models: \emph{Fourrier's Law}: $q= -ku'$, $k$=conductivity
    coefficient, transfer of heat in direction of decreasing
    temperature (hence the -). \emph{Conservation of energy}: $q' = f$.
  \item \textbf{Goal}: find $u$. \textbf{Equation of interest}: $-(ku')' = f$, the 1D diffusion
    equation.
  \item Typical strategy: lay down a grid $x_o=0, x_1, \dots, x_n,
    x_n+1=1$. Compute a discrete solution, $\bar{u}$, vector of size $n$. $\bar{u} = [u_1, \cdot, u_n]^T$, where $u_i \approx u(x_i)$
  \item \textbf{Claim}: we can find the discrete sol, $\bar{u}$ by
    solving an algebraic system of equations. For this example, this
    system is a matrix equation $$A\bar{u} = \bar{b}$$

  \end{itemize}
  \item No matter how hard I try, we're never going to get the exact
    solution. This process $A\bar{u}=\bar{b}$ leads to errors
  \item \textbf{Sources of error}:
    \begin{enumerate}
       \item Modeling error: we may not know $k$ exactly.
       \item Discretization error/Truncation error: difference between the discrete and
         the continuous values (from the approximation on a discrete
         set of points)
       \item Representation error: we don't have the entire
         $\mathbf{R}$, we only have a finite set, in floating point
         format. ($A$ and $b$ may have error)
       \item Additional error: from the computation of $\tilde{u}$,
         will get something else $\hat{\tilde{u}}\neq \tilde{u}$\\
        \textbf{we can show}: $$\frac{||\hat{\tilde{u}} -
           \tilde{u}||}{||\hat{\tilde{u}}||} \le K(A)\mu$$ That is if we
         solve the problem appropriately (like being careful about
         pivoting etc).
         
    \end{enumerate}
  \item We're really trying to solve $\tilde{A}\tilde{u} =
    \tilde{b}$., where $\tilde{A}\approx A$, $\tilde{u} =
    \bar{u}$. Typically $\approx$ is machine precision, $10^{-16}$

  \item In the end, we want $u(x)$, and would be happy with $u(x_j)$,
    $j=1,\dots, n$. That will get $\tilde{u_j}$
  \item The moral is that when we do this stuff, we're not just doing
    mathematics. 
\end{itemize}

\subsection{Floating Point Arithmetic}
\label{sec:floating}

decimal numbers (base 10).
Consider the example 6522 and 10.31
\begin{itemize}
\item $6522 = 6(10^3) + 5(10^2) + 2(10^1)+ 2(10^0)$
\item $10.31 = 1(10^1) + 0 + 3(10^-1)+ 1(10^-2)$
\item Normalize the numbers! then,,
\item $6522 = 6.522 \times 10^3$ so we can express numbers with one digit to the
  left o fthe decimal point.
\item $10.31 = 1.031 \times 10^1$
\item For any number but 0, it has a fomr $z\times 10^p$, where
  $z\in[1, 10)$.
\end{itemize}

Computers use binary representation. Example: $3_{10} = 1(2^1) + 1(2^0) =
11$ or $1.1000 \times 2^1$.

$23_{10} = 16 + 4 + 2 + 1 = 1(2^4) + 0(2^3) + 1(2^2)+ 1(2^1)= 10111_2$
or $1.0111 \times 2^4$ \emph{normalized}. Here, normalized means
$z\times 2^p$, where $z\in [1, 2)$

\pagebreak
\section{September 8th Class 3}
\label{sec:class3}

\subsection{Floating Point Operations}

Example: addition using d=5 binary digits

Add $3+23$.\\ \noi
Normalized 5-digit binary expressions:

$3 = 2^1 + 2^0 = 1.1000 \times 2^1$

$23 = 16 + 4 + 2 + 1 = 1.0111 \times 2^4$

To add these, shift the smaller number so that the exponents agree.

\begin{align}
1.1000 \times 2^1 & = 0.11000 \times 2^2 \\
& = 0.01100 \times 2^3 \\
& = 0.00110 \times 2^4
\end{align}



May need to round the result: 
3+34
32+2 = 

Shift smaller number
\begin{align*}
3 &==> 0.000110 \times 2^5\\
34 &==> 1.00010 \times 2^5\\
---&-------------\\
37 &==> 1.00101  \\
\end{align*}
\noi
The way the arithmetic is done: take 2 floating point numbers. The computer
hardware will take the correct result (stored temporarily), but will always
round to $d$ digits. In our examples we are using only $d=5$. in the real world we are usually dealing with $d=27$ or more. number will always be rounded to closest representation-- can be either up or down. 

so in this case, the stored result is rounded to $1.0011 \times 2^5$ which is $32+4+2 = 38$. 

Exercise: what two number would have the property whose sum's \textbf{correct}
result is 1.001001? Answer: 32+4+0.5. (Note: $2.5$ in binary is $1.0100 \times 2^1 = 2 + 1/2$). 

In general, given the real $x$, $fl(x) = $ closest floating point
number to $x$. $|x = fl(x)|$ is called the rounding error. For
operations op = $\{+,-,\times,\div\}$, if $x$ and $y$ are floating
point numbers, then $fl(x\text{ op }y) =$ floating point number closest to $x$ $op$ $y$. 

It is possible that both inputs are perfectly good FP numbers, but that the
result of the operation is not able to be represented in the floating point
system (eg. d is too small). For example, in our single precision example,
$d=23$, $m = -126 \le p \le M = 127$. Then, if $x = 1 \times 2^{100}$ and $y = 1 \times
2^{100}$ then $x*y = 1\times2^{200}$ which results in an overflow. 

\textbf{Definition} \emph{Machine Precision} or \emph{Machine Epsilon}
is the smallest floating point number $\mu$ such that $fl(1+\mu) \neq 1$ $(> 1)$

In IEEE arithmetic, for any real $x \neq 0$:

$$
\frac{|x - fl(x)|}{|x|} \le \mu
$$

Example: $d=5$. Find $\mu$. 

\begin{align*}
1 &= 1.0000 \times 2^0\\
\text{Add }z &= 0.00001 \times 2^0\\
--&------------\\
&=1.00001  
\end{align*}
=> which gets rounded to 1.0001. 

$fl(1+z) \neq 1$

Next smallest number to $z$. Normalized, $z = 1.0000\times 2^{-5}$. Next smaller number is $z = 1.1111 \times 2^{-6}$. 
\begin{align*}
1&= 1.00000\\
 +z&=0.0000011111\\
--&---------------\\
&=1.0000011111
\end{align*}
 

which gets rounded to 1.

In general, in $d$-digit binary arithmetic, the machine precision $\mu$ is
$2^{-d}$. 

For IEEE single precision, this is $2^{-23} \approx 1.2 x 10^{-7}$
For IEEE double precision, this is $2^{-53} \approx 1.1 x 10^{-16}$

Does IEEE arithmetic support the existence of numbers like $2^m$ or $2^M$?
\subsection{Relative Error}

We have a number $x$, and a representation of $x \approx \hat{x}$.

The relative error is given by

\[
\frac{||x - \hat{x}||}{||x||}
\]

And the absolute error is given by $||x - \hat{x}||$. 

The relative error is more important than the absolute error. Consider
for example a census, x\% absolute error in the number of ppl in class
compared to the same x\% absolute error in the population of nyc does
not mean the same thing.


\subsection{Forward vs. Backward Error Anlysis}

Generically speaking: we are seeking $y = f(x)$, and get $\hat(y) \neq y$. We want insight into $\frac{||y - \hat{y}||}{||y||}$. 

\textbf{Definition:} \emph{Forward error analysis} tries to keep track of errors as the computation proceeds. 

Example:

Solve $Ax = b$. We get $\hat{x}$ instead. And we are interested in $\frac{||x -
\hat{x}||}{||x||}$. Forward error analysis (egf. for gaussian elimination) is
not possible. (Well, it is possible, but the estimates tend to be wildly
inaccurate). 

\textbf{Definition:}\emph{Backward error analysis} makes the claim that $\hat{x}$ is the solution of a perturbed problem, $\hat{A}\hat{x} = b$ such that (if the solution is done right):

$$
\frac{||x - \hat{x}||}{||x||} \lessapprox \mu (p(n) \mu)
$$

where $p(n)$ is a slowly growing function of $n$ = problem size. 

We use this observation:
$Ax = b$
$\hat{A}\hat{x} = b$

and 

\begin{align}
A(x - \hat{x}) &= Ax - A\hat{x} \\
& = Ax - \hat{A}\hat{x} + \hat{A}\hat{x} - A\hat{x} \\
& = b - b + (\hat{A} - A)\hat{x} \\
& = 0 + E
\end{align}

where $E$ is our error. 

\begin{align}
& \rightarrow x - \hat{x} = A^{-1}E\hat{x} \\
& || x - \hat{x}|| \le ||A^{-1}|| ||E|| ||\hat{x}|| \\
& ||A^{-1}|| ||A||  ||E|| / ||A|| ||\hat{x} ||
\end{align}

$||A^{-1}|| ||A||$ is called $\kappa(A)$, and $||E||/||A|| \lessapprox \mu$

this $\rightarrow ||x - \hat{x}|| / ||\hat{x}|| \lessapprox \kappa(A) \mu $

$\mu \approx 10^{-16}$

$\kappa(A)$ depends on $A$, the statement of the problem.. 

We claim, that with a bit more work, we could put $||x||$ in the denominator. 
\pagebreak
\section{September 13rd Class 4}
\label{sec:class4}
\subsection{Intro to Probability}
\label{sec:probability}

\textbf{Discrete models}: Consider a game with a finite or countable
number of outcomes, $\Omega$, the \emph{sample space}.
\textbf{Definition:} \emph{Probability} for each $\omega_j \in
\Omega$, we have a number $p_j(\omega_j) = p_j$ s.t. $p_j \in [0, 1]$,
and $\sum_{j=1}^{\infty} p_j = 1$

Examples:
\begin{enumerate}
\item Roll a fair die outcomes: $\Omega=\{1,2,3,4,5,6\}$, $p_j = 1/6$, $j=1,2,\dots,6$
\item Roll two fair dice, outcomes: $\Omega=\{(1,1), (1,2), \cdots, (1,6),
  (2,1), \cdots, (6,6)\}$, $p_j = 1/36$, $j=1,\dots,36$.
\item put $n$ distinct balls into $N$ urns $N>n$. 1st ball has $N$ choices, so does the 2nd ball, etc, so
  the total number of outcomes is $N^n$.
\item A die is rolled until a six appears. possible outcomes:
  \begin{enumerate}
  \item Get a 6 on 1st trial: 6, $p(\omega_1)=1/6$
  \item something other than 6 on 1st trial: 16,26,36,46,56 (5 ways),
    $p(\omega_2) = 5/6(1/6) = 5/36$
  \item etc $p(\omega_i) = (1-p(\omega_1))^{i-1})p(\omega_1) = (5/6)^{i-1}(1/6)$
  \end{enumerate}
Here $\Omega$ is countably infinite. The sum:
\begin{align*}
\sum_{j=1}^\infty p_j &= 1/6 + (5/6)(1/6) + (5/6)^2(1/6) + \cdots\\
&=1/6\sum_{l=0}^\infty (5/6)^l\\
 &= 1/6(\frac{1}{1-5/6}) = 1/6(\frac{1}{1/6}) = 1
\end{align*} 

 \textbf{Definition:} An \emph{event} is a subset $A \subseteq
 \Omega$. Notation $P(A) = \sum_{\omega_J \in A}
 p(\omega_j)$. Properties: $\P(\Omega) =1, P(\empty)=0,
 P(A^c)=1-P(A)$.
If $A_1, A_2, \dots$ are pairwise disjoint, then $P(\cup_i A_i) =
\sum_i P(A_i)$.

\textbf{Definition:} A \emph{random variable} $X$ is a function
$X:\Omega \rightarrow \mathbf{R}$. $\forall E\in \mathbf{R}$, define
an \emph{event} $\{X\in E\} = \{\omega \in \Omega | X(\omega) \in E\}$
Can talk about probability of such an event as $P(X\in E)$

\textbf{Definition:} A \emph{distribution} of a discrete random
variable $X$ is a collection of distinct real numbers, or a set of
values $\{m_j\}\in [0,1]$, s.t. $\sum m_j = 1$, and $P(X=x_j) = m_j$.

Example: \emph{Binomial distribution}. Consider a random experiment
with two possible outcomes. Psobability of success = $p$, failure
$q=1-p$.
With coins, $p=1/2$. Perform this experiment $n$ times, $X=$\# of
successes. $X$ has a $Binom(p,n)$. The probability of exactly $l$
sucesses is $P(X=l) = {n \choose k} p^kq^{n-k}$
\pagebreak
\section{September 15th Class 5}
\label{sec:class5}

\subsection{Probability Cont.}
\label{sec:probability}
Experiment: three tosses of a coin $|\Omega|=2^3$

\textbf{Definitino: } The distribution of discrete random variables is
a collection of real positives $\{x_j\}^{\infty}_{j=1}$, s.t.  $P(X=x_j) = m_j$


\textbf{Definition: } A \emph{mass function} of a discrete random
variable $X$ is $m:\mathbf{R}\rightarrow [0, 1]$ s.t. $m(x)=P(X=x)\forall x
\in \mathbf{R}$.

\textbf{Definition:} The binomial distriubtion is a random experiment with two outcomes where
$p$ is the probability of success, $1-p$ is the probability of
failure. It performs the experiment $n$ times, and $X$ denotes the number of total
successes.

Q: what is the probability of exactly $k$ successes? ${n \choose k}
p^k(1-p)^{n-k}$, let's check $\sum_{j=1}^\infty m_j =1$. This is
$\sum_{k=0}^np^k(1-p)^{n-k} = (p + (1-p))^n = 1^n = 1$ so yes.

\textbf{Definition:} Continuous random variables and
distributions. ex: Average weight of 100 randomly selected
, where $\Omega$=$\{$100 tuples of weights $\}$, $X(\omega) =1/100$
(sum entries of $\omega$). Random number uniformly chosen from $[a,b]$.

\textbf{Definition:} A function $f$ on $\mathbf{R}$ is called a
\emph{density function} if it's non-negative and integrable and
$\int_{-\infty}^\infty f(x)dx = 1$.

A random variable with a density function $f$
means $$P(\{ \omega \in \Omega | X(\omega) \in A \}) = \int_Af(x)dx$$
for $A\subseteq \mathbf{R}$.

example: we say $X$ is uniformly distributed on $[a,b]$ if it has
density function 
$$f(x)=
\begin{cases}
  \frac{1}{b-a} & \text{ for } x\in [a,b]\\
  0 & \text{ otherwise }\\
\end{cases}
$$

Alternatively, $X$ is normally distributed with mean $\mu$ and
variance $\sigma^2$ if 
$$f(x) = \frac{1}{\sqrt{2\pi \sig^2}}e^{-\frac{(x-\mu)^2}{2\sig^2}}$$
Exercise: check $\int_{-\infty}^{\infty} f(x)dx=1$

\textbf{Defintion:} \emph{mean}, $\mathbf{E}(x)=\mu$,  of a random variable. Discrete case:
$\mu = \sum_{x\in \mathbf{R}}x\times m(x)$, continuous case:
$\mu = \int_{-\infty}^{\infty}x\times f(x)dx$
$$
\begin{cases}
  \sum_{x\in \mathbf{R}} x\times m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty x\times f(x) & \text{ Continuous } 
\end{cases}
$$

\textbf{Definition:} \emph{Distribution of a continuous r.v.}: $F(x) =
P(X(\omega) \le x) = \int_{-\infty}^xf(\xi)d\xi$. So if $F(x)$ is
normal with $\mathcal{N}(\mu, \sig)$, $\mu$ is at the tip of the
gaussian mountain, and $\mu$ has val $0.5$ in the CDF, $F(x)$.


Let $\Phi: \mathbf{R} \rarr \mathbf{R}$. \emph{Claim:} 
$$E(\Phi(x)) =
\begin{cases}
  \sum_{x\in \mathbf{R}} \Phi(x)m(x) & \text{ Discrete }\\
  \int_{-\infty}^\infty \Phi(x)f(x)dx & \text{ Continuous }  
 
\end{cases}
$$
This is not obvious.

Now how to define R.V, $X$, formally. $\Omega \rarr \textbf{R}$. Using
the example of 3 coin tosses, $\Omega =
\{000,001,\dots,111\}$. $X(\omega) =$ decimal version of binary value
of $\omega$. Then, $X(\omega)= [1, 7] \in \textbf{N}$. Now, we can do:
$\forall x\in \textbf{R}$, define $m(x)$ by $$
m(x)=
\begin{cases}
1/8 & \text{ if } 1\ge x\le 7 \\
0 & \text{ otherwise}
\end{cases}
$$

Let us call 1 or more consecu, tive tosses of same type as \emph{run}s. This is a
new R.V. where $\Omega$ is the same, and $Y(\omega) = $number of
runs.

i.e. $Y(000) = 1 = \Phi(0), Y(001) =2=\Phi(1), Y(010)=3=\Phi(2),
Y(011)=2, \dots$. Note $Y=\Phi(X)$. So $m_y(y) = P(Y=y)$, and $m_y(1)
= 1/4, m_y(2)=1/2, m_y(3)=1/4, m_y(\text{else}) =0$. Then,
\begin{align*}
\mathbf{E}(Y) &= \sum_{y\in R}ym_y(y)\\
& = 1/4 + 2(1/2) + 3(1/4) = 2  
\end{align*}

This is the same as 
\begin{align*}
\sum_{x\in R}\Phi(x)m(x) &= (1+2+3+2+3+2+1)(1/8)\\
&=16/8=2
\end{align*}
\end{enumerate}
\pagebreak
\section{September 20th Class 5}
\label{sec:class5}

\subsection{Continue on Probability}
\label{sec:prob3}

Given random variable $X$ and $E(x)=\mu = \int_{-\infty}^{\infty}xf(x)dx$,
the variance is $var(x)=\sigma=E[(X-\mu)^2] = E(X^2)-E(X)^2$
If $x$ has a desity function $f(x)$, then
$E(\phi(x))=\int_{-\infty}^\infty \phi(x)f(x)dx$

\subsection{Monte Carlo Integeration}
\label{sec:montecarlo}
Let $X$ be a uniformly distributed random variable on $[0,1]$. If
$\phi:[0,1]\rarr \mathbf{R}$, suppose we want an approximation to
$\int_0^1\phi(x)dx = I(\phi)$.

Sample $X$ from $[0,1]$, and get $x_1, x_2, \dots, x_n$. We'll
approximate $I(\phi)$ by the average
$\frac{1}{N}\sum_{i=1}^N\phi(x_i)$. This is the quadrature rule
$Q_{MC, N}(\phi)$.

Notice: $\phi(x)$ is a random variable. The expected value of
$\phi(x)$ is $E(\phi(x)) = \int_{0}^1 \phi(x)dx = I(\phi)$, because $X$
is uniform. (so $f(x) =
\begin{cases}
1 & \text{ if }  x\in [0,1]\\
0 & \text{ otherwise}
\end{cases}
$)

The approximation $Q_{MC,N}(\phi) = Q_N(\phi)$ is called the sample
mean of $\phi(x)$. Approximates the mean of $\phi$, which is $I(\phi)$
or $\mu(\phi)$. i.e. a trivial example if $\phi(x) = x$, then $I(\phi)
= \int_0^1xdx = \frac{1}{2}x^2|_0^1 = 1/2$

Contrast this with standard ways to do quadrature.
\emph{examples:}
\begin{itemize}
\item  the trapezoidal rule: area under the curve is approximated by
  the trapezoid bewteen $f(a)$ and $f(b)$. $\int_a^b\phi(x)dx \approx 1/2(b-a)(\phi(a)+\phi(b))$
\item the simpson's rule: $\int_a^b\phi(x)dx\approx 1/6(\phi(a) +
  4\phi(\frac{a+b}{2}) + \phi(b))(b-a)$
\item Composite trapezoidal rule on [0, 1]: $\approx h/2\phi(x_0) +h\sum_i^{N-1}f(x_i) + h/2\phi(x_N)$
\end{itemize}
\subsubsection{Monte Carlo history}
\label{sec:mchistory}

From high energy physics by  John von Newmann and Stanislaw
Ulam. Monte Carlo, an island in the meditteranian where people gamble.

\subsection{Error Analysis}
\label{sec:erroranalysis}
Simpson's rule is better. The error analysis of sympson's rule
is: $I(\phi) - Q_s(\phi) \le ch^4 = c1/N^4 = O(1/N^4)$

With trapezoid it's $O(1/N^2)$. Is this better than monte carlo? \\In
$1D$, monte carlo does worse.


\emph{Claim:} error $|I(\phi) - Q_{MN, N}(\phi)| "\le"
O(1/\sqrt{N})$. Suppose $N=10000$, so $1/100$, but in simpson's rule's
error, $O(1/10^{16})$. Simpson's does way better.

 From probability analysis, using the Law of Large Numbers, if we let
 $S_N = \phi(x_1) + \cdots + \phi(x_N)$, and the sample mean $S_N/N=\mu_N$,
 (which is our approximated integral), by LNN,
 $\lim_{n\rarr\infty}P(|\frac{S_N}{N} - I(\phi)| < \eps) = 1$, where
 $I(\phi)$ is the real $\mu$. That is $P(|\frac{S_N}{N} -
 I(\phi)| > \eps) \rarr 0$ as $N\rarr \infty$. In words, it's ``very
 unlikely'' that $Q_{MC,N}(\phi)$ is different from $I(\phi)$ when $N$
 is very large.

\textbf{Chebyshev's inequality}: Given $c>o$, $$P(|\frac{S_N}{N} -
\mu|\ge c) \le \frac{\sig^2}{Nc^2}$$ Commentary: suppose we want this
probability to correspond to 95\% confidence, that is the probability on the left
to be $.05$, then  we require $.05 \le \frac{\sig^2}{Nc^2}$, solve for
$c$ and we get $c^2\le \frac{\sig^2}{.05N}$ or $c\le
\frac{\sig}{\sqrt{.05N}}$. So with $95\%$ confidence, $error\le
\frac{\sig}{\sqrt{.05}}\frac{1}{\sqrt{N}}$
That's what we can say about the error. If we want $99\%$ confidence,
then $\sqrt{.05}$ becomes $\sqrt{.01}$

Two cons: convergence is quite a bit slower, we're not as confident as
the other quadrature methods (the others give us a guarantee)

Suppose instead of uniform, say we had two box, $N$ samples of
2-tuples, then with double integral and think about monte carlo. The
procedure is the same, sample, evaluate the value at that sample, then
take the average. Suppose we have a cube, same thing. Nothing of monte carlo is
tied to the underlying domain. The analysis using LLN and chebyshev is
always the same.

Consider 2D. Simpsons rule on $[a,b]\times[c,d]$. 
\begin{align*}
\int_0^\phi\int_a^b\phi(x,y)dxdy &\approx \text{let } \Phi(y) = \frac{b-a}{6}(\phi(a,y)+4\phi(\frac{a+b}{2}, y) + \phi(b,y))\\
&\approx \frac{d-c}{6}(\Phi(c) + 4\Phi(\frac{c+d}{2}) + \Phi(d))
\end{align*}, where $\Phi(\frac{c+d}{2}) = \phi(\frac{a+b}{2}, \frac{c+d}{2})$
Total number of points $N=n^2$, $h=1/n = 1/\sqrt{N}$. The error is
$O(h^4)= O(1/n^4) = O(1/N^2)$

In 3D, the error on simpsons is still $O(h^4)$, but $h=1/n, N=n^3$, so
this is $O(1/N^{4/3})$, in $D$ dimensions, it's $O(1/N^{4/d})$)

In monte carlo, the error analysis is the same in any dimension. Has
nothing to do with the integral. so the big pro is that we can never
be certain but we're always working with the same error around
$\frac{\sig}{\sqrt{a}}\frac{1}{\sqrt{N}}$.

Now, which technique is better? If we ignore the uncertainty, we're
asking when is $\frac{1}{N^{4/d}} < \frac{1}{N^{1/2}}$?

\begin{align*}
\frac{1}{N^{4/d}} &< \frac{1}{N^{1/2}}?\\
N^{1/2} &< N^{4/d}\\
\frac{1}{2} &<\frac{4}{d}\\
d &< \frac{4}{1/2} = 8
\end{align*}
So when $d<8$, simpson is better, else, MC is better. (Although we're
ignoring the constant and the uncertainty factor so it won't be exact
like this but still around there)

In model of particle physics, the number of dimensions is basically is
equal to the number of particles, which could be hundreds. It's not
unusual to have large $d$.

\emph{Example:} of high dimensional integral (in the text/wiki). In
particle physics, a \emph{partition function} $Z(\beta)$ describes the
statistical properties a system of $d$ particles, thermodynamic
equilibrium, \\$Z(\beta) = \int exp(-\beta H(p_1, \cdots, p_d, x_1,
\cdots, x_d)d^3p_1\cdots d^3p_dd^3x_1\cdots d^3x_d$, \\where this
integral is a $3d$ integral. where $d=$ number of particles, and
$\beta = \frac{1}{\alpha\tau}$, a Boltzmann constant, $\tau$ is
temperature, and $H$ is hte hamiltonian function. And the point is
that people care about this. This is about a 3 million dimensional
integral.

\pagebreak
\section{September 22 Class 6}
\label{sec:class6}

\subsection{Continue on Monte Carlo}
\label{sec:montecarlo2}
Monte Carlo Integration: Given a r.v. $Y$, $P(Y\subseteq
A)=E(I_A(Y))$, where $I_A(Y) =
\begin{cases}
 1 & \text{ if } Y\in A\\
 0 & \text{ otherwise} 
\end{cases}
$. 
And $E(I_A(Y)) = \int_AI_a(y)dy$, where the integral can be
multi-dimensional. One way to deal with high-dimensional integral is
to use Monte Carlo Integration.\\

\noi
\emph{Review}: Monte Carlo gives us the approximation $\int_{\mathbf{R^D}}\phi(x)dx =
I(\phi)$, where $I(\phi) \approx 1/N\sum_{n=1}^NQ(x_n) = Q_{MC,N}(\phi)$ where $\{x_n\}$
are samples of randome variable $X$.
\\
We showed that the error was $|I(\phi) - Q_{MC,N}(\phi)|"\le"
\frac{\sig}{c\sqrt{N}}$

\noi
If the goal is to make this bound $\le \tau$ tolerance,
i.e. $\frac{\sig}{c\sqrt{N}} \le \tau$, then we need 
\begin{align*}
\sqrt{N} &\le \frac{\sig}{c\tau}\\
N &\le  \frac{\sig^2}{c^2\tau^2}  
\end{align*}
many samples.
\subsection{Variance Reduction Methods}
\label{sec:variancereduction}
The aim is to reduce $\sig$
2 examples of idea to do this

\subsubsection{Antithetical variables}
Assume $\phi \in [0,1]$, and $I(\phi)
  =\int_0^1\phi(x)dx = E(\phi(x))$ (remember !!) where $X$ is a r.v. from a uniform
  distribution on $[0,1]$. ($E(x) = \int_0^1xdx = 1/2x^2|_0^1 = 1/2$,
  and we approximate $I(\phi)$ with
  $1/N\sum_n=1^NQ(x_n)$).\\ Antithetical variables are $x$, and $x^c$
  s.t. $x+x^c = 1$ or  $x^c= 1-x$, $x = \mu + d$, $x^c = \mu - d$

Assume $\sig$ small. Write $d = \sig \hat d$ ($d$ will not grow too
much because they're from a bounded distribution..) Let $D = X - \mu$,
another r.v., and let $\hat D = D/\sig$ also a r.v. Then,
\begin{align*}
E(\hat D) &= \frac{1}{\sig}E(D)\\
&= \frac{1}{\sig}E(x-\mu)  \\
&=\frac{1}{\sig}[E(x) - E(\mu)] \\
&=\frac{1}{\sig}[\mu-\mu] = 0
\end{align*}
Now, consider $\phi(x)$. (he used 1/2 as $\mu$ following the example on board)
\begin{align*}
\phi(x) &= \phi(\mu + d) = \phi(\mu + \sig\hat d)\\
&= \phi(\mu) +   \phi'(\mu)\sig \hat d + \phi''(\xi)\sig^2 \hat d^2
\text{ by taylor series}\\
&\approx \phi(\mu) +   \phi'(\mu)\sig \hat d + O(\sig^2)\\
\end{align*}
And $\phi(x^c)$
\begin{align*}
  \phi(x^c) &= \phi(\mu - d) = \phi(\mu - \sig \hat) \\
&\approx \phi(\mu) - \phi'(\mu)\sig\hat d +  O(\sig^2)
\end{align*}
SO, 
$$\frac{\phi(x)+\phi(x^c)}{2} = \phi(\mu) + O(\sig^2)$$
Now look at $I(\phi) = E(\phi(x)) = \int_0^1\phi(x)dx$. This is
\begin{align*}
 &=E[\phi(\mu) + \phi'(\mu)\sig \hat D + O(\sig^2)]\\
&=\phi(\mu) + \phi'(\mu)\sig E[\hat D] + O(\sig^2)\\
&=\phi(\mu) + 0 + O(\sig^2)
\end{align*}

Now a new integration method. Smaple $X$ $N$ times and get $x_1,
\dots, x_N$, also let $x^c_1,\dots, x^c_N$. Let the complementary
quantities:
$$
Q^c_{MC,N}(\phi) =  \frac{1}{2N}(\phi(x_1) + \phi(x_1^c) + \cdots +
\phi(x_n) + \phi(x_n^c)
$$

For each $n$,
\begin{align*}
 \phi(x_n) &= \phi(\mu + \sig \hat d_n) = \phi(\mu) + \phi'(\mu)\sig
 \hat d_n + O(\sig^2)\\
 \phi(x_n^c) &= \phi(\mu - \sig \hat d_n) = \phi(\mu) - \phi'(\mu)\sig
 \hat d_n + O(\sig^2)\\
1/2(\phi(x_n) + \phi(x_n^c)) &= \phi(\mu) + O(\sig^2)\\
\Rightarrow Q^c_{MC,N}(\phi) &= \frac{1}{N}(N\phi(\mu) +
N(\phi(\sig^2)) = \phi(\mu) + O(\sig^2)\\
\end{align*}
Now the difference: $I(\phi) - Q_{M,N}^c(\phi) = \phi(\mu) - \phi(\mu)
+ O(\sig^2)$
So \textbf{the error is 1/4th the size} or $\sig^2$ instead of
$\sig$.
Smmmary:
\begin{enumerate}
\item[Method 1] Take $N$ samples, error ~ $\frac{\sig}{\sqrt{N}}$
\item[Method 1] Take $2N$ samples, error ~ $\frac{\sig}{\sqrt{2N}}$
\item[Method 2] Take $N+N_{\text{antithetical}}$ samples, error ~ $\frac{\sig^2}{\sqrt{N}}$
\end{enumerate}

\subsubsection{Importance Sampling}
\label{sec:importancesampling}
$I(\phi) = \int_0^1\phi(x)dx$, written in a different way
is $$\int_0^1 \frac{\phi(x)}{p(x)}p(x)dx$$
Where $p$ is a positive function on $[0,1]$ s.t. $\int_0^1 p(y)dy=1$
. Notice $p(x)$ is a density function.

Suppose we can randomly sample variable $x$ from the distribution defined by
$p$. This means $P(X\in A) = \int_A p(x)dx$.

\noi \textbf{Rule:}
\begin{enumerate}
\item Sample $x_1, x_2, \dots, x_n$
\item Form the MC cost estimate, $I_N(\phi) = \frac{1}{N}\sum_{n=1}^N\frac{\phi(x_n)}{p(x_n)}$
\end{enumerate}

The variance of this process is $$\hat \sig^2 = \int_0^1
(\frac{\phi(x)}{p(x)} - I(f))^2p(x)dx$$
Error is proportional to $\frac{\hat\sig}{\sqrt{N}}$
We want $p$ s.t. $\hat \sig < \sig_{\phi}$
$I(f)$ is just a number. We would like $p$ close to
$\frac{\phi}{I(\phi)}$. But we don't know what $I(\phi)$ is.

\pagebreak{}
\section{September 27th Class 7}
\label{sec:class7}

\subsection{Continue on Importance Sampling}
\label{sec:importancesampling}

For variance reduction, we're computing $\int_0^1\phi(x) dx$, MC says
that this is approximated by $\frac{1}{N}\sum_i^N\phi(x_i)$. We'll write the
integral as $I(\phi) = \int_0^1\frac{\phi(x)}{p(x)}p(x)dx$, where $p$ is
positive and $\int_0^1p(x) = 1$, i.e. $p(x)$ is a density function.\\

We know that the error for MC is
$~\frac{\sig_{\phi}^2}{\sqrt{N}}$. Supposition is that we can sample
$\{x_i\}$ from a distributionwith density $p(x)$, and sampling will
approximate $I(\phi)$ by
$\frac{1}{N}\sum_i^N\frac{phi(x_i)}{p(x_i)}$. The variance of this
process: $\hat \sig^2 = \int_0^1(\frac{\phi(x)}{p(x)} -
I(\phi))^2p(x)dx$, and the error $~ \frac{\hat \sig^2}{\sqrt{N}}$, and
$\hat \sig < \sig_p$. We need $p$ and be able to sample from it to do
this.


We need a way to define $p(x)$ and a technique to sample from it. 

\subsubsection{Technique: Accept/Reject sampling method}
Supposed we have $q(x)\ge p(x)$ ($q$ could be a constant too).
Let $\hat q(x)= \frac{q(x)}{\int_0^1q(x)dx} = \frac{q(x)}{I(q)}$, and
so $I(\hat q) = 1$ (scaled version of q). (in the hw $\hat q$ is just
a constant)

\emph{Algorithm} to draw random numbers with density $p(x)$. 
\begin{enumerate}
\item Pick two random numbers $x'$ with density $\hat q$ and $y$ from
  uniform distribution [0,1].
\item Then make a decision: Accept $x'$ as one of my samples if $y\le \frac{p(x')}{q(x')}$,
  reject otherwise.
\item \emph{claim} the number of accepted samples is approximately $\frac{1}{I(q)}$
\end{enumerate}

Why does this work? \\Consider a function $C(\xi) = C_\ah(\xi) =
\mathbf{X}(\xi \le \ah)
\begin{cases}
  1&\text{ is }\xi \le \ah \\
0 & \text{ otherwise}
\end{cases}$. For $\ah\in [0,1]$, $\int_0^1C_\ah(\xi)d\xi =$ area under line from 0
to $\ah$, which is $\ah$. Now..use
\begin{equation}
  \label{eq:Cxi}
C(\xi) =
C_{\frac{p(x)}{q(x)}}(\xi)
\end{equation}
where $x$ is given.

We want to know if 
\begin{equation}
  \label{eq:wewant}
\mathbf{E}(X) \approx \int_0^1xp(x)dx =
\frac{1}{N}\sum_j^Nx_j
\end{equation}
 (we can take out $p(x)$ because of the way we
sampled $x_j$). Notice $p(x) = \frac{p(x)}{q(x)}q(x) =
\frac{p(x)}{q(x)}\hat q(x)I(q)$, now using \ref{eq:Cxi}, this is the
same as $\int_0^1C_{\frac{p(x)}{q(x)}}(\xi)d\xi \hat q(x)I(q)$.
Then,
\begin{align*}
\int_0^1xp(x)dx &=
\int_0^1xdx \int_0^1C_{\frac{p(x)}{q(x)}}(\xi)d\xi \hat q(x) I(q)\\
&=\int_0^1\int_0^1x C_{\frac{p(x)}{q(x)}}(\xi)\hat q(x)d\xi dx I(q)\\
&\approx \frac{1}{N'}\sum x_i C_{\frac{p(x)}{q(x)}}(\xi_i)I(q)\text{
  this step was doing MC to estimate}\\
\end{align*}
Using $N'$ samples from the distribution for $x$ taken from $\hat q$,
and $\xi$, taken from uniform dist. $C(\xi_i)=1$ if $\xi_i \le
\frac{p(x)}{q(x)}$, 0 otherwise. So the entire thing is equal to 
$$
\frac{1}{N}\sum_{\text{indicies of accepted samples}}x_i
$$
Using the claim that $\frac{N=\text{\# accepted}}{N'=\text{\# sampled}}
= \frac{1}{I(q)}$
($\frac{I(q)}{N'} = \frac{1}{N}$) So this is what we wanted \ref{eq:wewant}.

\textbf{Proof of the claim}: Rule is accept if $y\le
\frac{p(x')}{q(x')}$, which is $yq(x')\le p(x')$.
For a given $y$, $P(\text{resulting experiment leads to accept }x')$ is deteremined by integrating
$y\int_0^1q(x')dx' \le \int_0^1p(x')dx'$. So
$$y \le \frac{I(p)}{I(q)} = \frac{1}{I(q)}$$ because $p$ is a density
function.

Then $P(\text{experiment leads to acceptance}) = P(\text{sample $y$
  from uniform }\le \frac{1}{I(q)}) = \frac{1}{I(q)}$ (since
  $\int_0^{\frac{1}{I(q)}}dy = \frac{1}{I(q)}$

This probability is approximately just $\frac{N}{N'} \approx
\frac{1}{I(q)}$, which is our claim! (Reference R. Caflisch Acta
Numerica 1:49, 1998 Monte Carlo and quasi-Monte Carlo methods)

\subsection{Matrix Factorization Methods}
\label{sec:matrixfactorization}
Four basic algorithms:
\begin{enumerate}
\item Gaussian Elimination (solving $Ax=b$ where $A$ is nonsingular)
\item QR Factorization (same, or solving least squares min$||Ax-b||$
  where $A$ is a long thin $m$by$n$ matrix, $m>>n$)
\item Singular value decomposition (also for least squares)
\item Eigenvalue decomposition (SVD and this are used to solve eigen
  value problems, $Av= \lambda v$, find $v, \lambda$
\end{enumerate}

\pagebreak
\section{October 4th Class 8}
\label{sec:class8}

Corrections for hw 2:
\begin{itemize}
\item Problem 1(a): $f(X) \in [0,1]$, not $[-1,1]$
\item Problem 3: should read $\rho(x) = \frac{4}{10}\hat \rho(x)$
  because we need $\int_0^1\rho(x)dx = 1$. Also $q(x)=1.6$
  constant. When sampling, $q(x)$ induces a density function $\hat
  q(x)$ by $\hat q = \frac{q}{\int q(x)dx}=\frac{1.6}{1.6}$, which means that we
  still sample from $U(0,1)$, but when you compare the ratio $p/q$, we
  use 1.6.
\item For histogram \& density estimation: If we have a pdf $p(x)$,
  the $\int_{D~X}p(x)dx = 1$. For r.v. $X$ with density $p$, then
  $P(\ah \le X\le \beta) = \int_\ah^\beta p(x)dx$. 

If we're sampling $\{x_i\}$ from such an $X$(using reject/accept), then the number of $\text{num of
}x_i \in [\ah,\beta]/\text{num of samples} \approx \int_\ah^\beta
p(x)dx$. This is called the kernel density estimate. The goal here is
to approximate $p(x)$ by some constant $p_c(x)$ on $[\ah, \beta]$. If
we had $p_c$, then $\int_\ah^\beta p(x) \approx p_c[\beta-\ah]$. Turn
this around to define $p_c$ as 
$$p_c = \frac{\text{\# of }x_i\in[\ah, \beta]}{\text{\# of
    samples}}\frac{1}{\beta-\ah}$$
Given $x$, let $[\ah_j, \beta_j]$ be the interval containing $x$,
define $p_c(x)=$ number of $x_i\in[\ah_j,\beta_j]$/number of
samples$\frac{1}{\beta_j-\ah_j}$, and this is the histogram he's looking for.
Does it come out the way it should?

In matlab use $hist$, and if you really want to compare/plot $p_c$,
specify bin set \texttt{hist(X, [1/8,3/8/,5/8,7/8])}, it will give you
exactly what you want (same centers). Better yet, let that hist be $z$
and do a bar graph \texttt{bar(z, centers)}.
\item For prob 1.b, do $log(\text{error})$vs$log(N)$ scale. Second
  graph is to ask whether things really look clean (no, not really bc
  of the probabilistic nature). Because if we were
  to use trapezoid, we'll get a very clean ratio between the error.
\item For prob 1.c, 3 sets, each of 4 pictures.. sample $E_N$ $n$
  times, try different sets of $n$s. (I did 1000, use that for
  $\frac{\sig}{n}$..?, but try different samples).
\end{itemize}

\subsection{Matrix Factorization}
\label{sec:matrix}

\subsubsection{Gaussian Elimination}
\label{sec:gaussian}
Solve $Ax=b$, where $A$ $NbyM$ is non-singular. Procedure also called $LU$
factorization.
\begin{enumerate}
\item Add multiple of first row of $A$ and first entry of $b$ to the
  second to the last rows to produce zeroes on the first column of $A$
  except the first entry. We get $\hat A = L_1A$
\item repeat this, then $L_{n-1}\cdots L_1=U$ the upper triangle.
\end{enumerate}

\emph{Cost}: number of floating point multiplication and divisions:
\begin{itemize}
\item $n-1$ for column of $L_1$.
\item $(n-1)(n-1)$  to produce $\hat A$
\item Just the operation on $A$ is $n(n-1)$ for step 1.
\item plus $(n-1)(n-2)$ for step 2
\item $\cdots$ and we get $(n-(n-2))(n-(n-1)) = 2$ at step $n-1$
\end{itemize}
So total is 
\begin{align*}
  \sum_{j=1}^{n-1}j(j+1)&= \sum_{j=1}^{n-1}j^2 + \sum_{j=1}^{n-1}j\\
&=\sum_{j=1}^{n-1}j^2 + \frac{n(n-1)}{2}\\
&= O(\frac{1}{3}n^3) + n^2 = O(n^3)
\end{align*}
Exercise: Identify how to get $x$, \emph{back substitution} and what's
the cost? ($O(n^2)$)

What happens if $a_{00} == 0$?
Since $A$ nonsingular, we know that there must be at least one
$a_{j1}\neq 0$. Take the $j$ with largest $|a_{j1}|$ and switch it
with $a_{11}$. Formally, the switch is done by multiplying the system
by $P_1$, a matrix with all diagonal values $1$ but $a_{11}$ and
$a_{jj}$ and all 0 but $a_{0j}$ and $a_{j1}$., the permutation matrix.

So result is $L_1P_1Ax = L_1P_1b$, where $L_1$ is taken from
$P_1A$. This is called \emph{pivotting}.
\pagebreak

\section{October 6th Class 9}
\label{sec:class9}

\subsection{More Linear Algebra}
\label{sec:linearAlgebra}
\subsubsection{Cont on Gaussian Elimination}
Solving $Ax=b$, $A$ nonsingular. Without pivoting, $L_{n-1}\cdots L_1A
= U$, where $L_{n-1}\cdots L_1 = L^{-1}$ Claim: $L^{-1}$ and $L$ are
lower triangulare
\begin{itemize}
\item product of lower triangular matrices is a lower triangular. 
\item Inverse of lower triangular matrices is lower triangular.
\end{itemize}
So we have $$A = LU$$
Formally, solving $Ax=b$ consists of forming $U=L^{-1}A$ (step 1a) and $\hat b = L^{-1}b$
(step 1b) and solving $Ux=\hat b$ for $x$ (step 2).

We showed last time that (1a) costs $O(n^3/3)$. Claim: cost of (1b) is
$O(n^2)$ and cost of (2) is $O(n^2)$.

Step 1 is $x_n=b_n/U_{n,n}$
Idea of solving $Ux=\hat b$ to do row $j$
\begin{verbatim}
for j=n:-1:1
   x_j = (\hat b_j - \sum_{l=j+1}^n U_{j,l}x_l)U_{j,j}
end
\end{verbatim}
For each step j, cost is:  $n-j + 1$ plus 1 for divide
So the total cost is $\sum_{j=1}^n(n-j+1)$, $n+1 - 1+n+1, -2+\cdots +
n+1-n = n+n-1 \cdots +1 $ so total $\frac{n(n+1)}{2}$

One thing we missed is what's the entries of matrix $L$ look like. We
know $U$, it's what's been left behind. \emph{Claim}: the entries of
$L$ are - cost of $L_i$'s that are constructed. i.e. first column is
$\begin{smallmatrix}
1\\ a_{21}/a_{11}\\a_{31}/a_{11}\\ \vdots \\ a_{nn}/a_{11}
\end{smallmatrix}$

To do this correctly, we may need to \emph{pivot}. When we pivot in
floating point, we compute a solution $\hat x$ that satisfies
$(A+\delta A)\hat x=b$, where $\frac{||\delta A||}{||A||}$ is
small. What we looked at in week 1, $\frac{||x-\hat x||}{||x||} =
||A||||A^{-1}||\frac{||\delta A||}{||A||} = \kappa(A)$

Now, look at the relative error $$\frac{||x-\hat
  x||}{||x||}=\frac{||x-\hat x||}{||\hat x||}\frac{||\hat
  x||}{||x||}$$
\begin{align*}
Ax&=b \\
(A+\delta A)\hat x &= b\\
A\hat x&= b-(\delta A)\hat x\\
\hat x &= A^{-1}b - A^{-1}(\delta A)\hat x\\
&= x -  A^{-1}(\delta A)\hat x\\
||\hat x|| &\le ||x|| + ||A^{-1}(\delta A)\hat x|| \text{ take the
  norm}\\ 
&\le  ||x|| + ||A^{-1}|| ||(\delta A)||||\hat x|| \\
||\hat x|| - ||A^{-1}|| ||(\delta A)\hat x|| &\le ||x ||
1-\kappa(A)\frac{||\delta A||}{||A||}||\hat x|| &\le ||x||
\end{align*}
$\kappa(A)$ is a property of our matrix A. In numerical computing, we
can only work with A with reasonable conditioning number.\\
Put things together we have
$$\frac{||x-\hat
  x||}{||x||} \le \frac{ \kappa(A)\frac{||\delta A||}{||A||}}{1-\kappa(A)\frac{||\delta A||}{||A||}}$$
The way to think bout hte denominator:$\frac{||\delta A||}{||A||}$
small means aroung $10^{-12}$~$10^{-14}$~$10^{-16}$. Suppose
$\kappa(A)=500$, then the denominator is
$1-500(10^{-12})=1-5(10^{-10})$ not that bad at all. In fact, we can
deal up till like $5000$ can handle it until $10^{10}$ (for this
proble)

\subsection{Least Squares Problem}
\label{sec:leastsquares}

Given data $\{(x_i,y_i)\}$, find a simple function $p(x)$
s.t. $p(x_i)\approx y_i$. Example of such a function is
$p(x)=\ah_0+\ah_1x$ a linear polynomial. 

\noi
A least squares fit is to find $\ah_0$ and $\ah_1$ s.t. sum of squares
of $y_i-(\ah_0+\ah_1x_i)$ is minimized. $min
\sum_{i=1}^n[y_i-(\ah_0+\ah_1x_i)]^2$
Note: if we tried to make $p(x_i)=y_i$ for all $i$, we have
\begin{align*}
\ah_0+\ah_1x_1 &= y_1\\  
\ah_0+\ah_1x_2 &= y_2\\ 
&\vdots\\
 \ah_0+\ah_1x_m &= y_m\\  
\end{align*}
$m$ equations and $2$ unknowns. Write in matrix form:
$$\begin{pmatrix}
  1&x_1\\
  1&x_2\\
\vdots\\
  1&x_m\\
\end{pmatrix}
\begin{pmatrix}
  \ah_0\\ \ah_1
\end{pmatrix} = \begin{pmatrix}
  y_1\\ \vdots \\ y_m
\end{pmatrix}$$
Write this $Aa=y$. In general, there is no solution. Gvien any $\ah =
\begin{smallmatrix}
  \ah_0\\ \ah_1
\end{smallmatrix}$, let $r=y-Aa$, a vector of length $m$.

Notice $r_i=y_i-[Aa]_i=y_i-(\ah_0+\ah_ix_i)$. We can restate the goal
as to minimize $\sum_i^n r_i^2 = ||r||_2^2$

\section{October 11th Class 10}
\label{sec:class10}

\subsection{Continue on Least Squares}
\label{sec:leastsqares}
Given $A\in \mathbf{R}^{m\times n}$, $m>n$, full-rank, $y\in
\mathbf{R}^m$, we want to \textbf{find $a\in \mathbf{R}^n$ s.t. $||y-Aa||_2$
is minimal.}

\emph{Example}: Data $(x_1,y_1), \dots, (x_m, y_m)$, we want to find a
polynomial $p_1(x) = a_0+a_1x$ s.t. $p(x_i)\approx p(y_i)$.
$$
\begin{pmatrix}
  1 & x_1\\
1 &x_2 \\
\vdots & \vdots \\
1 & x_m
\end{pmatrix}
\begin{pmatrix}
  a_0 \\ a_1
\end{pmatrix}
=
\begin{pmatrix}
  y_1\\ y_2 \\ \vdots \\ y_m
\end{pmatrix}
$$

Geometric Interpretation (m=3): $(x_1, y_1), (x_2, y_2), (x_3,
y_3)$. see notebook. Projection of $\vec y$ on the plane spanned by
$\vec 1$ and $\vec x$ is the solution $a_0\vec 1 + a_1\vec x = Aa$ that
minimizes $||y-Aa||_2$. 

From this geometric argument, we have $y-Aa \perp $ plane, which is
the set of linear combination of columns of $A$. That is:
$$y-Aa \perp range(A)$$ where $range(A)=\{Aw| w\in \mathbf{R}^n\}$
So
\begin{align*}
<Aw, y-Aa> = 0\\
<w, A^T(y-Aa)> = 0 \text{ } \forall w\in \mathbf{R}^n
\end{align*}
For any $\vec w$ we get range of $A$
Take for example of $\vec w=A^T(y-Aa) \in \mathbf{R}^n$ That is, 
\begin{align*}
<A^T(y-Aa), A^T(y-Aa))> &=0\\
|A^T(y-Aa)||^2_2 &= 0\\
&\Rightarrow A^T(y-Aa) = 0 \text{ or }\\
A^TAa = A^Ty
\end{align*}
$A^TAa = A^Ty$ is the normal equations. $A^TA$ (squre, $n\times n$) is
non-singular (because $A$ is full-rank). We can solve this by gaussian elimination.
We don't want to because of conditioning issues imposed by gaussian elimination.

\subsection{QR Decomposition}
\textbf{Definition:} A matrix $Q\in \mathbf{R}^{m\times m}$ is
\emph{orthogonal} if $Q^TQ=I$

\textbf{Theorem:} For any $A\in \mathbf{R}^{m\times n}$, $m>n$, $\exists$,
an orthogonal matrix, $Q\in \mathbf{R}^{m\times m}$, and an upper
triangular matrix $R\in \mathbf{R}^{m\times n}$ s.t. $A=QR$

To minimize $||y-Aa||^2_2$, we want
\begin{align*}
  <y-Aa, y-Aa> &= <y-QRa, y-QRa>\\
&=<QQ^Ty-QRa, QQ^Ty-QRa> \text{ since } QQ^T=I\\
&=<Q(Q^Ty-Ra), Q(Q^Ty-Ra)>\\
&=<Q^Ty-Ra), (Q^Ty-Ra)> \text{ left mult} Q^T\\
&=||\hat y - Ra||^2_2
\end{align*}
Where $\hat y = Q^Ty$. To minimize $||\hat y - Ra||_2$, 
$$||
\begin{pmatrix}
\hat y_{1:N}\\ \hat y_{n+1:m}  
\end{pmatrix} - 
\begin{pmatrix}
R_1a \\ 0
\end{pmatrix}||
$$
Where $R_1$ is the upper $n\times n$ subblock of $R$.
Solve $R_1a = \hat y_{1:n}$. We can't do anything about $\hat
y_{n+1:m}$.
$R_1$ is an upper triangular, it's non-singular because $A$ is
full-rank (this is a \emph{claim:} if $A$ if full-rank then $R_1$ is non-singular). We can
do back-substitution!

\textbf{Pf} of claim 1:
\begin{align*}
  Q&=QQ^TQ= QI\\
&=(QQ^T)Q \text{ mult right by }Q^{-1}\\
QQ^{-1}&= (QQ^T)QQ^{-1}\\
I &= QQ^T
\end{align*}

So to solve least squares, do the $QR$ decomposition, then solve
$R_aa=\hat y_{1:n}$. So to get $a$, we need $R_1$ and the first $n$ entries
of $\hat y$.

So lets write $Q$ as $[Q_1 | Q_2]$, where $Q_1$ is $m\times n$, $Q_2$
is $m \times (m-(n+1))$. Then, $Q^Ty =
\begin{pmatrix}
  Q_1^Ty\\
Q^T_2y
\end{pmatrix}$, where $Q_1^Ty$ is $\hat y_{1:n}$. So all we need is
$Q_1R_1$. But if we want to know the minimum values of least squares,
you need all of $\hat y$, this will only get us $\hat y_{1:n}$.

\subsection{Why QR and not LU?: Roundoff effects}
\label{sec:roundoff}
Given $Q$ ($Q_1$), $R$ ($R_1$), \textbf{Effects of Roundoff}:
With $LU$:
\emph{Recall from last week}: for solving $Ax=b$ for $A$ $n\times n$ nonsingular, we
showed that $\frac{||x-\hat x||}{||x||} \le \kap(A)\frac{||\del
  A||}{||A||}/ 1-\xi$, $\xi$ very small and irrelevant ($\frac{||\del
  A||}{||A||}$ is also small)and $\hat x$ solves
$(A+\del A)\hat x) = b$.


Consider the normal equation: $A^TAa = A^Ty$. Last week's analysis
gives us $$\frac{||a-\hat a||}{||a||}\le \kap(A^TA)\frac{||\del
  (A^TA)||}{||A^TA||}$$
\emph{Claim:} if $A$ were square, then $\kap(A^TA)$ is $\kap(A)^2$.

If we use $QR$ decomposition to solve the least squares problem, then
the analogous bound will be
$$\frac{||a-\hat a||}{||a||}\le [\kap(A^TA)\frac{||y-Aa||}{||Aa||} +
\sqrt{\kap(A^TA)}]\frac{||\del A||}{||A||}$$
So only if $||y-Aa||$ is small, effect of $\kap(A^TA)$ is negligible,
only bad term is $\sqrt({\kap(A^TA)}$, but this is just
$\kap(A)$. $\hat a$ is the least squares solution to minimize
$||(A+\del A)\hat a - y||_2$.
\pagebreak

\section{Class 11 October 13th}
\label{sec:class11}
Midterm: October 27th
\begin{enumerate}
\item some kind of analysis, a program provided to do something or us
  program something..
\item no proving, but operation counts or reasoning
\end{enumerate}
Projects: use some of the ideas we explored. Testing it against an
application, design, apply, write it up. A month work.

\subsection{QR Factorization}
\label{sec:QR2}

$A\in \mathbf{R}, m\le n$, full rank. Claim: $\exists Q$, orthogonal,
and $R$ upper triangular s.t. $A=QR$ where $Q$ n by n, $R$ m by n, the
upper n by n part upper triangular$=R_1$, rest all 0.

\textbf{How to make $Q$?}

\subsubsection{Method 1: Gram-Schmid Orthogonalization}
For $Q_1$ and $R_1$ only here.
Steps:
\begin{enumerate}
\item set Column 1 of $A$, $a_1 = q_1r_{11}$. We need $q_1^Tq_1= 1$
  (col of $Q$ is has norm 1). We can get this by taking $\hat q_1 =
  a_1$, then $q_1=\frac{\hat q_1}{||\hat q_1 ||} = \frac{a_1}{||a_1||}
  $
Let $r_{11} = ||a_1||$, then $a_1 =r_{11}q_1$
\item Require $a_2 = q_{1}r_{12} + q_2r_{22}$ at this point we know
  $q_1$ and $a_2$. we have two conditions: 1. $q_1^Tq_2= 0$ and
  2. $q_2^Tq_2 = 1$


For 1: take $\hat q_2 = a_2 - q_1r_{12}$, then impose $\lc \hat q_2,
q_1\rc = 0$ so $\lc \hat q_2,
q_1\rc = \lc a_2, q_1\rc  - r_{12}\lc q_1, q_1\rc $ but $\lc q_1, q_1\rc =1$ and we know
$\lc a_2, q_1\rc $. So $r_{12} = \lc a_2, q_1\rc $ so now $\hat q_2$ is defined. 

For 2: take $q_2 = \frac{\hat q_2}{||\hat q_2||}$ so $r_{22} = ||\hat
q_2||$, this gives $a_2 = q_{1}r_{12} + q_2r_{22}$.
\item (Part of step 3): $a_3 = q_1r_{13} + q_2r_{23} + q_3r_{33}$, and
  force $\lc q_1, q_3\rc  =0$, $\lc q_2, q_3\rc =0$, $\lc q_3, q_3\rc  = ||q_3|| = 1$.
Define $\hat q_3 = a_3 - q_1r_{13} - q_2r_{23}$, impose the condition
with $\hat q_3$, then solve for $q_3$ using the definition of $\hat q_3$.
\end{enumerate}

\textbf{Pseudocode}:
\begin{algorithmic}
\FOR{ $k=1 \to n$}
  \STATE $\hat q_k \gets a_k$
  \FOR{ $i=1 \to k-1$}
     \STATE $r_{i,k} = \langle q_i, a_k \rangle$
     \STATE $\hat q_k \gets \hat q_k - q_i r_{ik}$
  \ENDFOR
  \STATE $r_{kk} = ||\hat q_k||$
  \STATE $q_k \gets \hat q_k/r_{kk}$
\ENDFOR
\end{algorithmic}

To solve $\min ||y-Aa||$, we know solve $R_1a = Q_1^Ty$, for $a$. Can
we get all the residual just from this? We said no because otherwise
we can't calculate the residual but this isn't true. Once we have $a$, it's an exact solution or it isn't. if it's
not, $||y-Aa||$ is how big the residual is. We don't need $Q_2$! But
there must be, so think about it..

If we want all of $Q=[Q_1; Q_2]$ A different way of doing it:
\subsubsection{Householder Matrix}
Given $\vec v\in \mathbf{R}^m$, Define a special orthogonal matrix 
$$P= I - 2\frac{\vec v \vec v^T}{\vec v^t\vec v}$$ (a function of
$\vec v$)
Claim: $P$ is symmetric, $P=P^T$, (``obvious'') and orghotonal
\begin{align*}
P^TP &= (I - 2\frac{\vec v \vec v^T}{\vec v^t\vec v})(I - 2\frac{\vec
  v \vec v^T}{\vec v^t\vec v})\\  
&= I - 4\frac{\vec v \vec v^T}{\vec v^t\vec v} + 4 \frac{(\vec v \vec
  v^T)(\vec v \vec v^T)}{(\vec v^t\vec v)^2}\\
&= I - 4\frac{\vec v \vec v^T}{\vec v^t\vec v} + 4 \frac{(\vec v\vec
  v^T)}{(\vec v^t\vec v)} \text{ insides cancel}\\
&= I
\end{align*}

Given a vector $\vec x$, we want to find another vector $\vec v$ s.t.
$P$ generated by $v$ satisfies $P_v\vec x =(
\begin{smallmatrix}
  e_1 \\ 0 \\ \vdots \\ 0
\end{smallmatrix})$. Zero below the first entry. Supposed I had
this. Then (looking for $\vec v$),
\begin{align*}
  P_vx &= (I - 2\frac{\vec v \vec v^T}{\vec v^t\vec v})\vec x\\
&= \vec x - 2\frac{\vec v^T \vec x}{\vec v^t\vec v})\vec v\\
&=
\begin{pmatrix}
  x_1\\x_2\\ \vdots \\ x_m
\end{pmatrix} - c\begin{pmatrix}
  v_1\\v_2\\ \vdots \\ v_m
\end{pmatrix} \\
\end{align*}
We need $x_i = cv_i$, $c\in \mathbf{R} \forall i=2\dots m$
This tells us that $v = c(x+ \ah e_1)$, take $c=1$ and want $\vec v =
\vec x + \ah e_1$. 

For this $\vec v$:, $v^Tx = x^Tx +\ah x_1$ since $e_1^Tx = x_1$
Then $vv^T = \lc x+\ah e_1, x+\ah e_1 \rc = x^Tx + 2\ah x_1 + \ah^2$
So 
\begin{align*}
2\frac{v^Tx}{v^Tv} &= 2\frac{x^tx+\ah x_1}{x^Tx + 2\ah x_1 + \ah^2}\\
&\text{ take $\ah^2=x^Tx = ||x||$ }\\
&=2\frac{x^tx+||x|| x_1}{2(x^Tx + ||x||x_1)}\\
&=1
\end{align*}
Then $x-v = x - (x+\ah e_1) = -\ah e_1$.

We've shown that given $\vec x$ we can find a $\vec v$ that does this.
Big picture, do this to every column of $A$, then we get bunch of $P$
which will be our $Q$! then the remainder gives us upper triangular
matrix $R$. 

\pagebreak

\section{Class 11 October 18th 2011}
\label{sec:class11}

\subsection{Eigenvalue Problems}
\label{sec:eigenvalue}

$A\in \mathbf{R}^{n\times n}$, find $\vec v\in \mathbf{R}^n$ (or
$\mathbf{C}^n$) and a scalar $\lam$ (possibly complex) s.t. $$A\vec v =
\lam v$$

Example of source: consider systems of ordinary differntial equations
of order $n$, $\frac{dx}{dt} = Ax$, $x = [x_1(t), x_2(t), \cdots,
x_n(t)]^T$, where $A$ is $n$ by $n$ matrix. $\frac{dx}{dt}$ is a
vector whos first entry is the first row of $A$ times
$x_1(t)$. $\frac{dx_1}{dt} = a_{11}x_1 + \cdots + a_{1n}x_n$,
$\frac{dx_2}{dt} = a_{21}x_2 + \cdots + a_{2n}x_n$ and so forth.

If $A$ has $n$ linearly independent eigenvectors, $v_1, v_2, \dots,
v_n$, with associated eigenvalues $\lam_1, \lam_2, \dots,
\lam_n$.($Av_i = \lam_iv_i$. Then $x(t)$ can be written as linear combination of these
eigenvectors (because they form the basis of
$\mathbf{R}^n$. i.e. $x(t) = g_1(t)v_1 + g_2(t)v_2 + \cdots+
g_n(t)v_n$.

The left hand side of $\frac{dx}{dt} = Ax$ is now:
$$\frac{dx}{dt} = \frac{dg_1}{dt}v_1 + \frac{dg_2}{dt}v_2 + \cdots +
\frac{dg_n}{dt}v_n$$
The right hand side is:
\begin{align*}
Ax&=g_1(t)Av_1 + g_2(t)Av_2 + \cdots + g_n(t)Av_n\\
&= g_1(t)\lam_1v_1 + g_2(t)\lam_2v_2 + \cdots + g_n(t)\lam_nv_n
\end{align*}
Now setting them equal to each other,
we have $\frac{dg_i}{dt}=g_i(t)\lam_i$.
so $g_i = c_1e^{\lam_it}$, where $c_i$ some constant. (because the der
of $\frac{d(c_1e^{\lam_it})}{dt} = c_1e^{\lam_it}\lam_1 = g_1\lam_1$)
So the solution is $$x(t) = c_1e^{\lam_1t}v_1+ c_2e^{\lam_1t}v_2
+\cdots + c_ne^{\lam_1t}v_n$$

\subsection{How to compute eigenvalues?}
\label{sec:computeeig}
Solve the characteristic polynomial: if $Av=\lam v$, then $(A-\lam I)v
= 0$. Which means $(A-\lam I)v = 0v$, i.e. $(A-\lam I)$ has a $0$
eigen value i.e is singular. Therefore, $det(A-\lam I)$ is
0. $det(A-\lam I)$ is a polynomial of degree n, the
\emph{characteristic polynomial}.

There is no finite form algorithm to compute the eigenvalues if $n\ge
5$, from Galois theory. (can't find roots of polynomial order bigger
than 5).

\subsubsection{Power method} assume $A$ has $n$ linearly independent
eigenvectors ($v_1,v_2,\cdots,v_n$, $\lam_1,\cdots,\lam_n$). Also assume $|\lam_1|>|lam_2|\ge \cdots \ge |\lam_n|$,
and we call $\lam_1$ the \emph{dominant} eigenvalue.

Any $w\in \mathbf{R}^n$ can be written as a linear combination of
these eigen vectors. $w=\ah_1v_1 + \ah_2v_2 + \cdots + \ah_nv_n$.
So 
\begin{align*}
Aw &= \ah_1Av_1 + \cdots + \ah_nAv_n  \\
&=\ah_1\lam_1v_1 + \ah_2\lam_2v_2 +\cdots + \ah_n\lam_nv_n  \\
&=\lam_1(\ah_1v_1 + \ah_2\frac{\lam_2}{\lam_1}v_2+ \cdots + \ah_n\frac{\lam_n}{\lam_1}v_n)  \\
\end{align*}
We know $(|\frac{\lam_i}{\lam_1}|)^2 <|\frac{\lam_i}{\lam_1}| < 1$ $\forall i>1$ and
Then 
\begin{align*}
   A^2w = A(Aw) &= \lam_1^2(\ah_1v_1 +
   \ah_2(\frac{\lam_2}{\lam_1})^2v_2+\cdots +
   \ah_n(\frac{\lam_n}{\lam_1})^2v_n)  \\
  A^kw &= \lam_1^k(\ah_1v_1 +
  \ah_2(\frac{\lam_2}{\lam_1})^kv_2+\cdots +
  \ah_n(\frac{\lam_n}{\lam_1})^kv_n)  \\
\end{align*}
Eventually, $(|\frac{\lam_i}{\lam_1}|)^k$ will go to 0. So for $k$
large enough, $A^kw \approx \lam_1^k\ah_1v_1$.

Pseudocode for this:
\begin{algorithmic}
\STATE $w_0= \to w$, $||w||=1$
  \FOR{ $i=1 \to \dots$}
     \STATE $\hat w_{i+1} = Aw_i$
     \STATE $p_{n+1} = \sqrt{\lc \hat w_{i+1}, \hat w_{i+1}\rc }$ just
     normalization
     \STATE $w_{i+1} = \frac{\hat w_{i+1}}{p_{i+1}}$
     \STATE $\mu_{i+1} = \lc Aw_{i+1}, w_{i+1}\rc$
  \ENDFOR
\end{algorithmic}
Because 
\begin{align*}
Av&=\lam v\\
\lc Av, v \rc &= \lc \lam v, v\rc\\
\lc Av, v \rc &= \lam\lc v, v\rc\\
\lam &= \frac{\lc Av, v \rc }{\lc v, v\rc}
\end{align*}

and $\mu$ is the estimate of $\lam$, and $w$ is the estimate of
eigenvector.

\subsubsection{Direct Method}\label{sec:directmethod}
Using QR algorithm:
\begin{algorithmic}
\STATE $A_0= \to A$
\FOR{ $i=1 \to \dots$}
   \STATE factor $A_k=Q_kR_k$
   \STATE $A_{k+1} \to R_kQ_k$
\ENDFOR
\end{algorithmic}

First assume all the eigenvalues of $A$ are real.
\emph{Claim:} as $k\rarr \infty$, $A_k$ converges  to an upper
triangular matrix. The eigenvalues of this upper triangular matrix is
the diagonals.

We stop iteratin when everything below the diagonals of $A_k$ is $\eps$, machine
precision. 

\emph{Claim:} $A$ and $A_k$ have the same eigen values.
Pf: $A_0=A$ trivial.
At first iteration, $A_0 = Q_0R_0$, $A_1 = R_0Q_0$. $R_0$ is
$Q_0^{-1}A_0$, since $Q_0$ is an orthogonal matrix, $R_0=Q_0^TA_0$
So $A_1 = (Q_0^TA_0)Q_0 = Q_0^TAQ_0$. So $A_1=Q_0^{-1}AQ_0$, this is
the \emph{similarity transformation}. $A_1$ and $A$ are similar, and
therfore they have the same eigenvalues. (if $B=P^{-1}AP$< then $A,B$,
are similar and they have the same eigenvalues.)

Second iteration: $A_1 = Q_1R_1$, $A_2=R_1Q_1$. Again $R_1 =
Q_1^TA_1$, so
\begin{align*}
A_2 &= Q_1^TA_1Q_1 \\ &= Q_1^T(Q_0^TAQ_0)Q_1\\  
&=(Q_0Q_1)^TA(Q_0Q_1)
\end{align*}
Call $Q = (Q_0Q_1)$. So $A_2 = Q^TAQ=Q^{-1}AQ$, so $A$ and $A_2$ are
similar as well. 

To do $A_k=Q_kR_k$ is $O(n^3)$, and we need to iterate at least $n$ times, so
the total cost is $O(n^4)$

How to make this into $O(n^3)$.

\begin{enumerate}
\item 

First reduce $A$ to upper hesenberg form:\\
\textbf{Definition:} $$
\begin{pmatrix}
  y & x & x & x & x & x\\
  x & y & x & x & x & x\\
  0 & x & y & x & x & x\\
  0 & 0 & x & y & x & x\\
  0 & 0 & 0 & x & y & x\\
  0 & 0 & 0 & 0 & x & y\\
\end{pmatrix}$$
A matrix where entries below the subdiagonal (x) is all 0. (y is the
diagonal).

Any real matrix can be transformed into an upper Hessenberg form using
Householder transformations ($I-vv^T$). With cost $O(n^3)$. This is an exercise.

Let $H=P^TAP$ be the upper Hessenberg matrix, where $P$ is the product
of householder transformations. Since $P$ orthogonal, $H$ and $A$
have the same eigenvalues.
\item apply $QR$ algorithms to $H$. The advantage is that to do $QR$
  on hessenberg matrix takes $O(n^2)$ instead of $O(n^3)$. 
\end{enumerate}
So the total cost is $O(n^3) + O(n^3)= O(n^3)$\\

How to make a hessemberg matrix:
\textbf{Givens Rotation}:
$P = 
\begin{pmatrix}
  c&s\\-s & c
\end{pmatrix}
$, where $c^2+s^2=1$, so $\begin{pmatrix}
  c&s\\-s & c
\end{pmatrix}
\begin{pmatrix}
  a\\b
\end{pmatrix}=
\begin{pmatrix}
  ca+sb \\-sa+cb
\end{pmatrix}
$. Our goal is to make the second component 0.
so $c$ and $s$ satisfy $-sa+cb = 0$ and $c^2+s^2=1$, which gives us
$s=\frac{b}{\sqrt{a^2+b^2}}$
and $c=\frac{a}{\sqrt{a^2+b^2}}$
The rotation matrix $P$ is orthogonal. $$P^TP = \begin{pmatrix}
  c&-s\\s & c
\end{pmatrix}\begin{pmatrix}
  c&s\\-s & c
\end{pmatrix}=
\begin{pmatrix}
  c^2+s^2& cs-sc\\ sc-cs & s^2+c^2
\end{pmatrix} = I$$

Assume we have the following upper hessenberg matrix:
$$\begin{pmatrix}
  h_{11} & h_{12} & h_{13}\\
  h_{21} & h_{22} & h_{23}\\
 0 & h_{32} & h_{33}
\end{pmatrix}$$
We want to make this into an upper triangular, so we need to get rid
of $h_{21}, h_{32}$.

Let $G_1 =
\begin{pmatrix}
  c&s&0\\-s&c& 0 \\ 0 & 0 & 1
\end{pmatrix}$, then 
$$G_1H = \begin{pmatrix}
  c&s&0\\-s&c &0 \\ 0 & 0 & 1
\end{pmatrix}\begin{pmatrix}
  h_{11} & h_{12} & h_{13}\\
  h{21} & h_{22} & h_{23}\\
 0 & h_{32} & h_{33}
\end{pmatrix}
=
\begin{pmatrix}
  \xi & \xi & \xi \\
-sh_{11}+ch_{21} & \xi & \xi\\
0 & h_{32} & h_{33}
\end{pmatrix}
$$
Solve $-sh_{11}+ch_{21}=0$ constrained to $s^2+c^2=1$, then
$s=\frac{h_{21}}{\sqrt{h_{21}^2+h_{11}^2}}$, and
$c=\frac{h_{11}}{\sqrt{h_{11}^2+h_{21}^2}}$

This cost us $O(n)$.

Now $G_1H = 
  \begin{pmatrix}
  \xi & \xi & \xi \\
0 & \xi & \xi\\
0 & h_{32} & h_{33}
\end{pmatrix}$. 

Using $G_2=
\begin{pmatrix}
  1 &0 &0\\
0 & c & s\\
0 & -s & c
\end{pmatrix}
$, do $G_2G_1H$ to get R
$  G_2G_1H = R$, where $H=(G_2G_1)^TR_0$, and $G_2G_1$ is $Q_0$.
SO $H_1 = R_1Q_1 = G_2G_1 H G_1^TG_2^T$, multiplication is order n.
\pagebreak

\section{Class 12 October 20th 2011}
\label{sec:class12}
Midterm 27th (next thursday). Maybe some operation counts, analysis,
semi-rigorous on round-off effects (first 3-4 lectures).
HW will be out before the exam, no expectation to do it.
Cover materials all the way up to this class.

\subsection{Why we want $Q_2$}
$A=QR=Q_1R_1$ Only need $Q_1$,$R_1$ to solve least squares
problem. $Q=[Q_1|Q_2]$, $R=[R_1; 0]$. $A^T=[R_1^T 0][Q_1^T; Q_2^T]$,
  vector $v=Q_2\ah \in range(Q_2)$.

Then 
\begin{align*}
A^Tv &= [R_1^T 0][Q_1^TQ_2\ah; Q_2^TQ_2\ah]   \\
&=[R_1^T 0][0; \ah] = 0\\
\end{align*}
Because $$I=Q^TQ = [Q_1^T; Q_2^T][Q_1 Q_2] =
\begin{pmatrix}
  Q_1^TQ_1 & Q_1^TQ_2\\\
  Q_2^TQ_1 & Q_2^TQ_2\\\
\end{pmatrix} =
\begin{pmatrix}
I&0\\0& I  
\end{pmatrix}
$$

Now the \emph{claim}: if $A^Tv = 0$, then $v\in range(Q_2)$. 

This means the
colums of $Q_2$ span the null space of $A^T$ -> this is the reason why
we want to know the entire $Q$ and not $Q_1$
(not on the exam)

Proof of claim: Given $v$ s.t. $A^Tv = 0$, $v$ is an $m$-vector, (the
larger dimension). So $v= Qa$ ($Q$ is a matrix of $m$ linearly
independent columns). then $v=Qa = [Q_1Q_2][a_1;a_2] = Q_1a_1 +
Q_2a_2$.
Then 
\begin{align*}
A^Tv &=[R_1^T 0][Q_1^T; Q_2^T][Q_1Q_2][a_1;a_2]\\  
&=[R_1^T 0]I[a_1;a_2] = 0\\  
\end{align*}
If $A$ is of full-rank, then $R_1$ is also full-rank, which means
$a_1=0$ (because $R_1$ is non-singular, the only way we can get
$R_1^Ta_1=0$ is if $a_1=0$), and $v=Q_2a_2\in range(Q_2)$.

\subsection{Eigenvalue problem}
Given $A$, an $n$ by $n$ matrix (in $ \mathbf{R}$ or $
\mathbf{C}$). We want to find $\lam, v$ s.t. $Av - \lam v = 0$. Maybe we
want $n$ of these (if they exist) linearly independent.

\subsubsection{Power method}
Start with arbitrary $x^{(0)}$. If $A$ has $n$ eigenvectors, then
$x^{(0)} = \ah_1v_1 + \ah_2v_2 + \cdots + \ah_nv_n$, a linear
combination of $v$.

\begin{align*}
Ax^{(0)}&= \ah_1\lam_1 v_1 + \ah_2\lam_2 v_2 + \cdots + \ah_n\lam_n
v_n\\
A^2x^{(0)}&= \ah_1\lam_1^2 v_1 + \ah_2\lam_2^2 v_2 + \cdots +
\ah_n\lam_n^2 v_n\\
&\vdots\\
A^kx^{(0)}&= \ah_1\lam_1^k v_1 + \ah_2\lam_2^k v_2 + \cdots +
\ah_n\lam_n^k v_n\\
&= \lam_1^k (\ah_1v_1 + \ah_2(\lam_2/\lam_1)^k v_2 + \cdots + \ah_n(\lam_n/\lam_1)^k v_n\\
\end{align*}
Suppose $|\lam_1| > |\lam_i|$, for all $i\neq 1$. Then, all of
$\frac{\lam_i}{\lam_1}^k \rarr 0$ as $k\rarr \infty$.

Now, we only need to consider $A^kx^{(0)} \sim $ direction of $v_1$.
Even if $\ah_1=0$ , unless all of the data is a power of 2, I'll get
the answer, because there will be a rounding error. so even if
$\ah_1$ starts out to be 0, it won't.
(Try it with a 3 by 3 example).

Say $A^kx^{(0)}$ normalized. We're only getting $v_1$, how do I get
$\lam_1$?
Notice: $Av_1=\lam_1v_1$, look at $v_1^TAv_1 = v_1^T\lam_1v_1 =
\lam_1v_1^Tv_1$
so $\lam_1 = \frac{v_1^TAv_1}{v_1^Tv_1}$. 

Now look at $x^{(k)}$ k-th iterate obtained by the power method. Now
look at $$\frac{x^{(k)'}TAx^{(k)}}{x^{(k)'}x^{(k)}}$$ and this is the
estimate for $\lam_1^{(k)}$.

To check for convergence, look
at $$\frac{||Ax^{(k)}-\lam_1^{(k)}||}{||x^{(k)}||}\rarr_? 0$$
Stop when this is less than some tolerance $\eps$.

To look for the smallest $\lam$, look for the largest
$\frac{1}{\lam_i}$ because 
\begin{align*}
Av&=\lam v\\
v &= \lam A^{-1}v
\frac{1}{\lam}v = A^{-1}v
\end{align*}
To do this, factor $A=LU$, then in the power method when multiplying
$A$ (to perform the operation $x^{(k+1)} = A^{-1}x^{(k)}$), instead
solve the system:
$Ax^{(k+1} = x^{(k)}$ using $A=LU$. because you never want to compute
inverse of $A$ directly!!!

Two reasons:
\begin{itemize}
\item More stable numerically to use gaussian elimination
\item This is just cheaper.
\end{itemize}


\subsubsection{Compute all the eigenvalues}
of $A$,
Step 0 is to find an orthogonal $Q$ s.t. $Q^TAQ = $Hessenberg
  form. Nonzero above the subdiagonal.
How to do this? The givens rotation, or householder matrix!

With A, apply householder transformation on the left: $Q_1A =
\begin{pmatrix}
 \xi & \xi &\cdots & \xi\\
 \xi &\xi &\cdots & \xi\\
 0 &\xi &\cdots & \xi\\
\vdots & \xi &\cdots & \xi\\
0 &\xi&\cdots & \xi\\
 \end{pmatrix}
$
Then, apply $Q_1$ on the right. this will give me $Q_1AQ_1^T$ (can
write it like that because householder transformations are symmetric).
\emph{Claim}: the result is still 0 in the first column below the
second entry. (WHY?? follows from the definition of the householder
matrix (how $v$ is made).

Next step is apply it again to the second column, do it $n-1$ times
and get the hessenberg-form.

(that's the \emph{preliminary} step --\_\_--)

Now step1: start the $QR$ iteration:
\begin{algorithmic}
\STATE$H_0=H$
\FOR {$k=0,1,2,\dots$}
\STATE $H_k=Q_kR_k$
\STATE $H_{k+1}\to R_kQ_k$ 
\ENDFOR  
\end{algorithmic}

Notice: $H_{k+1}= Q^TH_kQ$ similarity matrix

Shifted version:
\begin{algorithmic}
\STATE$H_0=H$
\FOR {$k=0,1,2,\dots$}
\STATE $\hat H_k=H_k - \rho I$ subtract $\rho$ from the diagonal
\STATE $\hat H_k=Q_kR_k$ then do the factorization
\STATE $ H_{k+1}\to R_kQ_k + \rho I$ 
\ENDFOR  
\end{algorithmic}
 Now 
 \begin{align*}
H_{k+1} &= Q_k^T\hat H_k Q_k + \rho I\\   
&= Q_k^T(H_k -\rho I)Q_k + \rho I\\   
&=Q_k^T(H_kQ_k) - Q_k^T\rho I Q_k + \rho I\\
&=Q_k^T(H_kQ_k) - \rho I + \rho I = Q_k^TH_kQ_k\\
 \end{align*}
Similarity again!
When the subdiagonals go to zero, they go down to $\eps$ and consider
them as 0. THen we're left with an upper triangular matrix. But the
eigen values of the uppter triangular matrix is the values in on the
diagonal! So we have all of them. (From Galois, we can't do this, but
we ``can'' with machine precision).

Why do tehy go to zero? Claim: given
$(
\begin{smallmatrix}
  \ah & \gam \\ \beta & \del
\end{smallmatrix})
$, if $\beta$ is small, $\beta\approx \eps$, then after 1 $QR$ step,
$\beta \rarr O(\eps^2)$

The last 2 by 2 matrix in $Q^TAQ$, the (2,1)  of the bottom 2 by 2
matrix goes to $\eps^2$, so once we do that, move up to the next 2 by
2 matrix on the diagonal.

\end{document}
