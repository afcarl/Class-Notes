\newcommand{\sig}{\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ah}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\kap}{\kappa}
\newcommand{\rarr}{\rightarrow}
\newcommand{\larr}{\leftarrow}
\newcommand{\lrarr}{\Leftrightarrow}
\newcommand{\ol}{\overline}
\newcommand{\mbb}{\mathbb}
\newcommand{\contra}{\Rightarrow\Leftarrow}
% for cross product
\newcommand{\lc}{\langle} %<
\newcommand{\rc}{\rangle} %>
\newcommand{\pri}{^{'}} %'

%other shortcuts
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\beq}{\begin{quote}}
\newcommand{\enq}{\end{quote}}
\newcommand{\hsone}{\hspace*{1cm}}
\newcommand{\hstwo}{\hspace*{2cm}}
% vision
\newcommand{\conv}{\convolution}
\newcommand{\Ix}{I_x}
\newcommand{\Iy}{I_y}
\newcommand{\Ixy}{I_{xy}}
% big 0
\newcommand{\cO}{\mathcal{O}}
% calc
\renewcommand{\vec}[1]{\boldsymbol{#1}}
% gradient
\newcommand{\grad}{\nabla}

\newcommand{\noi}{\noindent}
\parskip 5pt
\parindent 0pt

\documentclass[10pt,a4paper]{article}
\usepackage[letterpaper,hmargin=1.15in,vmargin=1.25in]{geometry} % full page dimensions
\usepackage{amsmath,amssymb,algorithmic,mathabx,hyperref}
\begin{document}
\title{Image Segmentation CS828 Spring '12}
\author{Angjoo Kanazawa}
\maketitle
\section{January 25th 2012 - Lecture 1}
\textbf{Definition}: Segmentation (for this class):
\begin{itemize}
\item About low level vision in general
\item Requires a lot of knowledge about th eworld, high level understanding,
quite challenging.
\item  So we're going to focus on simpler segmentation
that doesn't require that much knowledge about the world: Uniform
surfaces, smooth shape. Still there will be varietion in intensity.
\item Want to find uniform region in things (texture, color, motion, smoothness), not necessarily world property. Removed from true segmentation of objects but still useful.
\item Image is an 2D geometric structure. Segmentation is clustering
  that takes advantage of this structure. Based on the assumption that
  near-by pixels have the same intensity. 
\item 
\end{itemize}

We're going to look at
\begin{enumerate}
\item Diffusion
\item Anisotropic diffusion
\item Graph based algorithms: message passing, thinking of an image as
  a graph, every pixel is a node in a graph, edges to neighbors
  $\rarr$ Markov Random FIeld. Gives us a probablistic way to express
  the state of a node in relation to its neighbors. Usually NP-hard, but graph-cut and
  belief propagation algorithms still work. The biggest issue is when
  the number of labels is big.
\item Conditional Random Fields, a general version of MRF
\item Normalized Cut: form a graph
\item Wavelets
\end{enumerate}

\textbf{Math}
\begin{tabular}{l c r}
Fourier transforms &  Convolution & Diffusion\\
Wavelets & Level sets & Riemannian Geometry  \\
\end{tabular}

\textbf{Current Research}
\begin{tabular}{l r}
Bilateral filtering (by Morel) & Texture Segmentation\\
Cosegmentation & Affinity propagation\\
\end{tabular}

\textbf{Workload}
\begin{enumerate}
\item Reports (6 out of 8 papers):Be critical when reading papers, even if the paper is good, what is
the really important. Learn to recognize, have a taste. (10\%)
\item Presentations: 3 presentations per day, 15 min per paper 10 min
  each to discuss paper (15\%)
\item a take home midterm, Final all on lecture material (50\%)
\item Problem set/Project (25\%)
\end{enumerate}
\pagebreak
\section{January 30th - Lecture 2}

\subsection{Perceptual Grouping}

\begin{itemize}
\item Putting pieces to preceive as a whole.
\item Depends on the prior knowledge/statistics about the world.
\end{itemize}

\paragraph{History}
  \begin{itemize}
  \item Behaviorists dominated in early 20th century, wanted to make
    psychology scientific, focused on quantifyiable things.
  \item Rejected anything introspective or mind building internal representations.
  \item AI, computers, chomsky killed behaviorists.
  \item Gestalt movement claimed visual system perceived world as a
    objects and surfaces, as a whole and not as raw atomic stimulus/intensities.
  \end{itemize}
\paragraph{Classical principles/cues}
  \begin{itemize}
  \item Knowing the role of edges is critical to how we perceive an image
  \item Similarity, Good continuation, Common Form, Connectivity, Symmetry (seems
    to jump out), Convexity, Closure, Common Fate, Paraallelism, Collinearity
  \item convexity beats symmetry? Connectivity also beats symmetry?
  \end{itemize}

  \paragraph{Theories}
  \begin{itemize}
  \item We perceive shapes that are ``good form'': smooth curves,,
    pretty abstract
  \item Bayesian: organizaton that's most likely to be true. Not
    computationally friendly. Rather than checking all possible
    options, maybe we look for a certain small set of
    possiblities. Still doesn't explain everything
  \item 
  \end{itemize}
  
\pagebreak
\section{February 1st - Lecture 3: Fourier Transform}
\label{sec:lec3}

\subsection{Mathematical representation}

\label{sec:math-repr}
a point in a $\mathbf{R^2}$ can be represented in a
coordinate. If $p=(7,3)$, we really mean $p = 7(1,0) +
3(0,1)$. \emph{Any} point can be represented by a linear combination
of two vectors. The basis vectors are:
\begin{enumerate}
\item Span the entire space: every point in the space can be written by  linear combinations of these vectors.
\item Orthogonal: If not, moving in one direction will mean you'll be
  moving in the another direction 
\item Unit: if not, the distance from the origin will not be constant.
\end{enumerate}

We can compute the bases by
\begin{enumerate}
\item Linear Projection (inner product with each basis) 
  \begin{equation}
p = (p\cdot
  (1,0))(1,0) + (p\cdot (0,1))(0,1)\label{eq:1}  
\end{equation}
\item Magnitude of a point $||p||^2 = x^2 + y^2$
v\end{enumerate}

\subsection{Functions in $\mathbf{R}^1$}
The domain of the function is $[0,2\pi]$, and we'll deal with
functions in $\mathbf{R}^1$.

\textbf{Def:} a delta function: $$\del_s(t) =
\begin{cases}
  0 & s\neq t\\
\infty & s=t
\end{cases}, \int_0^{2\pi}\del_s(t) dt = 1$$

We'll write functions by using delta functions as a basis.

In infinite dimensions, 
$$f(t) = \int f_s(\del_s(t))ds$$ is the same as \eqref{eq:1} but in
infinite dimensions. 
Tw basis are orthogonal if their inner products are 0, in infinite
dimensions, this is taking the integral. So delta functions are orthogonal.

This is a bad representation in some ways. It doesn't converge to the
right representation (the function) quickly:  using countable number of delta functions will not be a good
representation of the function because it will only be correct in
those places. We also need a lot of co-efficients. 

\paragraph{Differen Representation}
\label{sec:diff-repr}
Divide the interval $[0, 2\pi]$ into short $k$ intervals with width
$\frac{2\pi}{k}$. Use a rectangle in a interval as basis. They are
orthogonal, so we can scale these rectangles and set it to a height
that is equal to the average of the function in that interval. We have
a piece-wise representation of a function using a finite basis. As $k\rarr \infty$, the approximation gets better. The
\emph{Reimann integral}. Here, we're stuck with a cetain level of
accuracy as we fix $k$.

To get an arbitrary accuracy, we can reuse basis from multiple
$k$s. i.e. if we divide the interval in 2, then 4, etc, then we'll get many
rectangles or infinite bases that are \emph{not} orthogonal, but can
represent any function with finite pieces.

Functions are uncountable, but we're trying to represent it as a
countable set of bases. But this is okay because we enforce the
functions to be continuous. 

\subsection{Fourier Series}
\label{sec:fourier-basis}
The basis elements:
\begin{itemize}
\item Height of $\sqrt{\frac{1}{2\pi}}$
\item $\frac{\cos(t)}{\sqrt{n}}$ all are multiplied by a constant so when
  integrated it is 1.
\item $\frac{\sin(t)}{\sqrt{n}}$
\item $\cos(2t)$, $\sin(2t)$
\end{itemize}

They are unit vectors (normalized) and they are orthonormal
i.e. $\int \sin(t)\cos(t)dt =0$. But better, draw them around
$\pi$. $\sin$ is symmetric around $\pi$, $\cos$ is negative
symmetric. So if they are multiplied together, the signs are different
so they cancel and gives you 0. 

Now, we can write any function as an infinite sum of these basis
elements:

\begin{equation}
  \label{eq:2}
  f(t) = a_0 + \sum_{k=1}^\infty a_k \cos kt + \sum_{k=1}^\infty b_k
  \sin kt
\end{equation}
If the sums were finite upto $N$, then $\lim{N\rarr \infty} ||f(t)|| =
0$. This is a better representation then the delta functions because
if we use enough co-efficients we will get really good approximation
to the function.

$\cos^{2n}(t/2)$: Look at waht $\cos(t/2)$ look like, then raise it to
a higher power. Really quicly, it will peak and look more like a delta
function. By adding a constant in, $\cos(t/2 + a)$, we can shift the
peaks. 

Becasue we know that we can approximate any function with infinite
delta functions, this means we can also do it with these basis. 
There are couple of identities by trigonometry to write higher power
trig functions as a single power functions. i.e. trig functions with
different frequences: 
$sin^2(t/2) = \frac{1-\cos(t)}{2}$, $sin^2(t) = \frac{1-\cos 2t}{2}$

\textbf{Intuition:} In practice, functions are smooth and with very
small coefficients we can get a very good approximations.

\paragraph{Notation}
\begin{equation}
  \label{eq:3}
  \cos kt + i \sin kt = e^{i k t}
\end{equation}

There are simple ways of computing these coefficients $a_k, b_k$. If
we want $a_k$, we \textbf{take the inner product }of the function and $\cos kt$
i.e. $\int f(t) \cos kt dt$.

\paragraph{Complex case}
Given $$c_k = \lc f, e^{ikt}, \rc = \lc f, \cos kt \rc + i\lc f, \sin
kt \rc,$$ $c_{-k} = \lc f, e^{i-kt}, \rc = \lc f, \cos kt \rc - i\lc f, \sin kt\rc$

Then
\begin{equation}
c_ke^{ikt} + c_{-k}e^{-ikt} = a_k \cos kt + b_k \sin kt\label{eq:5}
\end{equation}

We get back to the fourier representation.

Following from $a \sin t + b \cos t = c \cos (t + k)$, $k$ is the phase, or the shift
of functions.

\textbf{Parsevaal's Theorem:} Same as the pythagorean theorem:
$$\int f^2(t) dt = \frac{\pi}{2}a_0^2 + \pi\sum (a_k^2 + b_k^2)$$

This is good to use to measure how good our approximation is. 
So  
We can do $$
||(\int f(t) - a_0 -\sum_{k=1}^N a_k \cos kt - \sum_{k=1}^N
b_k)^2||  = ||(\sum_{N+1}^\infty a_k \cos kt - \sum_{N+1}^\infty b_k )^2||$$


\subsection{Fourier Transform}
\label{sec:fourier-transform-1}
Let $f(t)$ is periodic going from $[0, 2\pi l]$. Then, we can
represent $f(t)$ by
$$
  f(t)= \sum c_k e^{ikt/ l}
$$

(By dividing with $l$, we're stretching the basis element in $[0,
2\pi]$.) As $l\rarr \infty$, this gives us every possible fraction,
all of $\mathbf{Q}$. Which mean we write this as:

\begin{equation}
  \label{eq:7}
  f(t)= \int_{-\infty}^\infty F(k)e^{ikt}dk
\end{equation}

Remember: $e^{ikt}$ carries the orthonromal basis, now extending to
all of $\mathbf{R}$, this means the coefficients are now in the
$\infty$ domain so we write coefficients as $F(k)$, and call this the
\textbf{Fourier transform} of $f(k)$.

\eqref{eq:7} is the approximation of $f(t)$, the inverse operation to get
the fourier transform is;
\begin{equation}
  \label{eq:9}
F(k) = \int_{-\infty}^\infty f(k)e^{-ikt}dk  
\end{equation}

$e^{-ikt}$ is negative because it's the complex conjugate of
$e^{ikt}$, (square it we multiply
it with the complex conjugate.)
\pagebreak
\section{February 6th - Lecture 4: Smoothing \& Convolution}
\label{sec:lecture-4}

Why do we \emph{smooth} images? It's a way of passing information around, also it
connects it more to segmentation (looking for a uniform
property). When we smooth, we can take things that are similar and
make them more similar. It also allows us to represent images in
multiple scales, it helps us get rid of fine details, giving us coarser
representations of an image. That is we want to remove high frequency portion and
analyze the low frequency part. 

Smoothing can be done by \emph{convolution}.

In vision we always assume vision. Given a noisy input, the true intensity
+ noise, say $P_i = 100 + n_i$, smoothing takes the average of all pixels, we'll have
\begin{align*}
 &= \frac{1}{M} \sum P_i\\
&=  \frac{1}{M} \sum 100 + n_i\\
&= 100 + \frac{1}{M} \sum n_i\\
\end{align*}

A simple example of smoothing, the average of a lot of random variables makes the std of noise
smaller. 

\subsection{1-D image}
\label{sec:1-d-image}
Think of 1-D images as a function: $f(t)$. We want to replace a pixel
by the average of its neighbors. We write this as:
\begin{equation}
  \label{eq:4}
  h(t) = \frac{1}{2\del}\int_{t-\del}^{t+\del} f(t^{'}) dt^{'}
\end{equation}
(Where $t^{'}$ is just another points, not derivatives)

Let us define,
\begin{equation}
  \label{eq:6}
  g(t) =
  \begin{cases}
     \frac{1}{2\del} & \text{ for } -\del \le t \le \del\\
     0 & \text{ otherwise }
  \end{cases}
\end{equation}
Note that $g(t)$ sums to 1. (Like a pdf of $U(-\del, \del)$.)

Now, we can write \eqref{eq:4} as
\begin{equation}
  \label{eq:8}
  h(t) = \int_{-\infty}^\infty f(t-u)g(u)du
\end{equation}
It's as if we take the function $g$ and winding it up so that it's
centered around $t$ and taking the inner product to get $h$. It flips,
the left side of the filter is applied to the right side of point
$t$. The resulted $h$ is just shifting $g$ at everypoint and taking the
inner product.

We write this as
\begin{equation}
  \label{eq:11}
  h(t) = g(t)\conv f(t)
\end{equation}

 
It's natural to take the weighted average of your neighbors, because
it's more likely that people around you have more information. So we
use a gaussian filter.

\subsection{Convolution}
\label{sec:convolution}
Two properties:
\begin{enumerate}
\item Linear: $g\conv kf = k(g\conv f)$, $g\conv (f_1 + f_2) = g\conv
  f_1 + g\conv f_2$
\item Shit-invariant: Take my function and translate, then convolve
  with a filter is same as convolving with a filter and shifting
  it. i.e. if $f'(t) = f(t+k)$, $h= g\conv f$, and $h\pri = g \conv
  f\pri$, then $h\pri = h(t+k)$
\end{enumerate}

\textbf{Convolution Theorem}
Given $F$ as the fourier transform of $f$, ($F$ is the function of
frequency), and the same for $G$, $g$, and $H$, $h$.
\begin{equation}
  \label{eq:12}
f\conv g = h \lrarr FG = H  
\end{equation}

\emph{Proof}: $f\conv g = \int f(t-u)g(u)du$, call this $h$. To take
the fourier transform of this, we take the inner product of $h$ and
$e^{-itw}$.
So $$H(w) = \int e^{-itw} (\int f(t-u)g(u)du) dt$$
Define $v = t-u$. Now, 
\begin{align*}
  H(w) &= \int e^{-itw} (\int f(t-u)g(u)du) dt\\
&= \int e^{-i(v+u)w} (\int f(v)g(u)du) dv\\
&= \int e^{-iuw} g(u)du \int e^{-ivw}  f(v) dv\\
&= GF
\end{align*}

The sines and cosines are eigenvectors of functions because when you
convolve it with any filter it just scales it. 

The narrower the gaussian, the broader the fourier transform, The
broader my gaussian, the narrower my fourier transform. So the
lower frequency part gets preserved and the higher frequency (the edge
of gaussian) gets reduced more. 

\emph{Intuition}:
If the gaussian is so sharp that it's like a delta function, it'll
only scale the function at that point $t$. Then, the fourier transform of a delta function is uniformly 1 at all
frequencies. Because convolving with a delta function doesn't change
anything, so $f\conv \delta = f$, bug $F*G = H = F$, so $G$ is a
uniformly 1 that doesn't change anything.

Similarly, if the gaussian is so broad that it's like a uniform function, then
the fourier transform is like a delta function.

\emph{example} A sinc function: $G(w) = \int_{-\delta}^\del \frac{1}{2\delta} e^{-iwt}
dt = \frac{2\sin(\delta w}{w}$. Plot it, bad fourier transform because
the high frequency components go in and out.
Compared, the gaussian filter provides us a very good fourier
transform.

\paragraph{Why remove  higher frequency components?}
\label{sec:why-remove-higher}
If we assume the noise is i.i.d., we can show that the fourier
transform of the noise is uniform. i.i.d. noise has equal energy. Bc
this noise has the same energy everywhere, it's called the \emph{white
noise}. If we think of our image as some smooth pixels with a white noise.
Images tend to have much more low frequencies than high
frequencies. Noise is equal in low and high frequency, so if you
reduce the high frequency components, it significantly reduces the
noise. 

\textbf{Band-pass filter}: Looks like $U(a,b)$, it perfectly preserves
the low frequency component. It's fourier transform is the sinc
function. So there's limitation to use these perfect filters.

\textbf{High-pass filter}: inverse of $U(a,b)$. 

Fourier series: $$f(t) = a_0 + \sum a_k \cos(k t) + \sum b_k
\sin(k t)$$ ($K$ is the frequency)
The derivative:$$f'(t) = \sum - a_k \sin(k t) + \sum b_k
\cos(k t)$$

This is also a fourier series, \emph{taking the derivative has the
effect of scaling the coefficients by the frequency.} As frequency gets
higher, it amplifies the coefficients. 

This is why it's dangerous to take the derivative of a noisy image,
because the derivative ampliflies the high frequency components with a
lot of noise. 

Taking the derivative is like convolution.

\paragraph{Gaussian Filter}
\label{sec:gaussian-filter}
For any function, the more spatially localized (peaked) it is, the
broader it is in frequency. Vice versa.

\pagebreak

\section{February 8th - Lecture 5: Diffusion}
\label{sec:lecture-5}

\paragraph{Sampling theorem}
\label{sec:sampling-theorem}
Given $$f(t) = a_0 + \sum_{k=1}^T a_k \cos(kt) + \sum_{k=1}^Tb_k\sin(
kt) \text{ for } t=0, \frac{2\pi}{M}, 2\frac{2\pi}{M}, 3\frac{2\pi}{M}$$
This equation is linear in unknown coefficients ($2T+1$ many) so we need
$2T+1$ samples to solve this equation.

If we have an analog version of someone's speech, you can digitize it
by taking $2T+1$ samples knowing they are band-limited (otherwise
we'll lose information). Speech is fine but the best thing is to apply a band-pass filter to make sure that
it's band-limited.

If the signal isn't band-limited to begin with, i.e. 
$f(t) = a_0 + \sum_{k=1}^{3T} a_k \cos(kt) + \sum_{k=1}^{3T} b_k\sin(
kt) $, you have more unknowns with $2T+1$ samples, and you're ignoring
a lot of information and it's totally meaning less.

\emph{Aliasing} when high frequency is indistinguishable from low
frequency when sampled.

\subsection{Diffusion}
\label{sec:diffusion}
Diffusing is like smoothing, provides a physical analogy to
smoothing. By setting it up and solving diffusion as a PDE, we'll see that
writing smoothing as PDE can be modified so that edges can be
preserved?

Again, everything followed is in 1D, we go from discrete, continuous,
then back to discrete.

\paragraph{Discrete}
\label{sec:discrete}
Imagine you have a lot of small buckets with lots of particles in it,
describe each bucket by how many things are in it, the
\emph{concentration}, $C(x,t)$, which changes over discrete time steps. 
i.e. $C(1,0)$ tells us how many particles are in bucket 1 at time 0.
Some of these particles can jump to neighboring buckets. A reasonable
model for physical diffusion (milk and coffee, heat, etc).
\emph{Flux} , $J(x,t)$, is the net number of particles that are moving in the
positive direction.

Diffusion is \emph{isotropic} (equally likely to go to left and
right) and \emph{homogenous} (same things happen everywhere).

The relationship between flux and concentration can be modelled as
such:
\begin{equation}
  \label{eq:13}
  J(x,t) = -D \frac{\partial C}{\partial x}
\end{equation}
If more stuff goes to the left than the right, the flux is
negative. $D$ is a constant diffusion coefficient, what fraction of things move around, the ``diffusivity'' of
particles. If $D$ is low, less stuff moves around.
\begin{equation}
  \label{eq:14}
  \frac{\partial C}{\partial t} = - \frac{\partial J}{\partial x}
\end{equation}
How much $C$ changes over time? Then I want to count how much is
coming in and how much is coming out (flux).  So if flux is constant,
then the concentration is not changing. If the flux is increasing,
that means there's more stuff going out to the right. So if the change
of $J$ is positive, the change in $C$ is negative. 

If $D$ wasn't constant, it would depend on $x$, to do more interesting
kind of smoothing, we can make $D$ into a function of $x$.

Combine the equation to get rid of $J$ by taking PDE wrt $x$: $$  \frac{\partial J}{\partial x} = -D   \frac{\partial^2 C}{\partial
    x^2}$$
Plugging this back to \eqref{eq:14}, we get
\begin{equation}
  \label{eq:16}
  \frac{\partial C}{\partial t} = D \frac{\partial^2 C}{\partial
    x^2}
\end{equation}

A positive second derivative means concavity, a local minima, so the
concentration increases, similarly, concentration goes down at a local
maxima second derivative negative. This means that concentration is being
smoothed over time.

\paragraph{Numerical Analysis}
\label{sec:numerical-analysis}
A finite differential problem. Taking the taylor series for a fixed
$t$, we get
\begin{equation}
  \label{eq:17}
  c_{i+1} = c_i + \del x   \frac{\partial C}{\partial x} +
  \frac{1}{2} \del x^2   \frac{\partial^2 C}{\partial x^2} + \cO(\del x^3)
\end{equation}
(also $  c_{i-1} = c_i - \del x   \frac{\partial C}{\partial x} +
  \frac{1}{2} \del x^2   \frac{\partial^2 C}{\partial x^2} + \cO(\del
  x^3)$)

Ignoring the higher order terms, you get, in first-order,
\begin{equation}
  \label{eq:18}
\frac{\partial C}{\partial x} =
\begin{cases}
\frac{  c_{i+1} - c_i }{\del x} \\  
 \frac{  c_{i} - c_{i-1} }{\del x} 
\end{cases}
\end{equation}

Adding them together, we get
\begin{equation}
  \label{eq:19}
 \frac{\partial C}{\partial x}=  \frac{  c_{i+1} - c_{i-1} }{2\del x} 
\end{equation}
Better because this is symmetric. 

Doing the same thing to the second derivative (difference between the
first derivative of left and right)
\begin{equation}
  \label{eq:20}
  \frac{\partial^2 C}{\partial x^2} = \frac{  (c_{i+1} - c_i) - (c_{i} - c_{i-1}) }{\del x^2} 
\end{equation}

We could say similar thing to wrt to $t$:
\begin{equation}
  \label{eq:21}
  \frac{\partial C}{\partial t} = \frac{ c(x,t+1)- c(x,t)}{\del t} 
\end{equation}

Putting all of this together, \eqref{eq:16} becomes

\begin{align*}
  \frac{\partial C}{\partial t} &= D \frac{\partial^2 C}{\partial
    x^2}\\
 \frac{ C(i,t_0+1)- C(i,t_0)}{\del t} &= D \frac{ C(i+1, t_0) - 2C(i,t_0) +
   C(i-1, t_0) }{\del x^2} \\
C(i,t_0+1)&= C(i,t_0) + \frac{\del t D}{\del x^2} (C(i+1, t_0) - 2C(i,t_0) +
   C(i-1, t_0))\\
 &= (1-2\lam)C(i,t_0) + \lam (i+1,t_0) + \lam C(i-1, t_0)   
\end{align*}
Where $\lam = \frac{\del t D}{\del x^2} $. This is just another
convolution with a filter that looks like $l = (\lam, 1-2\lam, \lam)$.

Let $C(x,0) = f(x)$, to get the concentration at time $n$, I get the
initial concentration at time $0$ and apply convolusion with filter
$l$ $t_0$ times. i.e. $$C(x,t_0) = (l\times l \times \cdots \times
l)\times f$$But since convolution is associative, we can combine
the filters together. Convolving $l$ with $l$ over and over gives us a
gaussian.

So \emph{diffusion is just a convolution with gaussian, same thing as
low-pass filtering an image, smoothing}.

Limitation on $\lambda$:  

\begin{align*}
  1 - 2\lam &> 0\\
1 - 2\frac{\del t D}{\del x^2} &> 0\\
\frac{\del x^2}{2D} &> \del t 
\end{align*}

\paragraph{Another intuition}
Consider a single particle that is diffusion< This
is a random variable $x_i$, where $x_i = -\del x$ if it moves left at
time $i$, $\del x$ if it moves to right. After $T$ time steps, the
position of the particle is $\sum_{i=1}^T x_i$, where by LLN, this is
a r.v. with 0 mean Gaussian distribution. So the particles position
after $T$ time steps is a Gaussian, we can get it by convolving $x_0$
with a Gaussian or convolving it with a filter $T$ times, same thing.


\pagebreak
\section{February 13th - Lecture 6: Edge Detection}
\label{sec:lecture-6}

\paragraph{Paper Presentation Topics}
\begin{itemize}
\item Graph based, MRF/CRF
\item Texture: (texton-boost)
\item \textbf{Co-segmentation}
\item \textbf{Layout} (3D surface estimation) by Hoeim, Efros
\item Affinity propagation by Frey (tronto)
\item Edge detection: Malik, Basiri
\item Graph Cuts - Galun \href{''http://www.wisdom.weizmann.ac.il/~vision/SketchTheCommon/''}{'Detecting and Sketching the Common''} 
\item Semantic
\end{itemize}

\subsection{Edge Detection: Canny Edge Detector}
\label{sec:edge-detection}

Basic Idea: look at sudden changes in intensity and the first derivative $\Ix$. 
But we'd expect some noise, so we always have to smooth the image with
a gaussian before we take the derivative.

\textbf{Algorithm in 1D}:
\begin{enumerate}
\item Smooth with a gaussian
\item Take the first derivative
\item 
  \begin{enumerate}
  \item Is it strong? (of large magnitude) Everything above a certain
    threshold is strong, where image is changing rapidly.
  \item Pick points that are not only strong but also a local extrema
  \end{enumerate}
\end{enumerate}

This procedure is optimal to minimize the number of false detections,
while also optimizing how well we localize the edge.

Couple of parameters: The threshold is the tradeoff between false
positive/negativees, std of the Gaussian, $\sig$, is the width of the
filter, the wider it is the smoother $I$ and less noise, but less accurately we'll localize the edge.

\subsection{In 2D}
\label{sec:2d}
In 2D, things are little bit more complicated. A rapid intensity
change depends on the direction. We need to figure out which direction
the intensity changes  most rapidly.

Compute the image gradient, $(\Ix, \Iy)$, and the magnitude $||(\Ix,
\Iy)|| = \sqrt{\Ix^2 + \Iy^2}$. We can represent the direction with
the maximal change by a unit vector: 
\begin{equation}
  \label{eq:24}
\frac{(\Ix, \Iy)}{||(\Ix, \Iy)||}
\end{equation}

Intuition: When you take the first derivatie, you assume the image is
locally linear, like a tilted plane, where there's a direction with
big change, where orthogonal to that direction change is flat.

We take gradient $\grad I$ instead of a derivative in 2D, and in
asking is it strong, we ask if $|| \grad I||$ big. It's bad if you don't use a big enough gaussian. We want to
renormalize so that the filter sums to 1, otherwise it makes the image
dimmer or lighter than it actually is.

\textbf{Smoothing in 2D}\\
Gaussian in 1D: $$\frac{1}{\sig \sqrt{2\pi}}e^{-\frac{x^2}{2\sig^2}}$$
Gaussian in 2D: 
\begin{align*}
G(x,y) &= \frac{1}{\sig \sqrt{2\pi}}e^{-\frac{x^2+y^2}{2\sig^2}}\\
&= \frac{1}{\sig \sqrt{2\pi}}e^{-\frac{x^2}{2\sig^2}} e^{-\frac{y^2}{2\sig^2}}\\
\end{align*}
(Maybe midterm: prove that these are same things)
We can use seperable filters because we can divide the filter into 2 filters.

\textbf{Picking extrema in 2D}
What defined an edge is the local extrema \emph{in the direction of
  the gradient}. The orthogonal direction is where the edge
continues. A subtlety: Given a vector field, every pixel has a vector which is its
gradient. We want to know what the image gradient is where each vector
is pointing, but there might not be any pixel ther. We can calculate
what it would be if the pixel grid were dense by interpolating (linear
or bi-linear) between two image locations.

\textbf{Hysteresis} to figure out if $||\grad I||$ is big: if i pick a threshold that's too big, things
might get fragmented and miss some edges. With a lower threshold,
we'll get unneccessarily edges from noise in the background. We want
the best of both. One heuristic to get this is to do the high
threshold, then the low threshold to get weaker edges but only keep
them if they're close to the stronger edges.
This is a very basic perceptual grouping based on connectivity.

This is the prominent method for edge detection but it still doesn't really work in real images. No matter how you pick threshold,
we get too much or too less. Because edges/boundaries that are
intuitive to us may not be strong locally. Also around pointy edges/corners,
smoothing weakens them.

\subsection{Corner Detection}
\label{sec:corner-detection}
A way to define a corner is a smal region in the image where you have
image gradient change in both directions. Look at a small window (5 x
5), in this window look at the image gradients (25). Do PCA, principal
component analysis, on the gradients and find out how much variations
there are in image gradients in one direction. If image gradient only
goes in one direction, only one PCA will be strong, but if it
goes in $x$ and $y$ direction, both PCA will be strong.

Compute a scatter matrix, $$H =
\begin{pmatrix}
  \sum \Ix\Ix & \sum \Ix\Iy \\
  \sum \Iy\Ix & \sum \Iy\Iy \\
\end{pmatrix}
$$
Where the first eigen value $\lam_2$, smallest one, gives you how much
gradient is in the direction of least.  if both $\lam_2, \lam_1$ big,
it means gradients change in all direction. This is call the harris
corner detector. 

\pagebreak

\section{February 15th - Lecture 7: Non-linear Diffusion}
\label{sec:febr-15th-lect}

Presentation points:
\begin{itemize}
\item Understand what's the biggest contribution about the paper. No
  need to pay attention to all of th edetails in the paper.
\item Give context of this paper from state-of-the-art, past, and how
  it fits
\item Where significant problems identified, addressed?
\item Give opinions
\item Always understand the questions before answering it
\end{itemize}

\subsection{Nonlinear Diffusion}
\label{sec:nonlinear-diffusion}
Goal for today
Non-homogenous diffusion: when you're near the boundary don't smooth
so much. Anisotropic (\# of particles leaving is not same in all direction):.

In 2D isotropic, the flux is in the direction of the negative
gradient, down hill. In anisotropic, the matrial isn't necessarily
going down hill, the direction depends on a larger context.

\subsection{Review: Isotropic Diffusion}
$f(x)$ is the image at time 0, and $u(x,t)$ is the image at time $t$
Flux in 1D: $$j(x,t) = -D \frac{\partial u}{\partial x}$$
flows to the negative direction of the derivative. $D$ is how fast
stuff diffuse.
Flux in 2D: $$j = - D \grad u$$
If $D$ is constant, this is gaussian smoothing in 2D, isotropic and homogenous. If we make $D=D(x)$ a
function of location $x$, it's non-homogenous. $D$ could also be a tensor,
a 2x2 matrix, giving us two vectors, now diffusion is non-homogenous
and anisotropic.
Intuition in 1D: $$\frac{\partial u}{\partial t} = -\frac{\partial
  j}{\partial x}$$
in 2D: $$\frac{\partial u}{\partial t} = -\text{div} j$$
Where $$\text{div} j = \frac{\partial j}{\partial x} + \frac{\partial j}{\partial y}$$

Substituting things, the heat equation in 1D $$\frac{\partial u}{\partial t} = D \frac{\partial^2 u}{\partial
  x^2} $$
in 2D $$\frac{\partial u}{\partial t} = \text{div}(D \grad u) $$

Gradient is a vector $\grad u = (\frac{\partial u}{\partial x} ,
\frac{\partial u}{\partial y} )$. $\text{div} j$ is the sum of these, saying
how much material is piling up in this location.

\subsection{2D Non-homogenous, isotropic diffusion}
\label{sec:non-homog-isotr}
Let $D=g(x)$, a function of the image. The amount of diffusion is
different in locations but the way things diffuse is the same. Write
$\frac{\partial u}{\partial t} = \text{div}(g(x) \grad u) $.

We want to diffuse but not around the boundary, i.e. when the gradient
is big, not. To do this, we can
make $g$:

$$g(||\grad f||^2) = \frac{1}{\sqrt{1 + \frac{||\grad f||^2}{\lam^2}}}$$

When $||\grad f|| \rarr \infty$, $\sqrt{1 + \frac{||\grad
    f||^2}{\lam^2}} \rarr \frac{\lam}{||\grad f||}$, very small. So
we'll smooth less and less as the gradient gets bigger.
$\lam$ is set by hand, controls what's the point where we make things
non-homogenous (where to stop diffusing). This gives a linear PDE, and
we can't do this with a convolution anymore (because convolution
applies same thing everywhere).

Problem with this approach is that it depends on the original
image. Say we have a small gradient that gets smoothed out. It'll have
less influence but small structures like this leave
artifacts. Instead, we can take the gradient of the actual image (not
the original $f$).
\begin{equation}
g(||\grad f||^2) = \frac{1}{\sqrt{1 + \frac{||\grad u||^2}{\lam^2}}}\label{eq:25}
\end{equation}
This is the \textbf{Perona-Malik} model. This model is \emph{unstable}. Why?
When a gradient really big at one point, its left neighbor is not getting
anything, so the neighbor goes down (with a slow bumpy slope, we can
get staircases) i.e. things are going in the opposite direction than usual
diffusion (instead of hi to low, it can go from low to hi). So this is
unstable. We can go around this by smoothing the image before taking the
gradient. 

\subsection{2D Non-homogenous, anisotropic diffusion}
\label{sec:2d-non-homogenous}
Let $D$ a 2x2 matrix based on the image. So back to$\frac{\partial
  u}{\partial t} = \text{div}(D\grad u) $. $D$ has eigenvectors $v_1,
v_2$, $v_1$, where  $v_1 \parallel \grad u_\sig$ and $v_2 \perp \grad
u_\sig$. Set the eigenvalues $\lam_1, \lam_2$ to $\lam_1 = g(||\grad u_\sig
||^2)$ (the term in Perona-Malik) and $\lam_2 = 1$.

If $\grad u_\sig = \grad u$, we're just scaling $\lam_1$ by sometime,
and this is just Perona-Malik. $u_\sig$ is the original image smoothed
by gaussian of size $\sig$.

Intuition: around the boundary, at a finer scale, the gradient is
showing noise. If $\grad u_\sig$ large, means that we're near a
boundary. 
$\grad u_\sig$ has the largest direction $v_1$ (pointing towards the boundary), and a component
orthogonal $v_2$. We can always take $\grad u$ and decompose it
into $v_1, v_2$. If $\grad u_\sig$ big, $v_1$ gets scaled down, but
$v_2$ is totally preserved (it's just 1), so \emph{$\grad u$ points away
from the boundary.} 

These approaches are all still heuristics compared to the bilateral
filters.

\pagebreak

\section{Lecture 8 - Contours}
\label{sec:lecture-8-contours}

\subsection{Markov Representation}
\label{sec:mark-repr}

Each variable depends on its neighbors. Conditionally independent of
the variables far away.

Markov Chain: R.V. occuring over time. 1D.
Markov Random Field: 2D. In an image, a markov field, a
pixel depends on the immediate neighbors, but conditionally indepenent of
the rest of the pixels given the neighboring pixels.

In 1D, key is that if you know a single r.v. it divides the set into
two disjoint sets. The seperation allows us to use DP etc to find
optimal solution. In 2D, a pixel doesn't divide the image in two
disconnected sets, so the problems with optimal solution in 1D becomes
NP-hard.

Sometimes we want to have a stochastic model of
images/objects. Deniosing is a random process that adds noise. To get
the MAP estimate, I need some expectation about the image, some
prior. We can model the boundaries of objects, having some prior model
of the shape of an object is important, and markov models help us
build these prior.

One way to describe a texture is repetition with variation or samples
drawn from some random process. 


\paragraph{1D case}
\label{sec:1d-case}
Markov chains is important for classification: positive examples are
similar meaning that they are samples from the same distribution. We
can model actions/image sequences in a markov chain.

\textbf{Definition}: $x_1, x_2, \dots, x_n$ are random variables. $x_i$s form a Markov
Chain iff. $$P(x_j | x_{j-1}, x_{j-2}, \dots,x_1) = P(x_j | x_{j-1})$$

Diffusion is an example of a  markov chain. Knowing where the paritcle
was at time $t-1$ helps us guess where the particle maybe at time $t$,
but knowing that at time $t-2$ doesn't really help. 

Markov chain that can model contours. You want to find a contour in
the image that fits the image gradients (the edges) but it's also
plausible (to be a real shape).  We want to solve the contour, $c$,
given the image $I$. This is $$\arg\max_c P(c|I) =
\frac{P(I|c)P(c)}{P(I)} \approx P(I|c)P(c)$$ (The denominator doesn't
matter because $P(I)$ same for all. $PI(c)$ is is this contour
plausible, is it likely to be a boundary of a real object? To do this
we need a prior. 

We can learn such distribution, but generate a simple model. Define contour to be
a point that moves through a space with some particular
direction. Denote the direction by $\theta$. Then we can say 
$$x' =\cos \theta, x'=\sin \theta, \theta' = \mathcal{N}(0, \sig^2)$$
where $x'$ is the change in the $x$-coordinate etc. If sigma is small,
the countor is smooth (straight) and not very contorted. In another words, $\theta$ is going
through a diffusion process and the direction follows from it..

Let $\Gamma_t$ is the position of the contour at time $t$. Then,
without constant, 
$$P(\Gamma_t)\approx \prod e^{-\frac{\theta'(t)^2}{\sig^2}}$$
Better written as (dropping constant):
\begin{equation}
  \label{eq:26}
-\log P(\Gamma_t)\approx \int \theta^2dt
\end{equation}

``Snakes'' computes a contour and also acounts for the prior probability of
the contour, minimizing \eqref{eq:26}.

Is this a really good prior for contours? Obvious fixable problems:
\begin{itemize}
\item Probability of generating an perpendicular edge (discontinuity)
  is 0. Fix by adding a gaussian mixture for the model that $\theta'$
\item Convex shapes are relatively likely, where curvature is all
  positive, meaning curvature is in a single direction. But this model
  allows $\theta'$ to go in both directions.
\item We also want contours that are short and smooth. How: Fixed
  probability that contours will end, i.e. particles have a half life
  $\lam$ that we add to \eqref{eq:26}
\end{itemize}

\emph{Curve of least energy}, curve between two edges that minimizes
\eqref{eq:26} because it can be thought as a energy of contours. 


\pagebreak

\section{Lecture - 9 Markov Random Field}
\label{sec:lecture-9}
Term project: Baseline-comparison of several algorithms, High
end-conference paper submission.
EC for doing both.

\subsection{Markov Property}
\label{sec:markov-property}
$x_1, \dots, x_k$ form a markov chain if $P(x_k | x_{k-1}, \dots, x_1)
= P(x_k | x_{k-1})$.
Assume $x_k$ has a label,  $x_k \in S = {s_1, \dots,
  s_m}$. Probability distribution for $x_k$, $P(x_k)$ is the probability it has
each label. We can express this in a vector $P^K = \lc P(x_k=s_1)
\dots P(x_k=s_m)\rc$. Using the markov property, the transition can be
represented in a matrix multiplication.

\begin{equation}
  \label{eq:27}
P(x_k | x_{k-1}, \dots, x_1) = \sum_{j=1}^M P^{k-1}(j)P(x_k=s_i |
x_{k-1} = s_j)  
\end{equation}
Where we let $A_{ij} = P(x_k=s_i |x_{k-1} = s_j)$, an entry in the transition
matrix $A$.$A_{ij}$ is going from state $j$ to state $i$, always same
over time.  If columns of $A$ sum to 1, we say that $A$ is a
stochastic matrix. Here, it should sum to 1 because a column is probability of going from $j$ to all
$i$. Then, 
\begin{equation}
  \label{eq:28}
  P^k = AP^{k-1}
\end{equation}
The inner product of the first row of $A$ and $P^{k-1}$ is same as \eqref{eq:27}.

$A$ is an easy way to figure out the stable distribution of this
chain. i.e.
\begin{align*}
   P^2 &= AP^1\\
   P^3 &= AP^2 = A(AP^1)\\
&\vdots\\
P^n &= A^{n-1}P^1
\end{align*}
In the end we get the largest eigenvector of $A$. This is actually the
power method of finding eigenvectors.
Let $P^1 = a_1v_1 + a_2v_2$, where $v_1,v_2$ are the eigenvectors. If
we apply $A$ to $P^1$, we get:
\begin{align*}
AP^1 &=a_1Av_1 + a_2Av_2\\
&= a_1 \lam_1v_1 + a_2 \lam_2v_2
\end{align*}
And in general
\begin{equation}
A^{n}P^1 = a_1\lam_1^nv_1 + a_2\lam_2^nv_2\label{eq:29}
\end{equation}
For a stochastic matrix, largest eigenvalue is 1, so we preserve
$v_1$. If we want to know the steady state of a markov chain, we take
the eigenvector corresponding to the largest eigenvalue.

Technicality: $P^1$ matters to do this: It has to be possible to
reach any state from any other state. And largest eigenvalue has to be
unique. 

Diffusion is a markov process.
Recall: gaussian smoothing is repeatedly applying a filter like $\lam,
1-2\lam, \lam$ to an image. i.e.
$$\begin{pmatrix}
  \lam & 1-2\lam& \lam & 0 & \dots 0\\
0&  \lam & 1-2\lam& \lam &  \dots 0\\
\vdots
\end{pmatrix}$$
Convolution is equivalent to matrix multiplication with this matrix.
Eigenvectors of this is harmonic, sines and cosines.

\subsection{Relaxation Labeling}
\label{sec:relaxation-labeling}

Basic idea: Suppose I want to label regions. If you know that there's
a telephone, the thing underneath it is likely to be a table etc. The
label of one thing gives you evidence about labels of other objects in
the scene. The point of relaxation labeling is to figure out how to
combine all these information.

\textbf{Linear version of relaxation labeling} assumes that evidences can be
linearly combined. An example of edge labeling: label foreground or
background. I.e. things that are close to each other and connect
smoothly to each other are foreground. One way to solve this problem
by prodviding edges between line segments. If the edge between
segments is strong, then the two segments are highly correlated.
Initialize everything at 0.5. Initial condition doesn't matter. 
This is different from belief propagation, because a node may get a
strong evidence but it's not an independent evidence. (Because you
started the rumor). In belief propgation, you're really careful not to
double count an evidence. 

Another interpretation of this algorithm is that edges are like
transition probabilities of particles. Steady state is defined by how
well particles can get to certain nodes.

1D case is nice because we can do matrix multiplication to find the
steady state i.e. taking eigen values. You can also do dynammic
programming. 

\subsection{Intelligent Scissors}
\label{sec:intelligent-scissors}
An interactive segmentation method. Idea: click on two different
pixels. Find the best boundaries that connect those two
points. Because it's interactive, it doesn't have to be perfect. In
order to solve this, we use markov/diffusion model for contours. We
want the contours to have relatively low curvature (smooth) and close
to points of high image gradient.

Add edges and weight it with whether this edge is a good boundary or
not, then just use any shortest path algorithms (DP).
Take the perpendicular derivative of an edge, you get high cost if
derivative is small, vise versa. Cost is high if you're not following
the boundary (gradient).

Next thing is smoothness/curvature. You need three points to define a
curvature. Create a node for every pixel, find its image gradient, when
you move, the penalty is if the image gradient is changing a lot or
not. 

\subsection{Review of DP}
\label{sec:review-dp}
Problem: we know the state as $x_1=0$, $x_n=1$. Find the most likely
sequence of intermediate states given the markov model, kind of like
the contour problem. The most trivial solution is consider all
possible combinations and try it - clearly exponential. You don't
really have to test all. If some intermediate $x_k=0$, we can split
the probelm into two: what's the problem of going from $x_1$ to $x_k$,
$x_k$ to $x_n$, same for $x_k=1$. 




\pagebreak

\section{Lecture 10 - Markov Random Field}
\label{sec:lecture-10-markov}
MRF is a collection of sites $S=\{ s_1, \dots, s_n\}$ (in 2D grid of
sites $S = \{S_{ij}\}$), and a set of labels $L=\{L_0\})$. Every site
is labeled with a labeling $f=\{f_1, \dots, f_n\}$.

It would make sense to talk about $P(f)$, which is the joint
probability of assigning a label to all sites. 
With a markov chain, $s_k$ will be dependent on $f_{k+1}, f_{k-1}$,
but given those it's conditionally independent from the rest. Here,
similar, but not two but all the neighbors and a site is conditionally
independent given all the neighbors, the markov blanket.
Formally, the neighborhood of $s_i$ is $N_i$, where $s_i\notin N_i$, and $s_i \in
N_j \iff s_j \in N_i$.
An MRF is $P(f) > 0 \forall f$. The key property is that 
\begin{equation}
  \label{eq:30}
P(f_i |f_{S-\{i\}} = P(f_i | f_{N_i})  
\end{equation}

You can take any probability distribution and make it in to a MRF just
by making everything into neighbors. The computational effort depends
on the size of the neighborhoods. If the neighborhood size is too big,
we don't get the benefit from making it into a MRF. Think of a grid
structure. 

\emph{Example}: in denoising, the label for each pixel is its true
noise free intensity. Putting it in MRF says that the true intensity of a pixel depends on the
true intensity of its immediate neigbhors but it's conditionally
independent of the rest given those neighbors.
In stereo, labels are the disparity level. 
We can also use this for less structured (not grids) things, like
detecting line segements and labeling it as a
foreground/background, where the neighbors are defined by those
segments around the line segment.

We can ask questions like ``What's the MAP estimate of assigning
labels?'', marginalizing over one site/rv: ``what's the probability distribution of
that site given the rest?''. We also want to learn this probability
distribution modeled by MRF, i.e. if we get bunch of examples of
labeled images. But these questinos are not trivial.

The big result that brings all of this together is the equivalence between MRF and
the gibbs distribution. 

\subsection{Gibbs Distribution}
\label{sec:gibbs-distribution}
A clique is a set of sites $\{s_{i_1}, s_{i_2}, \dots \}$
s.t. $s_{i_j} \in N_{i_k} \forall j,k$. $P(f)$ is a \textbf{gibbs
  distribution } iff.
\begin{equation}
  \label{eq:31}
  P(f) = \frac{1}{Z}e^{-u(f) / T}
\end{equation}
$T$ is referred to as the temperature and $u(f)$ is the energy
function/potential, can be written as
\begin{equation}
  \label{eq:32}
  u(f) = \sum_{c\in C} V_c(f)
\end{equation}
Where $V$ is the clique potential of $C$. In a grid, we have a trivial clique (by itself), two clique (two
pairs) in vertical and horizontal direction. In \eqref{eq:31}, the
exponent should look like summing multiple probabilities of the
clique. $Z$ is the normalization factor $Z= \sum_{f\in F} e^{-u(f)/T}$.

For vision, MRF is builds a prior. But we haven't discussed about how
well this prior fits to the image we have (i.e. denoising). So we have
\textbf{observations} , $X$. 

Usual vision problems form MRF s.t. every obesrvation have a specific
independent structure. 
I.e. given $X=\{x_1, \dots,x_n\}$,
\begin{equation}
P(x_i | f, X/\{x_i\}) = P(x_i | f_i)\label{eq:33}
\end{equation}
Implicitly we've had this assumption in the denoising papers we read,
because we assume that the actual observation (noise+truth)'s noise is
independent of every other pixel's noise. 

Continuing with the denoising example, we want solve this labeling
problem. In denoising it amounts to recovering the true intensity.
So given an observation, $x_i = f_i + e_i$ where $e_i$ is an
i.i.d. noise from $\mathcal{N}(o, \sig^2)$. We want
$$ \arg\max_f P(f | X)= \arg\max_f\frac{P(X|f) P(f)}{P(X)}.$$ by Bayes
law. Well $$P(X|f) = \prod P(x_i | f_i)$$   because of \eqref{eq:33},
and in image denoising, we have $P(x_i | f_i) =
\frac{1}{\sqrt{2\pi}\sig}e^{ \frac{-(f_i-x_i)^2}{2\sig^2}}$.

We can define the single clique potential as $$V_c(f_i) = \frac{(f_i -
  x_i)^2}{2\sig^2}$$ and for a pair clique, we define $V_c(f_i, f_j) =\begin{cases}
0 & \text{ if }  f_i = f_j\\
k & \text{ otherwise}
\end{cases}
$

The mot probable configuration is where the whole image is
constant. Total variation prime which is an explicit description would be $V_c^{tv} = (f_i, f_j) = |f_i
- f_j|_l$, but practically people don't do this. And the optimal MAP estimates of MRF is
$\mathcal{NP}$-hard 

Once we have these clique potentials, we have an optimization problem
of minimizing $u(f)$. 

White noise is called white because the fourier transform of the noise
is constant. Color noise might have a low freq components in fourier
transform, which means the noise of the neighbors are related. What if
we have color noise?
If we know $f_1$, previously we would've said $x_1$ is independent of
everything else. But now since the noise is correlated, it's not the
case here because the observations are related now and we have a
$v$-structure. 

Classic example: earthquake and burglur. They are independent event,
but if we give an observation that an alarm goes off. This gives
evidence about the burglar and the earthquake and makes them dependent
because if earthquake happened, there's a low chance that burglary
also happened.

Now \emph{condition everything on the image} .In MRF, we can figure
out the joint distribution of everything and we have a full generative
model $P(f, X)$. But if we condition, we get the \textbf{conditional
  random field} where we sought for $P(f|X)$. We don't have the
generative model anymore, we don't have the distribution of the model,
but having the image gets rid of the dependency problems. 

\pagebreak

\section{Lecture 11 - CRF}
\label{sec:lecture-11}
SItes = $S=\{s_i\}$, labels $f=\{f_i\}$, Neighborhoods: $j\in N_i \iff
s_j $ is a neighbor of $s_i$. 

Two restrictions to be a MRF:
$P(f) > 0 \forall f$
$P(f_i | f_{s-\{i\}} = P(f_i | f_{N_i})$ 

For \textbf{CRF}, the graph is globally conditioned on the observation. None of
the sites will be conditionally independent because all of the sites
share the same observation.

How does this change things?

\subsection{Gibbs Distribution}
Equivalent to MRF. For something to be a gibbs distribution, we say it
has this form: $P(f) = \frac{1}{Z}e^{ -u(f)/T}$ where $u(f)$ is the
energy. Key things is that $u$ can be factored. In particular, we say
$u(f) = \sum_{c\in C} V_c(f)$. where $C$ is the set of all of the
cliques, and $V_c$ is the potential of clique $c$.

In CRF, the only difference is that we have $u(f, x)$, energy
depending on the labels AND the observations. Where
\begin{align*}
  u(f,x) &= \sum_{c\in C} V_c(f,x)  \label{eq:35}\\
&= \sum V_c(f_i, X) + \sum V_c(f_i, f_j, X)
\end{align*}

Pictorically, in MRF, every site has a single observation (value of
pixel at $i$), in CRF, every site has a same observation, all of
observed data. With a CRF, the label can be dependent on the whole
image, or a piece of a image, but in MRF, you can only model the
dependency on a single pixel.

MRF is generative, it models the entire probability distribution
$\arg\max_f P(f|X) \propto P(X|f)P(f)$. This might be very difficult! CRF models
$P(f|X)$ directly and it's a discriminative model.

\subsection{MAP Inference}
\label{sec:inference}
Problem of $\arg\max_f P(f | X)$. What's your best guess as to what's
the best answer?

In general NP-hard, so all solutions are iterative. How to chose the
intial point? Heuristic is: try a bunch of random solutions, iterating
it until you hit a local solution, pick that as your starting
point. Only helpful if you can get the right place to start in 10\%,
but in MRF usually it's so rare to hit the rigth place that it won't
really help. Another approach is ignoring the interaction
potential. Just base the starting point on the unary potential.

\paragraph{Iterated Conditional Modes}
$\hat f$ current labeling. Perform:
for random $i$, $\hat f_i = \arg\min_{f_i} u(f_i\cup \hat f_{j\neq i},
X)$ (maximize the probability so minimize the energy). If unary
constraint is really strong, this is going to find the optimal. This
gets stuck really easily in local-minima.

\paragraph{Simulated Annealing}
On convex function: Start some place, you move down (towards the
minima). 
If your function is not convex (with a lot of local minima), even if
you're at a local minima you should be bouncing around to get out of it.

Given $\hat f$, pick $f_i$ randomly $$p = \min(1, \frac{P(\hat f_{i\neq
    j} \cup \{ f_i\}) }{P(\hat f) }$$ if the ratio $> 0$, this means that the old
labeling was better, with ICM, we would've never moved. Instead here
we still move sometime.

$T$ in the gibbs distribution is the temperature. $e^{-u(f)/T}$, when $T$
is high, the probability landscape is very flattened and the ratio is
around 1, so whether it's good or not we always move. As temperature
gets low, the labeling with the highest probability dominates (gets 1 
and the rest 0), and the more spread out the probabilities are and less likely
for us to move again.

\subsection{Graph Cuts}
\label{sec:graph-cuts}
Simulated annealing is old, not ppl use graph cuts.

Intuition: With ICM, you're considering one node at a time and saying
would it improve if I change this? Idea here is to consider changing a
lot of nodes at the same time.
Two steps: $\ah$-expansion and $\ah-\beta$ swap.

Problem of ICM is that pairwise constraints will not like changing one
pixel at a time. $\ah-\beta$ swap can swap all the pixels that has
either label, fix all other pixels, then relabel all those pixels in an optimal way just by using
the two label. If this labeling is better, we'll take it.
This is called the graph cut because we're making it into a graph and
the min-cut corresponds to the best configuration of $\ah-\beta$
labels.

\paragraph{$\ah-\beta$ swap}
Construct a graph where the terminal nodes are $\ah$ and $\beta$, take
all sites currently labeled $\ah$ or $\beta$ as nodes and connected to
both terminal nodes. We want the best cut where the cost of the cut is the weight of the edge. The
cut corresponds to a labeling, and weight corresponds to the clique
potential of that labeling.
Goal is to find a cut so that $\ah$ and $\beta$ are disconnected. If you cut the edge connecting to $\ah$, that nodes gets $\ah$. Think
 of edge as the clique potential, so cutting the edge is like taking
 the potential.

The min-cut will always keeps exactly one of the edge connecting to a
terminal node. You also make edges between the sites, so that if sites
don't ahve the same label they need to be cut.
To make sure that each edge represents the clique potential, for
example a site's edge connected to $\ah$ has weight $V_c(f_i
= \ah, X) + \sum_{j\in \mathcal{N}_i} V_c(f_i=\ah, f_j, X)$ where
$f_j\neq \ah, \beta$. For edges between sites, the weight there is
$V_c(f_i, f_j, X)$. For this to work, you need to design the pair-wise
potential to be s.t. if $f_i = f_j$, cost or the potential is 0. It
also has to be symmetric (doesn't matter if it's labeled $\ah$ or
$\beta$). 

We create this graph, then do a min-cut $O(n^2)$ pretty fast
practically.

\paragraph{$\ah$-expansion}
\label{sec:ah-expansion}
Similar, whether they keep $\ah$ or everything else become $\ah$ or
stay the same.

The whole algorithm is start with an initial labeling, then do
$\ah-\beta$ swap (or $\ah$-expansion) until you converge.

Suppose we have a binary labeling problem. Then if we do min-cuts we
have the globally optimal solution. So binary problems aren't really NP-hard.

\pagebreak

\section{Lecture 12 Normalized Cut}
\label{sec:lecture-12}
MIn cut - cost of the cut is the sum of the weight of the edges, the
min cut is the cut with smallest cost. 

Normalized Cut: to assign weight, you can look at the intensity
differences $(I_i - I_j)$. But you want it s.t. weight is really low when the
intensities are very different, so you do 
\begin{equation}
  \label{eq:36}
\exp(-\frac{||I_j - I_j||^2}{\sig})  
\end{equation}
This is heavily biased towards smaller regions. If the edge weight is
$\eps$, but the image is big enough s.t. the cut has a total cost of
$n\eps$. If $n\eps > 2$, it's better to cut off a single pixel of cost
2. The most influencial way to remove the bias is Normalized cut.

\textbf{Grab cut}: Image witha n object in the middle. Min-cut will
just cut off a tiny piece. Construct a graph using something like
\eqref{eq:36}. Then the user traces this is forground and and this is
background. So we reconstruct the graph with terminal nodes
\emph{foreground} and \emph{background}. If a pixel has foreground,
it's weight to the foreground pixel is $\infty$. Since this is
interactive, user can fix it up. This is more like a MRF, where edges
are pairwise costs and ``grabCut'' does the user interaction by
squares. State of the art interactive segmentation result.

\paragraph{Normalized-Cut}
A generic graph algorithm. $V = \{ \text{all vertices}\}$, a \emph{cut
} divides $V$ into $A$, $B$ s.t. $A\cap B = \emptyset, A\cup B = V$. A
cost of  a cut is
\begin{equation}
  \label{eq:37}
cut(A,B) = \sum_{u\in A, v\in B}w(u,v)
\end{equation}
Sum over $u$ and $v$ that are neighbors.
An \emph{association} between $A$ and $B$ is the total sum of weights
coming out of all of $A$.
$assoc(A,V) = \sum_{u\in A, v\in V}w(u,v)$

Proposed min cut
\begin{equation}
  \label{eq:39}
  \min_{A,B} Ncut(A,B) = \frac{cut(A,B)}{assoc(A,V)} + \frac{cut(B,A)}{assoc(B,V)} 
\end{equation}
For small cut, the ratio would be 1, which would be high, so we avoid
cutting smaller corners.

Suppose we had an image that was all white. The best Ncut is a
straight line in the middle. Vertical lines are equally low in cost so
in terms of mincut that's good. By making them in the middle, you make
the two terms equal. By symmetry the minimum is when those two are the
same.

So this has a bias towards equal sized shapes and a more compact,
convex shapes. A circle vs very snake line polygon, there'll be more cuts
between $A$ and $B$ in the snake, so circle is better. In general it thinks circular is
better. (Some disadvantages to this although the biases here is better
than biases in min-cut. Still it's generally a bad thing to have a
bias that you can't control becuase it won't fit the problem.)

\paragraph{Computation}
\label{sec:computation}
Computing this minimization is $NP$-hard (if you make your solution to
have discrete yes foreward or no backward). You can relax the
problem's discrete constraints (1 or -1), then the problem has a
simple solution with eigenvalues.

Let $W$ be the matrix of edge weights, symmetric because the graph is
undirected. $W_{j,i} = $ edge between $i$ and $j$. $x$ is a solution
vector and $x_i = \{1, -1\}$.
$d$ is a vector of total weights, $d_i = \sum_{j\in V} W_{ij}$ is the
sum of all the edges leaving node $i$.
$D$ is a diagonal matrix $d_i$'s on the diagonal. $d_i$ is just the
sum of $i$-th column of $W$.

With a lot of arithmetic manipulation, they show that $Ncut$ is
equivalent to solving the following:
\begin{equation}
  \label{eq:40}
  Ncut \equiv \frac{y^T(D-W)y}{y^TDy}
\end{equation}
where $y = (\vec 1+x) - b(\vec 1-x)$ and $b\in \mathbf{R}$ that depends on the
segmentation, the ratio of all the edges in $A$ leaving $A$ and that
of $B$. $b = \frac{ assoc(A,V)}{assoc(B,V)}$. If $x_i=1, y_i = 2$,
$x_i=-1, y_i = -2b$.

Example for intuition: Suppose $b = 1$, we can divide images in to two
equal sets. Suppose $\vec x$ has bunch of 1s followed by bunch of
-1s. $x = (1,  \dots, 1, -1 \dots, -1)^T, y = (2, \dots, 2, -2, \dots,
-2)^T$. So $Dy = Wy = (2d_1, 2d_2, \dots,  2d_k, -2d_{k+1}, \dots, -2d_n)^t$. When we multiply $W$ by $y$, we get positive values for first
part, negative values for the second pard... NEXT LECTURE.

\paragraph{How to solve \eqref{eq:40}?}
We let $Z = D^{1/2}y$ to remove $D$ from the denominator. So we get $y
= D^{-1/2}Z$ and now \eqref{eq:40} becomes $$\frac{z^t
  D^{-1/2}(D-W)D^{-1/2}z}{z^Tz} = \frac{z^TMz}{z^Tz}$$, where $M =
D^{-1/2}(D-W)D^{-1/2}$. This is still symmetric becaue both row and
col are scaled in the same way. Since $M$ is a symmetric matrix, we
can decompose it into:
$C = Q^TMQ$, where $Q$ is an orthonormal matrix and $C$ is diagonal,
which is equivalent to saying $QCQ^T = M$. Let $Qv = Z$,  where $v =
Q^TZ$, substitue all, then we get
\begin{align*}
\frac{z^TMz}{z^Tz}&=\frac{v^tQ^tCQ^TQv}{v^TQ^TQv}\\
&=\frac{vCv}{v^Tv}\\
&= \frac{v_1^2\lam_1^2 + \cdots + v_n^2\lam_n^2 }{v_1^2 + \cdots + v_n^2}
\end{align*}
Where the diagonal elements in $C$ $\lam_1, \lam_2, \dots,
\lam_n$, because they are the eigenvalues of $M$.

This is basically a weighted average by $\lam_i$s. This is minimized
by giving the smallest one the most weight. So this is minimized by $v
= (0, \dots, 0, 1)$, where $z$ is the eigenvector associated with the
smallest eigenvalue of $M$.

Extra condition, we actually don't want the smallest eigenvalue of $M$ because if we
have a trivial segmentation where all the values in $\vec x$ is 1,
the smallest eigenvalue would be 0 ($(D-W)y = 0$). So we want the
second smallest eigenvalue. $z$ is the eigenvector associated with the
second smallest eigenvalue.

This entire process gives us a continuous values of $\vec x$. We pick
some threshold and say that everything above is $A$ and everything
below is $B$. Since we really want to minimize the cost, we can try
every single threshold and test which one gives us the minimum
Ncut. Because there are only $N$ possible threshold. This isn't so expensive.

Size of $M$ is $n^2$ where $n$ is the \# of pixels. This is pretty expensive
if $n$ is big. To get around this, we don't have to connect every
pixel to every pixel, say within 10 pixels. So every pixel is
connected to 100 pixels. It will give us a million by million graph
that's very sparse. But there's a method to find the eigenvalues with
sparse graph.

What do they actually use for edge weights? They use $d(i,j)=
\begin{cases}
  1 & \text{immediate neighbor}\\
  2 & else
\end{cases}$, and 
$$e^{\frac{-d(i,j)^2}{\sig_d}} e^{\frac{-(I_i, I_j)^2}{\sig_I}}$$ This
  looks exactly like bilateral filter

\pagebreak

\section{Lecture 13: Parametric Clustering}
\label{sec:lecture-13}
Midterm assigned 3/26, due 4/2.
\subsection{Project 1}
Taking the first derivative a sobel mask $(-.1, 0, .1)$.
For NL-means, $h$ is the big parameter that determines how much
smoothing should be done. 
\subsection{Normalized Cut Example}
Recap: $W$ , edge weights between all pairs of pixels, $D$, a diagonal
matrix where $D_{ii}$ = sum of all weights leaving/connected to vertex
$i$. $\vec x$ a vector with domain $\{-1, 1\}$.
What you want to do is to $$\min \frac{ assoc(A,B)}{assoc(A,V)} +
\frac{ assoc(B,A)}{assoc(B,V)}$$ and this is equivalent to  $\min
\frac{y^T(D-W)y}{y^TDy}$, where $y=(1+\vec x) - b(1-\vec x)$, $b =
\frac{ assoc(A,V)}{assoc(B,V)}$.
Suppose $b=1$. Then $y\in \{-2, 2\}$, everything is symmetric. 

Intuition why $\min \frac{ assoc(A,B)}{assoc(A,V)} +
\frac{ assoc(B,A)}{assoc(B,V)} \equiv \min
\frac{y^T(D-W)y}{y^TDy}$:
$Wy$,  a column vector, the inner product of the first row of $W$ (all
edges leaving $A$) and $\vec y$, so $Wy =
\begin{pmatrix}
  2assoc(v_1, A) - 2assoc(v_1, B) \\
  2assoc(v_2, A) - 2assoc(v_2, B) \\
\vdots
\end{pmatrix}$

Now, $y^TWy$, a quadratic equation, every edge between $A$ and $A$ are
multiplyed by $2$, and every edge between $A$ and $B$ are mulitplied
by $2$ and $-2$, and every edge between $B$ and $B$ are multiplied by
$2$. think about parts of  that's $2$, i.e. vertices that
correspond to $A$. Each element in $Wy$ is multiplyed by $2$, you'll
get 4 times $assoc(A,A)$, when $-2$ is multiplied by 
$$y^TWy = 4 assoc(A,A) -4assoc(A,B)  - 4assoc(B,A) + 4assoc(B,B).$$

First row of $D$ gets multiplied by $y$, 
\begin{align*}
y^TDy &= y^T
\begin{pmatrix}
  y_1assoc(v_1,V)\\
  y_2assoc(v_2,V)\\
\vdots
\end{pmatrix}\\
&=  4assoc(A,V) -4assoc(B, V)  
\end{align*}

All of the edge weights multiplied by 4. They all cancel out so remove
them.
Now the ratio 
\begin{align*}
y^TWy/y^TDy &= \frac{assoc(A,V) + assoc(B,V) - assoc(A,A) -
assoc(B,B) - 2assoc(A,B)}{assoc(A,V) + assoc(B,V)}\\  
&=\frac{assoc(A,B) + assoc(B,A) + assoc(A,B) + assoc(B,A)}{assoc(A,V)
  + assoc(B,V)}\\
&=\frac{2assoc(A,B)}{2assoc(A,V)} + \frac{2assoc(B,A)}{2assoc(B,V)}\\
&=\frac{assoc(A,B)}{assoc(A,V)} + \frac{assoc(B,A)}{assoc(B,V)}\\
\end{align*}
Which is what we wanted.
Since $assoc(A,V)$ is edges coming out from all of $A$, if you
subtract it with $assoc(A,A)$, we get $assoc(A,B)$.

Normalized cut can be applied to any graph, it's just a way of taking
a graph and seperating it in two parts. If two nodes (pixels) are
similar, we have a stronger edge between them.

Going more abstract, think about the problem as clustering points in
high dimensional space: K-means, EM

\subsection{Parametric Clustering}
Few examples: a bunch of points in a 3-D space, and we  want to group these
points together in a cluster. A good cluster is a cluster where the
points are close togehter as possible given a limited number of
clusters. In vision, your three dimensions might be RGB. You might
want to cluster according to colors. If you discretize those clusters,
finding segments might be easier. Another reason you might want to do
this is to add 2 more dimensions, RBG + X and Y. Then clusters are
those similar in color and space. This can be very effective.
Another example is a perceptual grouping problem. If you draw three
lines (2 with positive slope, 1 intersecting both), there are 3
clusters and the points are close to those lines. You can represent
things parametrically. For a line, you need n-1 to
represent direction, n-1 as the shift = $2(n-1)$ parameters.
Looking at lines can give you structures of a building, another reason
for looking at lines is to estimate the plane. If you move a camera in
one direction and if you track points, in the image, objects move in
the opposite direction but background moves slower. As camera moves,
points on a plane moves in affine transformations, so you can cluster
points in similar affine transformation (6 free degrees). These are
all parametric clustering problems.

Technically this is known as the chicken and egg problems. If we knew
the parameters, we can figure out the assignments to the cluster, if
you know the assignments you can figure out the parameters that
fit. The way to go around this is iterative method, where you have an
initial guess and you keep updating until you converge. Many of these problems
might be NP-hard to find the optimal solution, so we use heuristics
which is the iterative algorithm. Although this is heuristics in
practice it often works very well.

\paragraph{K-means}
\label{sec:k-means}
We have bunch of points $x_1, \dots, x_n$, and we want to form $K$
clusters $A_1, \dots , A_k$, where $x_i \in A_j$ means point $i$ is in
cluster $j$. Ever cluster will have a parameter that describes it and
here we'll use cluster centers, $c_1, \dots, c_k$. Our objective is
\begin{equation}
  \label{eq:41}
  \min_{A, c} \sum_{j=1}^k\sum_{x_i\in A_j} ||c_j - x_i||^2
\end{equation}

The algorithm:
\begin{enumerate}
\item   Guess centers $c$
\item Assign each point to nearest center
\item Recompute $c$ s.t. $c_j  = \sum_{x_i\in
  A_j}\frac{x_i}{||A_j||}$
\item Repeat until convergence. (no assignments change)
\end{enumerate}

The reassignment can only reduce the cost. If you want to
minimize $\sum_{x_i\in A_j} ||c_j - x_i||^2$, you can take the
derivatie but you find out that the center is the average of all the
points in the cluster. Convergence? It might not converge to a global
minima but it will converge to a local minima. We'll have a cycle if
there's a point half way in between and it keeps on changing. But
cycling doesn't change the cost, so if convergence is defined by when
cost doesn't change, convergence is assured. 

Used in color quantization, when you want to compactly represent a
colro image. Every pixel has $RBG$, which is 24 bits. Suppose you want
to represent 4 bits not 24, this means we only have 16 colors. You can
quantize it and it'll still look good. For something like this we
don't necessarily have to have the global clustering. 

The first guess in $K$-means is really important. You can get
situations where all of your points gets assinged to a single
cluster. You need good heuristics to chose the starting guess. 

\pagebreak
\section{Lecture 14: EM}

\paragraph{Midterm} Due 4/4/12.
You can talk about the material, using the internet as a resource is
fine, anything you find. Cite any material used outside the class
website
Questions
\begin{enumerate}
\item $G_\sig$ a gaussian filter with std $\sig$, unknown. Use it to filter a
  cosine function. Figure out what's the value of filtered functions
  at other points.
\item Apply perona-malik to a sine wave, what is the method noise?
  Idea is if the filter is perfect the signal won't change. But it
  does, have a sense of how much method changes the original signal
\item Prove the method noise of non-local means is like white noise. Apply
  NL-means to sine wave (the analytic expression of the method noise
  is a pain), look at the method noise of two different locations, show
  that they are uncorrelated even if those points are close (or correlated). Prove
  true or false. \textbf{Extra credit} if you can characterize the method noise
  intuitively. (Bound doesn't matter but you can assume that $t\in [0, 2\pi]$.
\item MRF, a rabbit moving from one region to a neighboring region
  with equal probability, (never stay at the same region). After a
  while what's the probability distribution (stable distriubtion?)
  Give a numerical distribution (numbers).
\item MRF 4 labels for every pixel. Create a gibbs distribution using
  the provided priors (Next
  to each other means the 4 connected neighbors). Then optimize this
  by a graph cut algorithm, can we get into a local optimum that's not
  global? give a proof that this won't happen, or an example where it
  will get stuck in a local optima.
\item Given an $N$ by $N$ image with a grid structured graph (4
  connected), with weight 1 for all edges. Suppose there is an
  rectangle that doesn't touch the image boundary ($A$ by $B$)
  \begin{enumerate}
  \item what's  the Ncut cost of doing this two-part seperation.
  \item (Not saying normalized cut, he's asking us about the global
    solution, not the solution Ncut produces) Think about what gives
    you the global solution. Intuition is if you applied Ncut that
    finds the global minimum, does it find the rectangle? If not, can
    we change the cost so that this rectangle is the global solution?
  \item Has nothing to do with (b). Prove that rectangle that optimize this cost globally is actually a
    square. $A$ and $B$ have to be integers, so you can formulate the
    graph in continuous case and show that it's a square in continuous
    case. Let $R$ the $A$ by $B$ rectangle, and the rest $I-R$, if you
    find $ncut(R, I-R)$ (global minimum) is $R$ a square?
  \end{enumerate}
\end{enumerate}

\subsection{EM}
\label{sec:EM}
EM is just like $K$-means but rather than doing hard assignments does
soft assignments. Instead of taking a point and assigning it to a
cluster, you assign partial values of how sure the point belongs to
all clusters. When recomputing the center, take the weighted average
of confidence values. This helps us not converge into a bad local
minima.

Another way of looking at EM from Bayesian pov. We are going to assume
that the points were generated by a mixture of gaussians. The points
were generated by one of these gaussians. Fit a mixture of gaussians
to our points i.e. fitting a probability distribution to data. Each
cluster is assigned a different gaussian distribution. Each gaussian
with different size, mean, and co-variance. The real reason
why we use a mixture of gaussians is not because we think it's
accurate but because of it's nice properties we just want to use the
most simplest distribution that we can get away with. 


\paragraph{Mixture of Gaussians}
\label{sec:mixture-gaussians}
(Assume for simplicity covariance is a real number, gaussian circular)
Formally, $$P(x; \vec \mu, \vec \sig) = \sum_{k=1}^K p_k g(x; \mu_k, \sig_k)$$
We have $K$ gaussians each with its own $\mu$ and $\sig$. Now, given
some data we want to fit some distribution to it. Let $\vec \pi$ be the
vector of all $p_k$ values. What we want to do is
$$  \max_{\vec \pi, \vec \mu, \vec \sig} \sum_n P(x_n; \vec \mu, \vec \sig,
  \vec \pi)
$$
Maximum likelyhood approximation. This would be easy if we knew which
gaussian a point belongs to. Let $z_{k,n} = 1 $ iff $x_n \in G_k$
and $0$ otherwise. So $z$ is a hidden varialbe that tells you which
distribution (gaussian) this point comes from. Now we can re-write the
problem as
\begin{equation}
  \label{eq:43}
  \max_{\vec \pi, \vec \mu, \vec \sig} \sum_n \sum_k P(x_n; \mu_k, \sig_k, \pi_k)z_{k,n}
\end{equation}

Now, replace $z_{k,n}$ with $\mathbf{E}(z_{k, n}; \mu_k, \sig_k,
\pi_k)$. Now you iterate between fixing paramters, finding
$\mathbf{E}z_{k,n}$, then using that maximizing the equation by turing
the parameters.

So, 

\textbf{E step}:
\begin{equation}
z_{k,n}^i = \frac{p_k^i g(x_n; \mu_k^i, \sig_k^i)}{\sum_k
  p_k^ig(x_n; \mu_k^i, \sig_k^i) }\label{eq:44}
\end{equation}
Probability that distribution $k$ generated ($p_k$ is the probability
that the point came from the $k$-th gaussian) normailzed is the expectation.

\textbf{M step}:
\begin{equation}
  \label{eq:45}
  \mu_k^{i+1} = \frac{\sum_{n}  z_{k,n}^i x_n}{\sum_n z_{k,n}}
\end{equation}
Just the average, weighted by the expectation of where $n$-th point
comes from. Just like the sample mean. Exactly the same thing for
vaiance.
\begin{equation}
  \label{eq:46}
  p_k^{i+1} - \frac{1}{N}\sum z_{k,n}
\end{equation}

In stead of taking the expectation of $z$ if we took the maximum this
would be $k$-means. 

\paragraph{Kernel Density Estimation}
\label{sec:kern-dens-estim}
Suppose we have a bunch of points and we want to build a probability
distribution that fits the data. Instead of fitting it in mixture of
gaussians, put a small guassian distribution on every sample, so if
there are $n$ points, we get a mixture of $n$ gaussians. The advantage
of thsi is that in the limit as $n \rarr \infty$ and make gaussians
smaller and smaller, the limit approaches the true distribution
(trusting the data). The disadvantage is that this requries a lot of
parameters, each point is a parameter.

A combination is \textbf{mean-shift} segmentation algorithm. Take your
data, model it as kernel density estimation, then find the modes of
this distribution and treat those as cluster centers. Modes are the
points of local maximum (the biggest value, the peaks). Intuition is
starting off at one point, then imagine that point is a center of a
gaussian distribution, weight all of the points, and take the weighted
mean of all points (just like the maximization step in EM), then move
to the new mean and keep doing this. If you keep on doing this this
converges to the mode of a kernel density estimation. It's like EM
with only one class. 

\pagebreak

\section{Lecture 15: Level Sets}
\label{sec:lecture-15}
Idea is to do ``Curve evolution'', how a curve moves over time. Start
with a closed contour, then you have a some equation/force about how you
want this contour to change. Track how this contour moves over time.

Motivation in respect to vision/segmentation: One of the oldest approach is to
find the skeletons of objects with graph like edges inside (stick
figures). Skeleton seems like a very useful. The first algorithm that
was proposed was to move the curve inward and shrink it until you get
a medial axis. Definition of a skeleton is where the contours collide
by shrinking (equidistant from multiple points from a
contour). Another example of curve evolution has to do with
segmentation. Finding a boundary of an object with some noise around
it. One way to do this is to form an objective function (that explains
what a boundary should be like) and do gradient descent. The function
might say that the boundary has to have high gradient and the inside
and outside of the boundary should have uniform intensity.  A natural
way of doing segmentation. To improve the boundary, you need a method
of evolving curves, you're doing optimization in a weird space where
what you're changing is a contour. Many classical methods that try to
do this curve evolution (even outside vision). i.e. how does the oil
drop in water change? (you know the forces given by water) Started out
as a numerical analysis problem. It turns out there are subtlties
to do this right, 80's 90's ppl had interesting insights and started
using for vision.

It's trickier than it looks.

Say $c(s,t)$ a curve, $s$ location $\in \mathbf{R}^2$, $t$, time. $t=0$ is the initial
curve. i.e. $c(1/2, 0)$ is some point $(x,y)$ on the curve at time
0The first derivative $c_s \equiv$ tangent, $c_{ss} \equiv$ curvature
normal. The temporal derivative $c_t(s,0)$ is how it's moving. Assume that the
shrinking/motion is always to the curvature normal pointing inside (move in orthogonal
direction).

Shrinking can be described by
\begin{equation}
  \label{eq:47}
c_t(s,t) = n(s,t)
\end{equation}
 It says the way the
contour is moving is in the direction of the normal all at a constant
speed. \eqref{eq:47} is a part of the Grassfire algorithm. This is one
of the simplest curve evolution you can think of. 

One problem, if you have a completely smooth curve (infinitly
differentiable at all points). It's possible that at a small amount of
time you might get a V (absolute value function), a cone, and get
discontinuity. (ex on contour like a half of the tao sign) Now your
evolution is no longer well-defined. 

Another problem, you can start with a curve that looks like a dumbel
(hyoutan), you might change the topology of the contour by getting two
disconnected circles. Level sets give you a way to deal with these
changes in the topology.

Reminder note: Curve evolutino can be used for image
denoising. Suppose you have a noisy image, look at the iso-illuminate
contour (contour where intensity is all 0 and 1). Around the noise,
you'll get weird contours, around objects you get smooth contour, so
by doing curve evolution noises disappear pretty quicky.

To see what the hazards are, here's an one of the earliest method. The
Snakes method by Kass, Witkin, and Terzopoulos (~'87). Their idea was
to have a user roughly draw a contour and move it s.t. it'll go around
where the gradient is high, and that the integral of the contour to be
small (forcing contour to be small). How? Put points uniformly along
the contour, for each one of the points compute the gradient and use
finite differences to compute teh curvature and do gradient
descent. This works pretty well, advantage is that it's intuitive but
has couple of problems. 
\begin{enumerate}
\item Pointed out by the level sets handoug. You can have something
  smooth to get a cone structure and markers get really close and
  other parts markers get very far so the discretizaton gets
  unstable. It can work but it can have some artifacts.
\item These markers do not give you a way to change the topology.
\end{enumerate}
But very influencial.

\subsection{Level Set Approach}
\label{sec:level-set-approach}
Write curve evolution as a differential equation and look into a
stable way to solve this equation numerically. 
We start with
\subsubsection{Boundary Value Formulation}
We assume as the curve evolves it never crosses the same location
twice. Then we can describe the curve evolution by $T(x,y)$ at what
time did the curve cross position $(x,y)$? Locations where this is 0
are the initial conditions. Now we can write it as following
\begin{equation}
  \label{eq:10}
||\grad T || \dot F = 1
\end{equation}
$F$ is the speed that depends on the image. $||\grad T||$ is
saying that the slower the crossing time as more rapidly the things
are changing. 
Example $T(0) = 0, T(1) = 2, T(2)=4$. The change in $T' = 2$, the
speed if $F=1/2$ because it takes 2 seconds to move forward. The
faster it's moving the more slowly the arrival time is changing. Think
of it as waiting for the train, if you wait for 10hrs (the change in
arrival time is BIG) it means that
the train is moving really SLOW. If the train is moving really FAST,
the change in arrival time ($||\grad T||$) is SMALL.

The assumption fails for certain curves (like telephone handle).

So write it as $\phi(x(t), t)=0$ $\phi$ is a function of position and
time. So when it crosses the same point twice, $\phi$ is different. When $\phi=0$ that means that at time $t$, the curve will be at
$x(t)$. (Like the equation of a circle $x^2+y^2=25$, when this is 0 it
means that a point $(x,y)$ is on the circle).

Using chain rule, we get the derivative
\begin{equation}
  \label{eq:15}
  \phi_t + \grad \phi(x(t),t)x'(t) = 0
\end{equation}
How $\phi$ is changing over time, with gradient over position and time
with change in position over time. This takes the place of \eqref{eq:10}
Relating speed with the change in contour. Since $F$ is the speed in normal
direction, it is equal to $x'(t) \dot n = F$  and $n$ is given by $n=\frac{\grad \phi}{||\grad
  \phi||}$. 
Combining these together you get 
\begin{align*}
 x'(t)\frac{\grad \phi}{||\grad \phi ||} &= F\\
  x'(t) \grad \phi&= F||\grad \phi ||\\
\end{align*}

and  from \eqref{eq:15}
\begin{equation}
  \label{eq:22}
\phi_t + F||\grad \phi || =0  
\end{equation}

We know $F$, don't really know $\phi$. Example to keep in mind is that
if $F=1$ we have grassfire. 
Another unintuitive thing about $\phi$ is that when it isn't 0, it
doesn't matter what it is, but numerically it's well-behaved around
where it'll be 0.

It's more like $F||\grad \phi ||$ is how the speed is changing. Object
whose cross sections have different topology.

Grass fire $c_t = n(s)$, constant speed, another natural evolution
is speed depending on the curvative, $c_t = k(s)n(s)$, so if you have high curvature
it moves slow. Then  you can write this as $c_t = c_{ss}$. This should
remind you of the diffusion eqution where it was something like $u_t =
u_{xx}$. It's kind of a diffusion along the contour. Ultimately this
will converge to a circle then a point and vanishes.

This is important because this is the kind of smoothing that prevents
discontinuities from the contour (grassfire gives us this). If you do
$c_t = n(s) +\eps k(s)n(s)$, because as curvature gets huge (around a
cone discontinuity point the curvature goes to $\infty$) the diffusion
process takes over and smoothes it out.  \eqref{eq:22} is called the
\textbf{initial value formulation}. \eqref{eq:10} is the \textbf{boundary
value formulation}.

\subsection{Upwind differencing}
\label{sec:upwind-differencing}
Make the problem simpler in 1D $$u_t + u_x = 0.$$ $u$ is a function in $x$ and
$t$, just a differential equation. We say $u(x,0) = f(x)$. We get some
initial values at time 0. $u$ is like $\phi$ over there. We can solve
this equation by
\begin{equation}
  \label{eq:23}
u(x,t) = f(x-t)  
\end{equation} Why is this a solution? Becuase
$u_t = -f'(x-t)$, $u_x = f'(x-t)$, so $u_t+u_x = 0$. \eqref{eq:23} is
saying that the
value of $u(1,0) = u(2,1)$, basically a function is just shifted if
you move forward in time. You need to discretize the problem
correctly. You need to estimate your new value by the point you moved
from, not what you had.....



\pagebreak

\pagebreak

\pagebreak

\pagebreak

\end{document}
